{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/zjy/project/MetaIM')\n",
    "pwd = '/home/zjy/project/MetaIM/data'\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "cora_dataset = Planetoid(root=pwd+'/cora', name='cora')\n",
    "data = cora_dataset[0]\n",
    "edge_index = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), (1000, 2, 2708))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "individual_infection_path = pwd+'/for_meta/cora_individual_infection_sir_200_sample_500.npy'\n",
    "seeds_infection_path = pwd+'/for_meta/cora_seed_infection_sir_200_sample_500.npy'\n",
    "\n",
    "individual_infection = np.load(individual_infection_path)\n",
    "seeds_infection = np.load(seeds_infection_path)\n",
    "individual_infection.shape,seeds_infection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "                       [ 633, 1862, 2582,  ...,  598, 1473, 2706]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(2708, 2708), nnz=10556, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 转换为 scipy 稀疏矩阵\n",
    "adj = to_scipy_sparse_matrix(edge_index)\n",
    "\n",
    "\n",
    "# def normalize_adj(mx):\n",
    "#     \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "#     rowsum = np.array(mx.sum(1))\n",
    "#     r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "#     r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "#     r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "#     return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "\n",
    "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "# adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "adj = torch.Tensor(adj.toarray()).to_sparse()\n",
    "adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_num = int(seeds_infection[0][0].sum())\n",
    "seed_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, individual_infection,seeds_infection, seed_num):\n",
    "        self.individual_infection = individual_infection\n",
    "        self.seeds_infection = seeds_infection\n",
    "        self.feat_shape = (len(individual_infection), seed_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seeds_infection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeds= np.nonzero(self.seeds_infection[idx][0])[0]\n",
    "        \n",
    "        feature = torch.zeros(self.feat_shape[0],self.feat_shape[1])\n",
    "        for i in range(len(seeds)):\n",
    "            seed_i_infection = torch.tensor(self.individual_infection[seeds[i]])\n",
    "            feature[:, i] = seed_i_infection\n",
    "            \n",
    "        label = self.seeds_infection[idx][1]\n",
    "        \n",
    "        return self.seeds_infection[idx][0], feature, label\n",
    "\n",
    "dataset = CustomDataset(individual_infection, seeds_infection, seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义划分比例\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# 划分数据集\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*train_ratio), int(len(dataset)*test_ratio)])\n",
    "\n",
    "train_batch_size = 32\n",
    "test_batch_size = 2\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAEModel(\n",
       "  (Encoder): Encoder(\n",
       "    (FC_input): Linear(in_features=2708, out_features=1024, bias=True)\n",
       "    (FC_input2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (FC_output): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (FC_input): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (FC_hidden_1): Linear(in_features=64, out_features=1024, bias=True)\n",
       "    (FC_hidden_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (FC_output): Linear(in_features=1024, out_features=2708, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data import model \n",
    "from data.model.model import VAEModel, Encoder, Decoder\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # hidden_dim = 256\n",
    "# # latent_dim = 64\n",
    "hidden_dim = 1024\n",
    "latent_dim = 64\n",
    "\n",
    "encoder = Encoder(input_dim= len(seeds_infection[0][0]), \n",
    "                  hidden_dim=hidden_dim, \n",
    "                  latent_dim=latent_dim)\n",
    "\n",
    "decoder = Decoder(input_dim=latent_dim, \n",
    "                  latent_dim=latent_dim, \n",
    "                  hidden_dim=hidden_dim, \n",
    "                  output_dim=len(seeds_infection[0][0]))\n",
    "\n",
    "vae_model = VAEModel(Encoder=encoder, Decoder=decoder).to(device)\n",
    "\n",
    "optimizer_vae = Adam([{'params': vae_model.parameters()}], \n",
    "                 lr=1e-3)\n",
    "vae_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain_vae_loss: 21347.2563\n",
      "Epoch: 2 \tTrain_vae_loss: 13956.9220\n",
      "Epoch: 3 \tTrain_vae_loss: 13606.5711\n",
      "Epoch: 4 \tTrain_vae_loss: 13557.6980\n",
      "Epoch: 5 \tTrain_vae_loss: 13525.8392\n",
      "Epoch: 6 \tTrain_vae_loss: 13515.2488\n",
      "Epoch: 7 \tTrain_vae_loss: 13506.9995\n",
      "Epoch: 8 \tTrain_vae_loss: 13501.8271\n",
      "Epoch: 9 \tTrain_vae_loss: 13493.3672\n",
      "Epoch: 10 \tTrain_vae_loss: 13492.0108\n",
      "Epoch: 11 \tTrain_vae_loss: 13488.3854\n",
      "Epoch: 12 \tTrain_vae_loss: 13485.7106\n",
      "Epoch: 13 \tTrain_vae_loss: 13477.9147\n",
      "Epoch: 14 \tTrain_vae_loss: 13472.6427\n",
      "Epoch: 15 \tTrain_vae_loss: 13473.6380\n",
      "Epoch: 16 \tTrain_vae_loss: 13471.1379\n",
      "Epoch: 17 \tTrain_vae_loss: 13469.7922\n",
      "Epoch: 18 \tTrain_vae_loss: 13464.6074\n",
      "Epoch: 19 \tTrain_vae_loss: 13463.7839\n",
      "Epoch: 20 \tTrain_vae_loss: 13464.6028\n",
      "Epoch: 21 \tTrain_vae_loss: 13465.6917\n",
      "Epoch: 22 \tTrain_vae_loss: 13461.6078\n",
      "Epoch: 23 \tTrain_vae_loss: 13459.0419\n",
      "Epoch: 24 \tTrain_vae_loss: 13455.8961\n",
      "Epoch: 25 \tTrain_vae_loss: 13453.1794\n",
      "Epoch: 26 \tTrain_vae_loss: 13452.6425\n",
      "Epoch: 27 \tTrain_vae_loss: 13451.1623\n",
      "Epoch: 28 \tTrain_vae_loss: 13452.5520\n",
      "Epoch: 29 \tTrain_vae_loss: 13452.1172\n",
      "Epoch: 30 \tTrain_vae_loss: 13448.2635\n",
      "Epoch: 31 \tTrain_vae_loss: 13446.5424\n",
      "Epoch: 32 \tTrain_vae_loss: 13446.0027\n",
      "Epoch: 33 \tTrain_vae_loss: 13444.8497\n",
      "Epoch: 34 \tTrain_vae_loss: 13442.6520\n",
      "Epoch: 35 \tTrain_vae_loss: 13444.1766\n",
      "Epoch: 36 \tTrain_vae_loss: 13444.3781\n",
      "Epoch: 37 \tTrain_vae_loss: 13445.2222\n",
      "Epoch: 38 \tTrain_vae_loss: 13440.9966\n",
      "Epoch: 39 \tTrain_vae_loss: 13439.2491\n",
      "Epoch: 40 \tTrain_vae_loss: 13439.0048\n",
      "Epoch: 41 \tTrain_vae_loss: 13438.7481\n",
      "Epoch: 42 \tTrain_vae_loss: 13439.9809\n",
      "Epoch: 43 \tTrain_vae_loss: 13437.3578\n",
      "Epoch: 44 \tTrain_vae_loss: 13435.2625\n",
      "Epoch: 45 \tTrain_vae_loss: 13437.6342\n",
      "Epoch: 46 \tTrain_vae_loss: 13435.7217\n",
      "Epoch: 47 \tTrain_vae_loss: 13432.9008\n",
      "Epoch: 48 \tTrain_vae_loss: 13424.8173\n",
      "Epoch: 49 \tTrain_vae_loss: 13405.0070\n",
      "Epoch: 50 \tTrain_vae_loss: 13388.5369\n",
      "Epoch: 51 \tTrain_vae_loss: 13373.7308\n",
      "Epoch: 52 \tTrain_vae_loss: 13367.9581\n",
      "Epoch: 53 \tTrain_vae_loss: 13358.7789\n",
      "Epoch: 54 \tTrain_vae_loss: 13352.2251\n",
      "Epoch: 55 \tTrain_vae_loss: 13335.0129\n",
      "Epoch: 56 \tTrain_vae_loss: 13321.4268\n",
      "Epoch: 57 \tTrain_vae_loss: 13294.9818\n",
      "Epoch: 58 \tTrain_vae_loss: 13283.8445\n",
      "Epoch: 59 \tTrain_vae_loss: 13265.7077\n",
      "Epoch: 60 \tTrain_vae_loss: 13241.9650\n",
      "Epoch: 61 \tTrain_vae_loss: 13215.0930\n",
      "Epoch: 62 \tTrain_vae_loss: 13194.2887\n",
      "Epoch: 63 \tTrain_vae_loss: 13170.5544\n",
      "Epoch: 64 \tTrain_vae_loss: 13154.9024\n",
      "Epoch: 65 \tTrain_vae_loss: 13143.2310\n",
      "Epoch: 66 \tTrain_vae_loss: 13115.7625\n",
      "Epoch: 67 \tTrain_vae_loss: 13094.4301\n",
      "Epoch: 68 \tTrain_vae_loss: 13074.9293\n",
      "Epoch: 69 \tTrain_vae_loss: 13038.2299\n",
      "Epoch: 70 \tTrain_vae_loss: 13005.0063\n",
      "Epoch: 71 \tTrain_vae_loss: 12985.0735\n",
      "Epoch: 72 \tTrain_vae_loss: 12956.8020\n",
      "Epoch: 73 \tTrain_vae_loss: 12947.2562\n",
      "Epoch: 74 \tTrain_vae_loss: 12927.9355\n",
      "Epoch: 75 \tTrain_vae_loss: 12891.7584\n",
      "Epoch: 76 \tTrain_vae_loss: 12873.1761\n",
      "Epoch: 77 \tTrain_vae_loss: 12841.0182\n",
      "Epoch: 78 \tTrain_vae_loss: 12814.4263\n",
      "Epoch: 79 \tTrain_vae_loss: 12775.3062\n",
      "Epoch: 80 \tTrain_vae_loss: 12762.4073\n",
      "Epoch: 81 \tTrain_vae_loss: 12717.2086\n",
      "Epoch: 82 \tTrain_vae_loss: 12668.5090\n",
      "Epoch: 83 \tTrain_vae_loss: 12645.1623\n",
      "Epoch: 84 \tTrain_vae_loss: 12639.4345\n",
      "Epoch: 85 \tTrain_vae_loss: 12582.3991\n",
      "Epoch: 86 \tTrain_vae_loss: 12531.0202\n",
      "Epoch: 87 \tTrain_vae_loss: 12479.6707\n",
      "Epoch: 88 \tTrain_vae_loss: 12411.2677\n",
      "Epoch: 89 \tTrain_vae_loss: 12392.1829\n",
      "Epoch: 90 \tTrain_vae_loss: 12374.1399\n",
      "Epoch: 91 \tTrain_vae_loss: 12319.7173\n",
      "Epoch: 92 \tTrain_vae_loss: 12270.8220\n",
      "Epoch: 93 \tTrain_vae_loss: 12225.3993\n",
      "Epoch: 94 \tTrain_vae_loss: 12222.3577\n",
      "Epoch: 95 \tTrain_vae_loss: 12157.2753\n",
      "Epoch: 96 \tTrain_vae_loss: 12100.0085\n",
      "Epoch: 97 \tTrain_vae_loss: 12064.4571\n",
      "Epoch: 98 \tTrain_vae_loss: 11986.3907\n",
      "Epoch: 99 \tTrain_vae_loss: 11885.9623\n",
      "Epoch: 100 \tTrain_vae_loss: 11792.3431\n",
      "Epoch: 101 \tTrain_vae_loss: 11734.0290\n",
      "Epoch: 102 \tTrain_vae_loss: 11710.9914\n",
      "Epoch: 103 \tTrain_vae_loss: 11601.0346\n",
      "Epoch: 104 \tTrain_vae_loss: 11475.5690\n",
      "Epoch: 105 \tTrain_vae_loss: 11399.0063\n",
      "Epoch: 106 \tTrain_vae_loss: 11355.3759\n",
      "Epoch: 107 \tTrain_vae_loss: 11288.3981\n",
      "Epoch: 108 \tTrain_vae_loss: 11220.6531\n",
      "Epoch: 109 \tTrain_vae_loss: 11147.6050\n",
      "Epoch: 110 \tTrain_vae_loss: 11082.6586\n",
      "Epoch: 111 \tTrain_vae_loss: 11022.4918\n",
      "Epoch: 112 \tTrain_vae_loss: 10989.9581\n",
      "Epoch: 113 \tTrain_vae_loss: 10959.6461\n",
      "Epoch: 114 \tTrain_vae_loss: 10821.7687\n",
      "Epoch: 115 \tTrain_vae_loss: 10737.6687\n",
      "Epoch: 116 \tTrain_vae_loss: 10647.9037\n",
      "Epoch: 117 \tTrain_vae_loss: 10514.5340\n",
      "Epoch: 118 \tTrain_vae_loss: 10420.7214\n",
      "Epoch: 119 \tTrain_vae_loss: 10244.9010\n",
      "Epoch: 120 \tTrain_vae_loss: 10080.9313\n",
      "Epoch: 121 \tTrain_vae_loss: 9943.9118\n",
      "Epoch: 122 \tTrain_vae_loss: 9846.8070\n",
      "Epoch: 123 \tTrain_vae_loss: 9681.5815\n",
      "Epoch: 124 \tTrain_vae_loss: 9542.0280\n",
      "Epoch: 125 \tTrain_vae_loss: 9459.7449\n",
      "Epoch: 126 \tTrain_vae_loss: 9291.5614\n",
      "Epoch: 127 \tTrain_vae_loss: 9040.5970\n",
      "Epoch: 128 \tTrain_vae_loss: 9011.5017\n",
      "Epoch: 129 \tTrain_vae_loss: 8974.3511\n",
      "Epoch: 130 \tTrain_vae_loss: 8801.0258\n",
      "Epoch: 131 \tTrain_vae_loss: 8675.4133\n",
      "Epoch: 132 \tTrain_vae_loss: 8483.9957\n",
      "Epoch: 133 \tTrain_vae_loss: 8287.9705\n",
      "Epoch: 134 \tTrain_vae_loss: 8110.4385\n",
      "Epoch: 135 \tTrain_vae_loss: 8094.5644\n",
      "Epoch: 136 \tTrain_vae_loss: 7958.4588\n",
      "Epoch: 137 \tTrain_vae_loss: 7992.7899\n",
      "Epoch: 138 \tTrain_vae_loss: 7969.0812\n",
      "Epoch: 139 \tTrain_vae_loss: 7853.4160\n",
      "Epoch: 140 \tTrain_vae_loss: 7427.3346\n",
      "Epoch: 141 \tTrain_vae_loss: 7121.1374\n",
      "Epoch: 142 \tTrain_vae_loss: 6897.0493\n",
      "Epoch: 143 \tTrain_vae_loss: 6690.9119\n",
      "Epoch: 144 \tTrain_vae_loss: 6529.5982\n",
      "Epoch: 145 \tTrain_vae_loss: 6377.2472\n",
      "Epoch: 146 \tTrain_vae_loss: 6138.9728\n",
      "Epoch: 147 \tTrain_vae_loss: 6054.2144\n",
      "Epoch: 148 \tTrain_vae_loss: 6050.1529\n",
      "Epoch: 149 \tTrain_vae_loss: 6115.5023\n",
      "Epoch: 150 \tTrain_vae_loss: 6082.2024\n",
      "Epoch: 151 \tTrain_vae_loss: 5873.7389\n",
      "Epoch: 152 \tTrain_vae_loss: 5522.9989\n",
      "Epoch: 153 \tTrain_vae_loss: 5205.1499\n",
      "Epoch: 154 \tTrain_vae_loss: 4978.0629\n",
      "Epoch: 155 \tTrain_vae_loss: 4642.3135\n",
      "Epoch: 156 \tTrain_vae_loss: 4431.1582\n",
      "Epoch: 157 \tTrain_vae_loss: 4317.8965\n",
      "Epoch: 158 \tTrain_vae_loss: 4378.6291\n",
      "Epoch: 159 \tTrain_vae_loss: 4372.8342\n",
      "Epoch: 160 \tTrain_vae_loss: 4368.1525\n",
      "Epoch: 161 \tTrain_vae_loss: 4347.1399\n",
      "Epoch: 162 \tTrain_vae_loss: 4343.0592\n",
      "Epoch: 163 \tTrain_vae_loss: 4308.3035\n",
      "Epoch: 164 \tTrain_vae_loss: 4244.4069\n",
      "Epoch: 165 \tTrain_vae_loss: 3888.2424\n",
      "Epoch: 166 \tTrain_vae_loss: 3444.7116\n",
      "Epoch: 167 \tTrain_vae_loss: 3169.6131\n",
      "Epoch: 168 \tTrain_vae_loss: 2931.9608\n",
      "Epoch: 169 \tTrain_vae_loss: 2615.2840\n",
      "Epoch: 170 \tTrain_vae_loss: 2286.8209\n",
      "Epoch: 171 \tTrain_vae_loss: 2102.0615\n",
      "Epoch: 172 \tTrain_vae_loss: 2172.1764\n",
      "Epoch: 173 \tTrain_vae_loss: 2203.6747\n",
      "Epoch: 174 \tTrain_vae_loss: 2111.9755\n",
      "Epoch: 175 \tTrain_vae_loss: 2042.6536\n",
      "Epoch: 176 \tTrain_vae_loss: 1907.7919\n",
      "Epoch: 177 \tTrain_vae_loss: 1843.4245\n",
      "Epoch: 178 \tTrain_vae_loss: 1897.4258\n",
      "Epoch: 179 \tTrain_vae_loss: 2039.6098\n",
      "Epoch: 180 \tTrain_vae_loss: 2179.4975\n",
      "Epoch: 181 \tTrain_vae_loss: 2586.7103\n",
      "Epoch: 182 \tTrain_vae_loss: 2716.1252\n",
      "Epoch: 183 \tTrain_vae_loss: 3128.1051\n",
      "Epoch: 184 \tTrain_vae_loss: 3491.8937\n",
      "Epoch: 185 \tTrain_vae_loss: 3449.9925\n",
      "Epoch: 186 \tTrain_vae_loss: 2912.5527\n",
      "Epoch: 187 \tTrain_vae_loss: 2101.0825\n",
      "Epoch: 188 \tTrain_vae_loss: 1537.6653\n",
      "Epoch: 189 \tTrain_vae_loss: 1242.3757\n",
      "Epoch: 190 \tTrain_vae_loss: 943.0971\n",
      "Epoch: 191 \tTrain_vae_loss: 694.3618\n",
      "Epoch: 192 \tTrain_vae_loss: 541.5496\n",
      "Epoch: 193 \tTrain_vae_loss: 448.2195\n",
      "Epoch: 194 \tTrain_vae_loss: 402.3899\n",
      "Epoch: 195 \tTrain_vae_loss: 371.9995\n",
      "Epoch: 196 \tTrain_vae_loss: 382.2440\n",
      "Epoch: 197 \tTrain_vae_loss: 412.6302\n",
      "Epoch: 198 \tTrain_vae_loss: 431.3530\n",
      "Epoch: 199 \tTrain_vae_loss: 457.4531\n",
      "Epoch: 200 \tTrain_vae_loss: 458.1588\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    train_vae_loss = 0\n",
    "    mean_train_accuracy = 0\n",
    "    for batch_idx, x_feature_label in enumerate(train_loader):        \n",
    "        x = x_feature_label[0].to(device)\n",
    "        optimizer_vae.zero_grad()\n",
    "        loss = 0\n",
    "        for i, x_i in enumerate(x):\n",
    "            x_hat = vae_model(x_i)\n",
    "\n",
    "            reproduction_loss = F.binary_cross_entropy(x_hat, x_i, reduction='sum')   \n",
    "            loss += reproduction_loss    \n",
    "        train_vae_loss += loss.item()\n",
    "        loss = loss/x.size(0)\n",
    "        loss.backward()\n",
    "        optimizer_vae.step()\n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTrain_vae_loss: {:.4f}\".format(train_vae_loss / train_batch_size),\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vae_model.parameters():\n",
    "    param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, feat_dim, latent_dim, hidden_channels, out_channels, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.conv1 = GATConv(feat_dim, hidden_channels, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads+latent_dim, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, feat_i, x_i, edge_index):\n",
    "        x = self.conv1(feat_i, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        latten = F.relu(self.linear1(x_i))\n",
    "        x =  torch.cat((x, latten), dim=-1)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (conv1): GATConv(135, 512, heads=4)\n",
       "  (conv2): GATConv(2112, 1, heads=1)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data.model.gat import GAT, SpGAT\n",
    "\n",
    "# forward_model = SpGAT(nfeat=feat_num, \n",
    "#                 nhid=64, \n",
    "#                 nclass=1, \n",
    "#                 dropout=0.2, \n",
    "#                 nheads=1, \n",
    "#                 alpha=0.2)\n",
    "\n",
    "feat_num =seed_num\n",
    "\n",
    "forward_model = GAT(feat_num, latent_dim, 512, 1, 4)\n",
    "\n",
    "optimizer = Adam([{'params': forward_model.parameters()}], \n",
    "                 lr=0.001)\n",
    "\n",
    "adj = adj.to(device)\n",
    "forward_model = forward_model.to(device)\n",
    "forward_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(tensor):\n",
    "    # 计算每个特征的最小值和最大值\n",
    "    min_vals, _ = torch.min(tensor, dim=0)\n",
    "    max_vals, _ = torch.max(tensor, dim=0)\n",
    "\n",
    "    # 对每个特征进行归一化\n",
    "    normalized_tensor = (tensor - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTotal: 10710.9634 \tMean_train_accuracy: 0.4606\n",
      "\tMean_test_accuracy: 0.4630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 2 \tTotal: 9673.1859 \tMean_train_accuracy: 0.5180\n",
      "\tMean_test_accuracy: 0.5110 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 3 \tTotal: 9065.1428 \tMean_train_accuracy: 0.5869\n",
      "\tMean_test_accuracy: 0.5550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 4 \tTotal: 8568.2434 \tMean_train_accuracy: 0.6111\n",
      "\tMean_test_accuracy: 0.6110 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 5 \tTotal: 8250.6012 \tMean_train_accuracy: 0.6307\n",
      "\tMean_test_accuracy: 0.6330 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 6 \tTotal: 8091.8215 \tMean_train_accuracy: 0.6349\n",
      "\tMean_test_accuracy: 0.6340 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 7 \tTotal: 7961.0416 \tMean_train_accuracy: 0.6384\n",
      "\tMean_test_accuracy: 0.6370 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 8 \tTotal: 7896.8000 \tMean_train_accuracy: 0.6393\n",
      "\tMean_test_accuracy: 0.6390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 9 \tTotal: 7844.5958 \tMean_train_accuracy: 0.6352\n",
      "\tMean_test_accuracy: 0.6340 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 10 \tTotal: 7810.1778 \tMean_train_accuracy: 0.6426\n",
      "\tMean_test_accuracy: 0.6390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 11 \tTotal: 7781.3166 \tMean_train_accuracy: 0.6424\n",
      "\tMean_test_accuracy: 0.6360 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 12 \tTotal: 7749.8377 \tMean_train_accuracy: 0.6463\n",
      "\tMean_test_accuracy: 0.6360 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 13 \tTotal: 7722.4432 \tMean_train_accuracy: 0.6466\n",
      "\tMean_test_accuracy: 0.6430 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 14 \tTotal: 7677.2880 \tMean_train_accuracy: 0.6485\n",
      "\tMean_test_accuracy: 0.6420 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 15 \tTotal: 7648.5908 \tMean_train_accuracy: 0.6485\n",
      "\tMean_test_accuracy: 0.6440 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 16 \tTotal: 7588.2890 \tMean_train_accuracy: 0.6566\n",
      "\tMean_test_accuracy: 0.6470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 17 \tTotal: 7544.7874 \tMean_train_accuracy: 0.6498\n",
      "\tMean_test_accuracy: 0.6490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 18 \tTotal: 7519.8145 \tMean_train_accuracy: 0.6583\n",
      "\tMean_test_accuracy: 0.6490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 19 \tTotal: 7504.5132 \tMean_train_accuracy: 0.6621\n",
      "\tMean_test_accuracy: 0.6540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 20 \tTotal: 7438.5215 \tMean_train_accuracy: 0.6593\n",
      "\tMean_test_accuracy: 0.6520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 21 \tTotal: 7417.9641 \tMean_train_accuracy: 0.6578\n",
      "\tMean_test_accuracy: 0.6590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 22 \tTotal: 7391.7179 \tMean_train_accuracy: 0.6636\n",
      "\tMean_test_accuracy: 0.6540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 23 \tTotal: 7364.0979 \tMean_train_accuracy: 0.6624\n",
      "\tMean_test_accuracy: 0.6540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 24 \tTotal: 7347.0331 \tMean_train_accuracy: 0.6698\n",
      "\tMean_test_accuracy: 0.6620 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 25 \tTotal: 7317.9370 \tMean_train_accuracy: 0.6656\n",
      "\tMean_test_accuracy: 0.6730 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 26 \tTotal: 7282.4044 \tMean_train_accuracy: 0.6684\n",
      "\tMean_test_accuracy: 0.6680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 27 \tTotal: 7251.5104 \tMean_train_accuracy: 0.6687\n",
      "\tMean_test_accuracy: 0.6720 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 28 \tTotal: 7237.7622 \tMean_train_accuracy: 0.6701\n",
      "\tMean_test_accuracy: 0.6750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 29 \tTotal: 7216.7069 \tMean_train_accuracy: 0.6709\n",
      "\tMean_test_accuracy: 0.6700 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 30 \tTotal: 7191.0727 \tMean_train_accuracy: 0.6726\n",
      "\tMean_test_accuracy: 0.6760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 31 \tTotal: 7166.5076 \tMean_train_accuracy: 0.6680\n",
      "\tMean_test_accuracy: 0.6750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 32 \tTotal: 7148.2448 \tMean_train_accuracy: 0.6754\n",
      "\tMean_test_accuracy: 0.6750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 33 \tTotal: 7128.1034 \tMean_train_accuracy: 0.6790\n",
      "\tMean_test_accuracy: 0.6760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 34 \tTotal: 7105.3120 \tMean_train_accuracy: 0.6737\n",
      "\tMean_test_accuracy: 0.6760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 35 \tTotal: 7085.7997 \tMean_train_accuracy: 0.6789\n",
      "\tMean_test_accuracy: 0.6790 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 36 \tTotal: 7068.5673 \tMean_train_accuracy: 0.6779\n",
      "\tMean_test_accuracy: 0.6780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 37 \tTotal: 7066.2221 \tMean_train_accuracy: 0.6745\n",
      "\tMean_test_accuracy: 0.6800 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 38 \tTotal: 7054.6767 \tMean_train_accuracy: 0.6807\n",
      "\tMean_test_accuracy: 0.6810 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 39 \tTotal: 7020.0610 \tMean_train_accuracy: 0.6726\n",
      "\tMean_test_accuracy: 0.6770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 40 \tTotal: 7015.8695 \tMean_train_accuracy: 0.6700\n",
      "\tMean_test_accuracy: 0.6770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 41 \tTotal: 7026.8756 \tMean_train_accuracy: 0.6761\n",
      "\tMean_test_accuracy: 0.6710 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 42 \tTotal: 6985.0655 \tMean_train_accuracy: 0.6804\n",
      "\tMean_test_accuracy: 0.6800 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 43 \tTotal: 6954.7443 \tMean_train_accuracy: 0.6697\n",
      "\tMean_test_accuracy: 0.6760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 44 \tTotal: 6945.6502 \tMean_train_accuracy: 0.6767\n",
      "\tMean_test_accuracy: 0.6790 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 45 \tTotal: 6928.9205 \tMean_train_accuracy: 0.6798\n",
      "\tMean_test_accuracy: 0.6710 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 46 \tTotal: 6927.8400 \tMean_train_accuracy: 0.6757\n",
      "\tMean_test_accuracy: 0.6730 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 47 \tTotal: 6989.1505 \tMean_train_accuracy: 0.6752\n",
      "\tMean_test_accuracy: 0.6870 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 48 \tTotal: 6905.6866 \tMean_train_accuracy: 0.6773\n",
      "\tMean_test_accuracy: 0.6800 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 49 \tTotal: 6876.5338 \tMean_train_accuracy: 0.6777\n",
      "\tMean_test_accuracy: 0.6780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 50 \tTotal: 6891.0993 \tMean_train_accuracy: 0.6783\n",
      "\tMean_test_accuracy: 0.6810 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 51 \tTotal: 6894.8993 \tMean_train_accuracy: 0.6819\n",
      "\tMean_test_accuracy: 0.6810 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 52 \tTotal: 6846.2520 \tMean_train_accuracy: 0.6802\n",
      "\tMean_test_accuracy: 0.6770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 53 \tTotal: 6864.4106 \tMean_train_accuracy: 0.6751\n",
      "\tMean_test_accuracy: 0.6820 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 54 \tTotal: 6836.4663 \tMean_train_accuracy: 0.6805\n",
      "\tMean_test_accuracy: 0.6820 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 55 \tTotal: 6828.4906 \tMean_train_accuracy: 0.6832\n",
      "\tMean_test_accuracy: 0.6820 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 56 \tTotal: 6817.8536 \tMean_train_accuracy: 0.6781\n",
      "\tMean_test_accuracy: 0.6830 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 57 \tTotal: 6838.0107 \tMean_train_accuracy: 0.6796\n",
      "\tMean_test_accuracy: 0.6920 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 58 \tTotal: 6835.5765 \tMean_train_accuracy: 0.6786\n",
      "\tMean_test_accuracy: 0.6840 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 59 \tTotal: 6777.7531 \tMean_train_accuracy: 0.6819\n",
      "\tMean_test_accuracy: 0.6860 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 60 \tTotal: 6764.4790 \tMean_train_accuracy: 0.6826\n",
      "\tMean_test_accuracy: 0.6850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 61 \tTotal: 6790.1485 \tMean_train_accuracy: 0.6785\n",
      "\tMean_test_accuracy: 0.6890 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 62 \tTotal: 6766.7802 \tMean_train_accuracy: 0.6844\n",
      "\tMean_test_accuracy: 0.6850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 63 \tTotal: 6736.6836 \tMean_train_accuracy: 0.6799\n",
      "\tMean_test_accuracy: 0.6910 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 64 \tTotal: 6716.8232 \tMean_train_accuracy: 0.6867\n",
      "\tMean_test_accuracy: 0.6890 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 65 \tTotal: 6727.1279 \tMean_train_accuracy: 0.6896\n",
      "\tMean_test_accuracy: 0.6840 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 66 \tTotal: 6759.0432 \tMean_train_accuracy: 0.6828\n",
      "\tMean_test_accuracy: 0.6870 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 67 \tTotal: 6722.7640 \tMean_train_accuracy: 0.6919\n",
      "\tMean_test_accuracy: 0.6960 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 68 \tTotal: 6682.2046 \tMean_train_accuracy: 0.6858\n",
      "\tMean_test_accuracy: 0.6860 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 69 \tTotal: 6675.4267 \tMean_train_accuracy: 0.6830\n",
      "\tMean_test_accuracy: 0.6870 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 70 \tTotal: 6666.0490 \tMean_train_accuracy: 0.6897\n",
      "\tMean_test_accuracy: 0.6850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 71 \tTotal: 6666.4538 \tMean_train_accuracy: 0.6920\n",
      "\tMean_test_accuracy: 0.6990 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 72 \tTotal: 6645.2410 \tMean_train_accuracy: 0.6879\n",
      "\tMean_test_accuracy: 0.6900 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 73 \tTotal: 6628.4787 \tMean_train_accuracy: 0.6898\n",
      "\tMean_test_accuracy: 0.6850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 74 \tTotal: 6788.0339 \tMean_train_accuracy: 0.6812\n",
      "\tMean_test_accuracy: 0.6860 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 75 \tTotal: 6733.5695 \tMean_train_accuracy: 0.6903\n",
      "\tMean_test_accuracy: 0.6950 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 76 \tTotal: 6633.0387 \tMean_train_accuracy: 0.6865\n",
      "\tMean_test_accuracy: 0.6990 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 77 \tTotal: 6611.3085 \tMean_train_accuracy: 0.6895\n",
      "\tMean_test_accuracy: 0.6930 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 78 \tTotal: 6592.8139 \tMean_train_accuracy: 0.6904\n",
      "\tMean_test_accuracy: 0.6990 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 79 \tTotal: 6582.2823 \tMean_train_accuracy: 0.6940\n",
      "\tMean_test_accuracy: 0.6990 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 80 \tTotal: 6582.4646 \tMean_train_accuracy: 0.6944\n",
      "\tMean_test_accuracy: 0.6890 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 81 \tTotal: 6565.3528 \tMean_train_accuracy: 0.6979\n",
      "\tMean_test_accuracy: 0.6950 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 82 \tTotal: 6559.7031 \tMean_train_accuracy: 0.6901\n",
      "\tMean_test_accuracy: 0.7010 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 83 \tTotal: 6552.9855 \tMean_train_accuracy: 0.6998\n",
      "\tMean_test_accuracy: 0.7020 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 84 \tTotal: 6565.6369 \tMean_train_accuracy: 0.6932\n",
      "\tMean_test_accuracy: 0.7010 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 85 \tTotal: 6548.8712 \tMean_train_accuracy: 0.6946\n",
      "\tMean_test_accuracy: 0.7040 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 86 \tTotal: 6576.3039 \tMean_train_accuracy: 0.6947\n",
      "\tMean_test_accuracy: 0.6980 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 87 \tTotal: 6549.6006 \tMean_train_accuracy: 0.6899\n",
      "\tMean_test_accuracy: 0.6970 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 88 \tTotal: 6530.5946 \tMean_train_accuracy: 0.6977\n",
      "\tMean_test_accuracy: 0.7050 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 89 \tTotal: 6527.7491 \tMean_train_accuracy: 0.7008\n",
      "\tMean_test_accuracy: 0.7090 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 90 \tTotal: 6653.3159 \tMean_train_accuracy: 0.6983\n",
      "\tMean_test_accuracy: 0.7030 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 91 \tTotal: 6549.5823 \tMean_train_accuracy: 0.7006\n",
      "\tMean_test_accuracy: 0.6980 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 92 \tTotal: 6505.5368 \tMean_train_accuracy: 0.6996\n",
      "\tMean_test_accuracy: 0.7070 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 93 \tTotal: 6492.2115 \tMean_train_accuracy: 0.7082\n",
      "\tMean_test_accuracy: 0.7010 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 94 \tTotal: 6481.0600 \tMean_train_accuracy: 0.7004\n",
      "\tMean_test_accuracy: 0.7020 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 95 \tTotal: 6479.9065 \tMean_train_accuracy: 0.7054\n",
      "\tMean_test_accuracy: 0.7070 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 96 \tTotal: 6549.0224 \tMean_train_accuracy: 0.6994\n",
      "\tMean_test_accuracy: 0.7080 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 97 \tTotal: 6509.9503 \tMean_train_accuracy: 0.7040\n",
      "\tMean_test_accuracy: 0.7080 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 98 \tTotal: 6471.8810 \tMean_train_accuracy: 0.7044\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 99 \tTotal: 6528.6982 \tMean_train_accuracy: 0.7010\n",
      "\tMean_test_accuracy: 0.7090 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 100 \tTotal: 6461.4508 \tMean_train_accuracy: 0.7016\n",
      "\tMean_test_accuracy: 0.7110 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 101 \tTotal: 6442.8791 \tMean_train_accuracy: 0.7018\n",
      "\tMean_test_accuracy: 0.7140 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 102 \tTotal: 6435.7739 \tMean_train_accuracy: 0.7084\n",
      "\tMean_test_accuracy: 0.7120 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 103 \tTotal: 6442.7709 \tMean_train_accuracy: 0.7061\n",
      "\tMean_test_accuracy: 0.7020 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 104 \tTotal: 6525.9093 \tMean_train_accuracy: 0.7053\n",
      "\tMean_test_accuracy: 0.6980 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 105 \tTotal: 6499.2743 \tMean_train_accuracy: 0.6979\n",
      "\tMean_test_accuracy: 0.7100 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 106 \tTotal: 6443.4121 \tMean_train_accuracy: 0.7019\n",
      "\tMean_test_accuracy: 0.7010 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 107 \tTotal: 6415.3171 \tMean_train_accuracy: 0.7096\n",
      "\tMean_test_accuracy: 0.7180 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 108 \tTotal: 6410.1826 \tMean_train_accuracy: 0.7068\n",
      "\tMean_test_accuracy: 0.7120 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 109 \tTotal: 6472.1673 \tMean_train_accuracy: 0.6910\n",
      "\tMean_test_accuracy: 0.7140 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 110 \tTotal: 6517.9156 \tMean_train_accuracy: 0.7086\n",
      "\tMean_test_accuracy: 0.7050 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 111 \tTotal: 6423.2831 \tMean_train_accuracy: 0.7089\n",
      "\tMean_test_accuracy: 0.7180 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 112 \tTotal: 6390.7292 \tMean_train_accuracy: 0.7088\n",
      "\tMean_test_accuracy: 0.7130 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 113 \tTotal: 6387.4492 \tMean_train_accuracy: 0.7137\n",
      "\tMean_test_accuracy: 0.7070 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 114 \tTotal: 6376.3400 \tMean_train_accuracy: 0.7121\n",
      "\tMean_test_accuracy: 0.7090 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 115 \tTotal: 6406.1303 \tMean_train_accuracy: 0.7021\n",
      "\tMean_test_accuracy: 0.7100 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 116 \tTotal: 6469.7962 \tMean_train_accuracy: 0.7064\n",
      "\tMean_test_accuracy: 0.7150 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 117 \tTotal: 6382.1875 \tMean_train_accuracy: 0.7136\n",
      "\tMean_test_accuracy: 0.7140 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 118 \tTotal: 6364.2649 \tMean_train_accuracy: 0.7098\n",
      "\tMean_test_accuracy: 0.7150 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 119 \tTotal: 6356.1850 \tMean_train_accuracy: 0.7165\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 120 \tTotal: 6358.1559 \tMean_train_accuracy: 0.7113\n",
      "\tMean_test_accuracy: 0.7080 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 121 \tTotal: 6357.3387 \tMean_train_accuracy: 0.7147\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 122 \tTotal: 6357.2341 \tMean_train_accuracy: 0.7126\n",
      "\tMean_test_accuracy: 0.7020 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 123 \tTotal: 6492.6589 \tMean_train_accuracy: 0.7091\n",
      "\tMean_test_accuracy: 0.7110 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 124 \tTotal: 6496.9911 \tMean_train_accuracy: 0.7103\n",
      "\tMean_test_accuracy: 0.7170 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 125 \tTotal: 6409.2342 \tMean_train_accuracy: 0.7153\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 126 \tTotal: 6364.0948 \tMean_train_accuracy: 0.7185\n",
      "\tMean_test_accuracy: 0.7150 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 127 \tTotal: 6338.0420 \tMean_train_accuracy: 0.7216\n",
      "\tMean_test_accuracy: 0.7110 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 128 \tTotal: 6322.4707 \tMean_train_accuracy: 0.7157\n",
      "\tMean_test_accuracy: 0.7130 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 129 \tTotal: 6321.0988 \tMean_train_accuracy: 0.7206\n",
      "\tMean_test_accuracy: 0.7240 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 130 \tTotal: 6316.1608 \tMean_train_accuracy: 0.7133\n",
      "\tMean_test_accuracy: 0.7100 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 131 \tTotal: 6376.0937 \tMean_train_accuracy: 0.7159\n",
      "\tMean_test_accuracy: 0.7030 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 132 \tTotal: 6394.9886 \tMean_train_accuracy: 0.7199\n",
      "\tMean_test_accuracy: 0.7170 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 133 \tTotal: 6324.0640 \tMean_train_accuracy: 0.7189\n",
      "\tMean_test_accuracy: 0.7190 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 134 \tTotal: 6301.3880 \tMean_train_accuracy: 0.7202\n",
      "\tMean_test_accuracy: 0.7070 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 135 \tTotal: 6295.8229 \tMean_train_accuracy: 0.7128\n",
      "\tMean_test_accuracy: 0.7080 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 136 \tTotal: 6301.8190 \tMean_train_accuracy: 0.7218\n",
      "\tMean_test_accuracy: 0.7170 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 137 \tTotal: 6346.3557 \tMean_train_accuracy: 0.7232\n",
      "\tMean_test_accuracy: 0.7200 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 138 \tTotal: 6325.2735 \tMean_train_accuracy: 0.7176\n",
      "\tMean_test_accuracy: 0.7180 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 139 \tTotal: 6283.8861 \tMean_train_accuracy: 0.7206\n",
      "\tMean_test_accuracy: 0.7140 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 140 \tTotal: 6283.6555 \tMean_train_accuracy: 0.7246\n",
      "\tMean_test_accuracy: 0.7220 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 141 \tTotal: 6361.7957 \tMean_train_accuracy: 0.7137\n",
      "\tMean_test_accuracy: 0.7210 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 142 \tTotal: 6366.6774 \tMean_train_accuracy: 0.7201\n",
      "\tMean_test_accuracy: 0.7260 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 143 \tTotal: 6283.0687 \tMean_train_accuracy: 0.7250\n",
      "\tMean_test_accuracy: 0.7140 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 144 \tTotal: 6270.2674 \tMean_train_accuracy: 0.7291\n",
      "\tMean_test_accuracy: 0.7190 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 145 \tTotal: 6278.2373 \tMean_train_accuracy: 0.7231\n",
      "\tMean_test_accuracy: 0.7220 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 146 \tTotal: 6298.2605 \tMean_train_accuracy: 0.7151\n",
      "\tMean_test_accuracy: 0.7050 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 147 \tTotal: 6352.6177 \tMean_train_accuracy: 0.7246\n",
      "\tMean_test_accuracy: 0.7190 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 148 \tTotal: 6260.8919 \tMean_train_accuracy: 0.7254\n",
      "\tMean_test_accuracy: 0.7150 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 149 \tTotal: 6247.0071 \tMean_train_accuracy: 0.7314\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 150 \tTotal: 6266.2136 \tMean_train_accuracy: 0.7216\n",
      "\tMean_test_accuracy: 0.7190 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 151 \tTotal: 6252.0072 \tMean_train_accuracy: 0.7294\n",
      "\tMean_test_accuracy: 0.7210 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 152 \tTotal: 6233.4410 \tMean_train_accuracy: 0.7322\n",
      "\tMean_test_accuracy: 0.7190 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 153 \tTotal: 6225.7793 \tMean_train_accuracy: 0.7353\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 154 \tTotal: 6223.6481 \tMean_train_accuracy: 0.7298\n",
      "\tMean_test_accuracy: 0.7350 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 155 \tTotal: 6371.4994 \tMean_train_accuracy: 0.7234\n",
      "\tMean_test_accuracy: 0.7240 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 156 \tTotal: 6291.8604 \tMean_train_accuracy: 0.7281\n",
      "\tMean_test_accuracy: 0.7200 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 157 \tTotal: 6226.6344 \tMean_train_accuracy: 0.7249\n",
      "\tMean_test_accuracy: 0.7310 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 158 \tTotal: 6215.0173 \tMean_train_accuracy: 0.7326\n",
      "\tMean_test_accuracy: 0.7260 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 159 \tTotal: 6223.5975 \tMean_train_accuracy: 0.7274\n",
      "\tMean_test_accuracy: 0.7270 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 160 \tTotal: 6206.4677 \tMean_train_accuracy: 0.7301\n",
      "\tMean_test_accuracy: 0.7290 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 161 \tTotal: 6186.2094 \tMean_train_accuracy: 0.7312\n",
      "\tMean_test_accuracy: 0.7240 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 162 \tTotal: 6167.5151 \tMean_train_accuracy: 0.7346\n",
      "\tMean_test_accuracy: 0.7280 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 163 \tTotal: 6160.7641 \tMean_train_accuracy: 0.7337\n",
      "\tMean_test_accuracy: 0.7240 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 164 \tTotal: 6168.3441 \tMean_train_accuracy: 0.7321\n",
      "\tMean_test_accuracy: 0.7360 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 165 \tTotal: 6220.5653 \tMean_train_accuracy: 0.7363\n",
      "\tMean_test_accuracy: 0.7280 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 166 \tTotal: 6180.7210 \tMean_train_accuracy: 0.7337\n",
      "\tMean_test_accuracy: 0.7260 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 167 \tTotal: 6143.0539 \tMean_train_accuracy: 0.7377\n",
      "\tMean_test_accuracy: 0.7320 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 168 \tTotal: 6133.6613 \tMean_train_accuracy: 0.7411\n",
      "\tMean_test_accuracy: 0.7240 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 169 \tTotal: 6124.1182 \tMean_train_accuracy: 0.7421\n",
      "\tMean_test_accuracy: 0.7160 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 170 \tTotal: 6178.4545 \tMean_train_accuracy: 0.7363\n",
      "\tMean_test_accuracy: 0.7270 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 171 \tTotal: 6133.5598 \tMean_train_accuracy: 0.7406\n",
      "\tMean_test_accuracy: 0.7260 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 172 \tTotal: 6108.4684 \tMean_train_accuracy: 0.7400\n",
      "\tMean_test_accuracy: 0.7400 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 173 \tTotal: 6105.3715 \tMean_train_accuracy: 0.7396\n",
      "\tMean_test_accuracy: 0.7270 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 174 \tTotal: 6127.2134 \tMean_train_accuracy: 0.7439\n",
      "\tMean_test_accuracy: 0.7400 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 175 \tTotal: 6083.3237 \tMean_train_accuracy: 0.7453\n",
      "\tMean_test_accuracy: 0.7200 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 176 \tTotal: 6065.8266 \tMean_train_accuracy: 0.7434\n",
      "\tMean_test_accuracy: 0.7270 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 177 \tTotal: 6052.3372 \tMean_train_accuracy: 0.7376\n",
      "\tMean_test_accuracy: 0.7390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 178 \tTotal: 6247.1582 \tMean_train_accuracy: 0.7348\n",
      "\tMean_test_accuracy: 0.7270 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 179 \tTotal: 6091.7792 \tMean_train_accuracy: 0.7434\n",
      "\tMean_test_accuracy: 0.7370 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 180 \tTotal: 6037.4655 \tMean_train_accuracy: 0.7487\n",
      "\tMean_test_accuracy: 0.7200 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 181 \tTotal: 6015.8998 \tMean_train_accuracy: 0.7461\n",
      "\tMean_test_accuracy: 0.7210 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 182 \tTotal: 6002.0963 \tMean_train_accuracy: 0.7505\n",
      "\tMean_test_accuracy: 0.7350 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 183 \tTotal: 6021.5966 \tMean_train_accuracy: 0.7519\n",
      "\tMean_test_accuracy: 0.7290 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 184 \tTotal: 6024.6409 \tMean_train_accuracy: 0.7474\n",
      "\tMean_test_accuracy: 0.7340 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 185 \tTotal: 5981.4749 \tMean_train_accuracy: 0.7471\n",
      "\tMean_test_accuracy: 0.7280 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 186 \tTotal: 6043.5483 \tMean_train_accuracy: 0.7496\n",
      "\tMean_test_accuracy: 0.7320 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 187 \tTotal: 5969.8392 \tMean_train_accuracy: 0.7483\n",
      "\tMean_test_accuracy: 0.7330 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 188 \tTotal: 5953.4600 \tMean_train_accuracy: 0.7538\n",
      "\tMean_test_accuracy: 0.7360 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 189 \tTotal: 5939.7049 \tMean_train_accuracy: 0.7634\n",
      "\tMean_test_accuracy: 0.7460 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 190 \tTotal: 6134.7290 \tMean_train_accuracy: 0.7373\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 191 \tTotal: 6076.9571 \tMean_train_accuracy: 0.7508\n",
      "\tMean_test_accuracy: 0.7390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 192 \tTotal: 5952.1664 \tMean_train_accuracy: 0.7555\n",
      "\tMean_test_accuracy: 0.7470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 193 \tTotal: 5906.4798 \tMean_train_accuracy: 0.7546\n",
      "\tMean_test_accuracy: 0.7360 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 194 \tTotal: 5891.8321 \tMean_train_accuracy: 0.7558\n",
      "\tMean_test_accuracy: 0.7430 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 195 \tTotal: 5879.8799 \tMean_train_accuracy: 0.7528\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 196 \tTotal: 6023.4827 \tMean_train_accuracy: 0.7469\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 197 \tTotal: 5940.6897 \tMean_train_accuracy: 0.7521\n",
      "\tMean_test_accuracy: 0.7430 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 198 \tTotal: 5876.5269 \tMean_train_accuracy: 0.7564\n",
      "\tMean_test_accuracy: 0.7440 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 199 \tTotal: 5862.4965 \tMean_train_accuracy: 0.7591\n",
      "\tMean_test_accuracy: 0.7380 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 200 \tTotal: 5892.1212 \tMean_train_accuracy: 0.7526\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 201 \tTotal: 5920.0680 \tMean_train_accuracy: 0.7569\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 202 \tTotal: 5844.1694 \tMean_train_accuracy: 0.7597\n",
      "\tMean_test_accuracy: 0.7390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 203 \tTotal: 5840.1034 \tMean_train_accuracy: 0.7614\n",
      "\tMean_test_accuracy: 0.7390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 204 \tTotal: 5851.1337 \tMean_train_accuracy: 0.7519\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 205 \tTotal: 5833.3284 \tMean_train_accuracy: 0.7597\n",
      "\tMean_test_accuracy: 0.7440 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 206 \tTotal: 5804.7739 \tMean_train_accuracy: 0.7641\n",
      "\tMean_test_accuracy: 0.7580 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 207 \tTotal: 5825.9626 \tMean_train_accuracy: 0.7682\n",
      "\tMean_test_accuracy: 0.7420 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 208 \tTotal: 5833.0217 \tMean_train_accuracy: 0.7609\n",
      "\tMean_test_accuracy: 0.7430 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 209 \tTotal: 5785.4069 \tMean_train_accuracy: 0.7652\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 210 \tTotal: 5788.1591 \tMean_train_accuracy: 0.7608\n",
      "\tMean_test_accuracy: 0.7440 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 211 \tTotal: 5773.6570 \tMean_train_accuracy: 0.7643\n",
      "\tMean_test_accuracy: 0.7470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 212 \tTotal: 5912.3396 \tMean_train_accuracy: 0.7549\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 213 \tTotal: 5865.9234 \tMean_train_accuracy: 0.7626\n",
      "\tMean_test_accuracy: 0.7550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 214 \tTotal: 5777.4706 \tMean_train_accuracy: 0.7618\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 215 \tTotal: 5742.7854 \tMean_train_accuracy: 0.7632\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 216 \tTotal: 5736.6033 \tMean_train_accuracy: 0.7651\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 217 \tTotal: 5725.6006 \tMean_train_accuracy: 0.7696\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 218 \tTotal: 5753.0312 \tMean_train_accuracy: 0.7716\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 219 \tTotal: 5809.1811 \tMean_train_accuracy: 0.7657\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 220 \tTotal: 5731.3482 \tMean_train_accuracy: 0.7651\n",
      "\tMean_test_accuracy: 0.7500 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 221 \tTotal: 5713.1909 \tMean_train_accuracy: 0.7686\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 222 \tTotal: 5707.3620 \tMean_train_accuracy: 0.7745\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 223 \tTotal: 5733.9567 \tMean_train_accuracy: 0.7701\n",
      "\tMean_test_accuracy: 0.7440 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 224 \tTotal: 5719.3062 \tMean_train_accuracy: 0.7681\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 225 \tTotal: 5717.3527 \tMean_train_accuracy: 0.7559\n",
      "\tMean_test_accuracy: 0.7390 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 226 \tTotal: 5724.7070 \tMean_train_accuracy: 0.7659\n",
      "\tMean_test_accuracy: 0.7470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 227 \tTotal: 5703.2108 \tMean_train_accuracy: 0.7732\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 228 \tTotal: 5753.6052 \tMean_train_accuracy: 0.7644\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 229 \tTotal: 5697.6952 \tMean_train_accuracy: 0.7682\n",
      "\tMean_test_accuracy: 0.7550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 230 \tTotal: 5685.0786 \tMean_train_accuracy: 0.7750\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 231 \tTotal: 5669.9668 \tMean_train_accuracy: 0.7715\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 232 \tTotal: 5743.8484 \tMean_train_accuracy: 0.7633\n",
      "\tMean_test_accuracy: 0.7440 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 233 \tTotal: 5759.9726 \tMean_train_accuracy: 0.7713\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 234 \tTotal: 5672.7697 \tMean_train_accuracy: 0.7716\n",
      "\tMean_test_accuracy: 0.7500 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 235 \tTotal: 5662.0781 \tMean_train_accuracy: 0.7727\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 236 \tTotal: 5687.5768 \tMean_train_accuracy: 0.7721\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 237 \tTotal: 5653.1700 \tMean_train_accuracy: 0.7699\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 238 \tTotal: 5650.6203 \tMean_train_accuracy: 0.7737\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 239 \tTotal: 5639.4076 \tMean_train_accuracy: 0.7772\n",
      "\tMean_test_accuracy: 0.7610 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 240 \tTotal: 5629.2431 \tMean_train_accuracy: 0.7734\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 241 \tTotal: 5655.7623 \tMean_train_accuracy: 0.7739\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 242 \tTotal: 5771.5283 \tMean_train_accuracy: 0.7674\n",
      "\tMean_test_accuracy: 0.7670 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 243 \tTotal: 5649.4226 \tMean_train_accuracy: 0.7737\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 244 \tTotal: 5670.7969 \tMean_train_accuracy: 0.7774\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 245 \tTotal: 5664.4505 \tMean_train_accuracy: 0.7678\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 246 \tTotal: 5621.7287 \tMean_train_accuracy: 0.7727\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 247 \tTotal: 5609.7998 \tMean_train_accuracy: 0.7775\n",
      "\tMean_test_accuracy: 0.7500 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 248 \tTotal: 5623.2623 \tMean_train_accuracy: 0.7768\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 249 \tTotal: 5650.4604 \tMean_train_accuracy: 0.7771\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 250 \tTotal: 5606.2623 \tMean_train_accuracy: 0.7762\n",
      "\tMean_test_accuracy: 0.7340 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 251 \tTotal: 5647.0731 \tMean_train_accuracy: 0.7768\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 252 \tTotal: 5612.2804 \tMean_train_accuracy: 0.7744\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 253 \tTotal: 5638.9597 \tMean_train_accuracy: 0.7724\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 254 \tTotal: 5616.3816 \tMean_train_accuracy: 0.7770\n",
      "\tMean_test_accuracy: 0.7430 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 255 \tTotal: 5589.3109 \tMean_train_accuracy: 0.7787\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 256 \tTotal: 5617.8548 \tMean_train_accuracy: 0.7769\n",
      "\tMean_test_accuracy: 0.7670 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 257 \tTotal: 5591.7067 \tMean_train_accuracy: 0.7797\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 258 \tTotal: 5578.7739 \tMean_train_accuracy: 0.7776\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 259 \tTotal: 5583.2460 \tMean_train_accuracy: 0.7809\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 260 \tTotal: 5579.5736 \tMean_train_accuracy: 0.7745\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 261 \tTotal: 5646.6624 \tMean_train_accuracy: 0.7741\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 262 \tTotal: 5593.2973 \tMean_train_accuracy: 0.7713\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 263 \tTotal: 5608.0482 \tMean_train_accuracy: 0.7739\n",
      "\tMean_test_accuracy: 0.7610 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 264 \tTotal: 5698.9062 \tMean_train_accuracy: 0.7849\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 265 \tTotal: 5576.7802 \tMean_train_accuracy: 0.7841\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 266 \tTotal: 5553.0041 \tMean_train_accuracy: 0.7806\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 267 \tTotal: 5539.0486 \tMean_train_accuracy: 0.7808\n",
      "\tMean_test_accuracy: 0.7410 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 268 \tTotal: 5574.5382 \tMean_train_accuracy: 0.7798\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 269 \tTotal: 5543.4243 \tMean_train_accuracy: 0.7799\n",
      "\tMean_test_accuracy: 0.7660 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 270 \tTotal: 5531.9039 \tMean_train_accuracy: 0.7843\n",
      "\tMean_test_accuracy: 0.7580 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 271 \tTotal: 5556.6928 \tMean_train_accuracy: 0.7871\n",
      "\tMean_test_accuracy: 0.7470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 272 \tTotal: 5658.8565 \tMean_train_accuracy: 0.7810\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 273 \tTotal: 5573.0131 \tMean_train_accuracy: 0.7833\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 274 \tTotal: 5554.2205 \tMean_train_accuracy: 0.7784\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 275 \tTotal: 5548.8532 \tMean_train_accuracy: 0.7846\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 276 \tTotal: 5583.6074 \tMean_train_accuracy: 0.7817\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 277 \tTotal: 5548.8876 \tMean_train_accuracy: 0.7817\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 278 \tTotal: 5563.4601 \tMean_train_accuracy: 0.7811\n",
      "\tMean_test_accuracy: 0.7550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 279 \tTotal: 5555.1638 \tMean_train_accuracy: 0.7855\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 280 \tTotal: 5511.2656 \tMean_train_accuracy: 0.7907\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 281 \tTotal: 5499.3440 \tMean_train_accuracy: 0.7879\n",
      "\tMean_test_accuracy: 0.7610 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 282 \tTotal: 5504.6451 \tMean_train_accuracy: 0.7819\n",
      "\tMean_test_accuracy: 0.7670 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 283 \tTotal: 5494.4891 \tMean_train_accuracy: 0.7824\n",
      "\tMean_test_accuracy: 0.7730 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 284 \tTotal: 5622.0402 \tMean_train_accuracy: 0.7875\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 285 \tTotal: 5528.0220 \tMean_train_accuracy: 0.7882\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 286 \tTotal: 5496.9083 \tMean_train_accuracy: 0.7877\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 287 \tTotal: 5494.8953 \tMean_train_accuracy: 0.7876\n",
      "\tMean_test_accuracy: 0.7700 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 288 \tTotal: 5488.9419 \tMean_train_accuracy: 0.7896\n",
      "\tMean_test_accuracy: 0.7720 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 289 \tTotal: 5484.9125 \tMean_train_accuracy: 0.7838\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 290 \tTotal: 5493.4845 \tMean_train_accuracy: 0.7868\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 291 \tTotal: 5621.3588 \tMean_train_accuracy: 0.7831\n",
      "\tMean_test_accuracy: 0.7670 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 292 \tTotal: 5521.1611 \tMean_train_accuracy: 0.7869\n",
      "\tMean_test_accuracy: 0.7580 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 293 \tTotal: 5557.1895 \tMean_train_accuracy: 0.7861\n",
      "\tMean_test_accuracy: 0.7740 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 294 \tTotal: 5545.9595 \tMean_train_accuracy: 0.7847\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 295 \tTotal: 5481.4106 \tMean_train_accuracy: 0.7935\n",
      "\tMean_test_accuracy: 0.7550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 296 \tTotal: 5464.0350 \tMean_train_accuracy: 0.7923\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 297 \tTotal: 5465.9073 \tMean_train_accuracy: 0.7880\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 298 \tTotal: 5456.8469 \tMean_train_accuracy: 0.7894\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 299 \tTotal: 5479.2566 \tMean_train_accuracy: 0.7876\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 300 \tTotal: 5663.8896 \tMean_train_accuracy: 0.7800\n",
      "\tMean_test_accuracy: 0.7660 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 301 \tTotal: 5521.1865 \tMean_train_accuracy: 0.7845\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 302 \tTotal: 5458.6483 \tMean_train_accuracy: 0.7896\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 303 \tTotal: 5454.5815 \tMean_train_accuracy: 0.7884\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 304 \tTotal: 5449.1242 \tMean_train_accuracy: 0.7933\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 305 \tTotal: 5444.2415 \tMean_train_accuracy: 0.7880\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 306 \tTotal: 5452.6729 \tMean_train_accuracy: 0.7903\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 307 \tTotal: 5455.9519 \tMean_train_accuracy: 0.7882\n",
      "\tMean_test_accuracy: 0.7550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 308 \tTotal: 5448.4836 \tMean_train_accuracy: 0.7822\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 309 \tTotal: 5448.5385 \tMean_train_accuracy: 0.7921\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 310 \tTotal: 5447.7775 \tMean_train_accuracy: 0.7906\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 311 \tTotal: 5432.3111 \tMean_train_accuracy: 0.7905\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 312 \tTotal: 5434.0504 \tMean_train_accuracy: 0.7901\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 313 \tTotal: 5540.4354 \tMean_train_accuracy: 0.7827\n",
      "\tMean_test_accuracy: 0.7750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 314 \tTotal: 5497.9776 \tMean_train_accuracy: 0.7889\n",
      "\tMean_test_accuracy: 0.7660 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 315 \tTotal: 5473.2605 \tMean_train_accuracy: 0.7861\n",
      "\tMean_test_accuracy: 0.7470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 316 \tTotal: 5462.9463 \tMean_train_accuracy: 0.7913\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 317 \tTotal: 5426.2840 \tMean_train_accuracy: 0.7923\n",
      "\tMean_test_accuracy: 0.7710 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 318 \tTotal: 5410.2024 \tMean_train_accuracy: 0.7929\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 319 \tTotal: 5429.7196 \tMean_train_accuracy: 0.7945\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 320 \tTotal: 5409.0290 \tMean_train_accuracy: 0.7909\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 321 \tTotal: 5437.1955 \tMean_train_accuracy: 0.7928\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 322 \tTotal: 5414.7982 \tMean_train_accuracy: 0.7934\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 323 \tTotal: 5414.5493 \tMean_train_accuracy: 0.7974\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 324 \tTotal: 5409.6893 \tMean_train_accuracy: 0.7975\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 325 \tTotal: 5416.0672 \tMean_train_accuracy: 0.7963\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 326 \tTotal: 5410.6228 \tMean_train_accuracy: 0.7885\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 327 \tTotal: 5483.4208 \tMean_train_accuracy: 0.7950\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 328 \tTotal: 5424.0975 \tMean_train_accuracy: 0.7909\n",
      "\tMean_test_accuracy: 0.7500 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 329 \tTotal: 5421.0904 \tMean_train_accuracy: 0.7910\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 330 \tTotal: 5396.7516 \tMean_train_accuracy: 0.7876\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 331 \tTotal: 5386.5575 \tMean_train_accuracy: 0.7944\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 332 \tTotal: 5382.0216 \tMean_train_accuracy: 0.7941\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 333 \tTotal: 5446.7710 \tMean_train_accuracy: 0.7928\n",
      "\tMean_test_accuracy: 0.7850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 334 \tTotal: 5413.7258 \tMean_train_accuracy: 0.7969\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 335 \tTotal: 5395.5457 \tMean_train_accuracy: 0.7967\n",
      "\tMean_test_accuracy: 0.7810 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 336 \tTotal: 5438.6913 \tMean_train_accuracy: 0.7945\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 337 \tTotal: 5385.4425 \tMean_train_accuracy: 0.7969\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 338 \tTotal: 5363.6738 \tMean_train_accuracy: 0.8004\n",
      "\tMean_test_accuracy: 0.7770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 339 \tTotal: 5370.0769 \tMean_train_accuracy: 0.7936\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 340 \tTotal: 5354.7254 \tMean_train_accuracy: 0.8004\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 341 \tTotal: 5394.6155 \tMean_train_accuracy: 0.8028\n",
      "\tMean_test_accuracy: 0.7470 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 342 \tTotal: 5596.2708 \tMean_train_accuracy: 0.7926\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 343 \tTotal: 5402.7195 \tMean_train_accuracy: 0.7976\n",
      "\tMean_test_accuracy: 0.7490 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 344 \tTotal: 5378.5588 \tMean_train_accuracy: 0.7928\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 345 \tTotal: 5382.2850 \tMean_train_accuracy: 0.7978\n",
      "\tMean_test_accuracy: 0.7450 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 346 \tTotal: 5414.2546 \tMean_train_accuracy: 0.7977\n",
      "\tMean_test_accuracy: 0.7780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 347 \tTotal: 5390.7319 \tMean_train_accuracy: 0.7971\n",
      "\tMean_test_accuracy: 0.7750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 348 \tTotal: 5367.9274 \tMean_train_accuracy: 0.8034\n",
      "\tMean_test_accuracy: 0.7720 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 349 \tTotal: 5368.7747 \tMean_train_accuracy: 0.7946\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 350 \tTotal: 5349.2126 \tMean_train_accuracy: 0.7983\n",
      "\tMean_test_accuracy: 0.7620 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 351 \tTotal: 5377.2637 \tMean_train_accuracy: 0.7988\n",
      "\tMean_test_accuracy: 0.7730 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 352 \tTotal: 5409.8135 \tMean_train_accuracy: 0.7990\n",
      "\tMean_test_accuracy: 0.7710 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 353 \tTotal: 5353.9526 \tMean_train_accuracy: 0.7976\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 354 \tTotal: 5346.6710 \tMean_train_accuracy: 0.8001\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 355 \tTotal: 5393.5277 \tMean_train_accuracy: 0.7985\n",
      "\tMean_test_accuracy: 0.7850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 356 \tTotal: 5394.3394 \tMean_train_accuracy: 0.8022\n",
      "\tMean_test_accuracy: 0.7450 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 357 \tTotal: 5358.8274 \tMean_train_accuracy: 0.7983\n",
      "\tMean_test_accuracy: 0.7710 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 358 \tTotal: 5328.5579 \tMean_train_accuracy: 0.8025\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 359 \tTotal: 5335.7313 \tMean_train_accuracy: 0.7993\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 360 \tTotal: 5480.3563 \tMean_train_accuracy: 0.8003\n",
      "\tMean_test_accuracy: 0.7550 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 361 \tTotal: 5363.2885 \tMean_train_accuracy: 0.7974\n",
      "\tMean_test_accuracy: 0.7740 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 362 \tTotal: 5326.3109 \tMean_train_accuracy: 0.8029\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 363 \tTotal: 5316.6460 \tMean_train_accuracy: 0.8030\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 364 \tTotal: 5322.3646 \tMean_train_accuracy: 0.8030\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 365 \tTotal: 5385.9311 \tMean_train_accuracy: 0.8027\n",
      "\tMean_test_accuracy: 0.7660 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 366 \tTotal: 5341.0731 \tMean_train_accuracy: 0.7995\n",
      "\tMean_test_accuracy: 0.7450 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 367 \tTotal: 5349.7975 \tMean_train_accuracy: 0.7954\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 368 \tTotal: 5349.3364 \tMean_train_accuracy: 0.8022\n",
      "\tMean_test_accuracy: 0.7760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 369 \tTotal: 5323.6941 \tMean_train_accuracy: 0.8023\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 370 \tTotal: 5326.1359 \tMean_train_accuracy: 0.8053\n",
      "\tMean_test_accuracy: 0.7700 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 371 \tTotal: 5350.1871 \tMean_train_accuracy: 0.8034\n",
      "\tMean_test_accuracy: 0.7730 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 372 \tTotal: 5368.9217 \tMean_train_accuracy: 0.8008\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 373 \tTotal: 5349.3889 \tMean_train_accuracy: 0.8008\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 374 \tTotal: 5314.7666 \tMean_train_accuracy: 0.8051\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 375 \tTotal: 5315.6062 \tMean_train_accuracy: 0.8061\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 376 \tTotal: 5317.1437 \tMean_train_accuracy: 0.8027\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 377 \tTotal: 5307.2656 \tMean_train_accuracy: 0.8041\n",
      "\tMean_test_accuracy: 0.7780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 378 \tTotal: 5313.0910 \tMean_train_accuracy: 0.8024\n",
      "\tMean_test_accuracy: 0.7610 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 379 \tTotal: 5294.8840 \tMean_train_accuracy: 0.8060\n",
      "\tMean_test_accuracy: 0.7780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 380 \tTotal: 5339.3661 \tMean_train_accuracy: 0.8005\n",
      "\tMean_test_accuracy: 0.7850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 381 \tTotal: 5377.9884 \tMean_train_accuracy: 0.8021\n",
      "\tMean_test_accuracy: 0.7560 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 382 \tTotal: 5317.4720 \tMean_train_accuracy: 0.8029\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 383 \tTotal: 5294.5816 \tMean_train_accuracy: 0.7985\n",
      "\tMean_test_accuracy: 0.7770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 384 \tTotal: 5281.6589 \tMean_train_accuracy: 0.8042\n",
      "\tMean_test_accuracy: 0.7580 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 385 \tTotal: 5283.4454 \tMean_train_accuracy: 0.8014\n",
      "\tMean_test_accuracy: 0.7860 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 386 \tTotal: 5301.9532 \tMean_train_accuracy: 0.8029\n",
      "\tMean_test_accuracy: 0.7500 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 387 \tTotal: 5282.1710 \tMean_train_accuracy: 0.7983\n",
      "\tMean_test_accuracy: 0.7700 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 388 \tTotal: 5264.3330 \tMean_train_accuracy: 0.8051\n",
      "\tMean_test_accuracy: 0.7430 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 389 \tTotal: 5335.2621 \tMean_train_accuracy: 0.8055\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 390 \tTotal: 5285.2984 \tMean_train_accuracy: 0.8055\n",
      "\tMean_test_accuracy: 0.7450 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 391 \tTotal: 5281.9300 \tMean_train_accuracy: 0.8072\n",
      "\tMean_test_accuracy: 0.7570 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 392 \tTotal: 5268.4055 \tMean_train_accuracy: 0.8107\n",
      "\tMean_test_accuracy: 0.7660 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 393 \tTotal: 5331.4711 \tMean_train_accuracy: 0.8027\n",
      "\tMean_test_accuracy: 0.7800 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 394 \tTotal: 5319.4905 \tMean_train_accuracy: 0.8057\n",
      "\tMean_test_accuracy: 0.7810 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 395 \tTotal: 5291.2569 \tMean_train_accuracy: 0.8015\n",
      "\tMean_test_accuracy: 0.7650 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 396 \tTotal: 5271.6690 \tMean_train_accuracy: 0.8045\n",
      "\tMean_test_accuracy: 0.7670 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 397 \tTotal: 5333.1398 \tMean_train_accuracy: 0.7988\n",
      "\tMean_test_accuracy: 0.7680 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 398 \tTotal: 5277.8281 \tMean_train_accuracy: 0.8027\n",
      "\tMean_test_accuracy: 0.7670 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 399 \tTotal: 5283.0786 \tMean_train_accuracy: 0.8069\n",
      "\tMean_test_accuracy: 0.7820 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 400 \tTotal: 5276.0928 \tMean_train_accuracy: 0.8082\n",
      "\tMean_test_accuracy: 0.7760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 401 \tTotal: 5259.3128 \tMean_train_accuracy: 0.8020\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 402 \tTotal: 5256.7274 \tMean_train_accuracy: 0.8043\n",
      "\tMean_test_accuracy: 0.7500 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 403 \tTotal: 5250.8695 \tMean_train_accuracy: 0.8029\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 404 \tTotal: 5273.1612 \tMean_train_accuracy: 0.8008\n",
      "\tMean_test_accuracy: 0.7460 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 405 \tTotal: 5283.4988 \tMean_train_accuracy: 0.8055\n",
      "\tMean_test_accuracy: 0.7760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 406 \tTotal: 5276.5355 \tMean_train_accuracy: 0.8046\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 407 \tTotal: 5255.0639 \tMean_train_accuracy: 0.8046\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 408 \tTotal: 5270.3943 \tMean_train_accuracy: 0.8111\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 409 \tTotal: 5242.6859 \tMean_train_accuracy: 0.8084\n",
      "\tMean_test_accuracy: 0.7790 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 410 \tTotal: 5255.6364 \tMean_train_accuracy: 0.8024\n",
      "\tMean_test_accuracy: 0.7510 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 411 \tTotal: 5254.3346 \tMean_train_accuracy: 0.8016\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 412 \tTotal: 5237.4489 \tMean_train_accuracy: 0.8079\n",
      "\tMean_test_accuracy: 0.7600 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 413 \tTotal: 5255.8785 \tMean_train_accuracy: 0.8032\n",
      "\tMean_test_accuracy: 0.7580 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 414 \tTotal: 5247.6214 \tMean_train_accuracy: 0.8116\n",
      "\tMean_test_accuracy: 0.7540 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 415 \tTotal: 5317.0940 \tMean_train_accuracy: 0.8084\n",
      "\tMean_test_accuracy: 0.7590 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 416 \tTotal: 5269.5661 \tMean_train_accuracy: 0.8060\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 417 \tTotal: 5233.9815 \tMean_train_accuracy: 0.8084\n",
      "\tMean_test_accuracy: 0.7530 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 418 \tTotal: 5227.4014 \tMean_train_accuracy: 0.8059\n",
      "\tMean_test_accuracy: 0.7840 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 419 \tTotal: 5229.2230 \tMean_train_accuracy: 0.8074\n",
      "\tMean_test_accuracy: 0.7520 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 420 \tTotal: 5220.0982 \tMean_train_accuracy: 0.8011\n",
      "\tMean_test_accuracy: 0.7760 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 421 \tTotal: 5247.4988 \tMean_train_accuracy: 0.8063\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 422 \tTotal: 5230.6346 \tMean_train_accuracy: 0.8113\n",
      "\tMean_test_accuracy: 0.7770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 423 \tTotal: 5268.2449 \tMean_train_accuracy: 0.8108\n",
      "\tMean_test_accuracy: 0.7780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 424 \tTotal: 5246.5437 \tMean_train_accuracy: 0.8103\n",
      "\tMean_test_accuracy: 0.7640 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 425 \tTotal: 5226.3215 \tMean_train_accuracy: 0.8154\n",
      "\tMean_test_accuracy: 0.7690 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 426 \tTotal: 5222.9552 \tMean_train_accuracy: 0.8114\n",
      "\tMean_test_accuracy: 0.7820 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 427 \tTotal: 5231.7372 \tMean_train_accuracy: 0.8079\n",
      "\tMean_test_accuracy: 0.7840 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 428 \tTotal: 5240.2766 \tMean_train_accuracy: 0.8046\n",
      "\tMean_test_accuracy: 0.7780 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 429 \tTotal: 5210.2275 \tMean_train_accuracy: 0.8085\n",
      "\tMean_test_accuracy: 0.7750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 430 \tTotal: 5299.0215 \tMean_train_accuracy: 0.8073\n",
      "\tMean_test_accuracy: 0.7980 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 431 \tTotal: 5226.8485 \tMean_train_accuracy: 0.8074\n",
      "\tMean_test_accuracy: 0.7770 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 432 \tTotal: 5249.5676 \tMean_train_accuracy: 0.8066\n",
      "\tMean_test_accuracy: 0.7870 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 433 \tTotal: 5207.0468 \tMean_train_accuracy: 0.8088\n",
      "\tMean_test_accuracy: 0.7850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 434 \tTotal: 5202.9597 \tMean_train_accuracy: 0.8108\n",
      "\tMean_test_accuracy: 0.7720 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 435 \tTotal: 5192.1409 \tMean_train_accuracy: 0.8090\n",
      "\tMean_test_accuracy: 0.7910 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 436 \tTotal: 5213.4195 \tMean_train_accuracy: 0.8074\n",
      "\tMean_test_accuracy: 0.7630 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 437 \tTotal: 5204.6691 \tMean_train_accuracy: 0.8057\n",
      "\tMean_test_accuracy: 0.7850 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 438 \tTotal: 5181.2195 \tMean_train_accuracy: 0.8127\n",
      "\tMean_test_accuracy: 0.7750 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 439 \tTotal: 5187.0442 \tMean_train_accuracy: 0.8183\n",
      "\tMean_test_accuracy: 0.7740 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 440 \tTotal: 5208.6760 \tMean_train_accuracy: 0.8132\n",
      "\tMean_test_accuracy: 0.7900 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 441 \tTotal: 5199.6759 \tMean_train_accuracy: 0.8109\n",
      "\tMean_test_accuracy: 0.7480 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 442 \tTotal: 5190.5480 \tMean_train_accuracy: 0.8128\n",
      "\tMean_test_accuracy: 0.7820 \tMean_test_accuracy_sum: 0.4980\n",
      "Epoch: 443 \tTotal: 5191.4395 \tMean_train_accuracy: 0.8110\n",
      "\tMean_test_accuracy: 0.7860 \tMean_test_accuracy_sum: 0.4980\n"
     ]
    }
   ],
   "source": [
    "edge_index = edge_index.to(device)\n",
    "top_num = 500\n",
    "encoder = vae_model.Encoder\n",
    "\n",
    "for epoch in range(2000):\n",
    "\n",
    "    total_overall = 0\n",
    "    forward_loss = 0\n",
    "\n",
    "    mean_train_accuracy = 0\n",
    "    for batch_idx, x_feature_label in enumerate(train_loader):  \n",
    "        x =  x_feature_label[0].to(device)     \n",
    "        features = x_feature_label[1].to(device)\n",
    "        labels = x_feature_label[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        train_accuracy = 0\n",
    "        for i, feat_i in enumerate(features):\n",
    "            \n",
    "            # log_sum = torch.sum(torch.log(1 - x_i), dim=1, keepdim=True)\n",
    "            # feat = 1 - torch.exp(log_sum)\n",
    "            x_i = x[i]\n",
    "            x_i = encoder(x_i).detach()\n",
    "            x_i = normalize_features(x_i)\n",
    "            x_i = x_i.expand(features.shape[1], -1)\n",
    "            # final_feat_i =  torch.cat((feat_i, x_i), dim=1)\n",
    "            \n",
    "            \n",
    "            y_i = labels[i]\n",
    "            y_hat = forward_model(feat_i, x_i, edge_index)\n",
    "            _, top_indices_true = torch.topk(y_i.clone(), top_num)\n",
    "            label_2 = torch.zeros(y_i.shape).to(device)\n",
    "            label_2[top_indices_true] = 1\n",
    "            \n",
    "            _, top_indices_predict = torch.topk(y_hat.clone().squeeze(-1), top_num)\n",
    "            \n",
    "            # 将张量数组转换为Python列表\n",
    "            list1 = top_indices_true.tolist()\n",
    "            list_pre = top_indices_predict.tolist()\n",
    "\n",
    "            # 使用集合操作找到交集\n",
    "            intersection = list(set(list1) & set(list_pre))\n",
    "            accuracy_i = len(intersection) / top_num       \n",
    "            train_accuracy += accuracy_i \n",
    "\n",
    "            forward_loss = 0.5*F.mse_loss(y_hat.squeeze(-1), y_i, reduction='sum') + F.mse_loss(y_hat.squeeze(-1), label_2, reduction='sum')    \n",
    "            loss += forward_loss    \n",
    "        \n",
    "        total_overall += loss.item()    \n",
    "        train_accuracy /= len(features)\n",
    "        mean_train_accuracy = train_accuracy\n",
    "        loss = loss/features.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # for p in forward_model.parameters():\n",
    "        #     p.data.clamp_(min=0)\n",
    "        \n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTotal: {:.4f}\".format(total_overall / train_batch_size),\n",
    "        \"\\tMean_train_accuracy: {:.4f}\".format(mean_train_accuracy),\n",
    "        )  \n",
    "    \n",
    "    mean_accuracy = 0\n",
    "    mean_accuracy_sum = 0\n",
    "\n",
    "    \n",
    "    for batch_idx, x_feature_label in enumerate(test_loader):  \n",
    "        x =  x_feature_label[0].to(device)\n",
    "        features = x_feature_label[1].to(device)\n",
    "        labels = x_feature_label[2].to(device)\n",
    "        \n",
    "        accuracy = 0\n",
    "        accuracy_sum = 0\n",
    "        \n",
    "        for i, feat_i in enumerate(features):\n",
    "            x_i = x[i]\n",
    "            x_i = encoder(x_i).detach()\n",
    "            x_i = normalize_features(x_i)\n",
    "            x_i = x_i.expand(features.shape[1], -1)\n",
    "            # final_feat_i =  torch.cat((feat_i, x_i), dim=1)\n",
    "            y_i = labels[i]\n",
    "            _, top_indices_true = torch.topk(y_i, top_num)\n",
    "            \n",
    "            y_hat = forward_model(feat_i, x_i, edge_index)\n",
    "            \n",
    "            _, top_indices_predict = torch.topk(y_hat.squeeze(-1), top_num)\n",
    "            \n",
    "            sum_pre = torch.sum(feat_i, dim=1, keepdim=True)\n",
    "            _, top_indices_sum = torch.topk(sum_pre.squeeze(-1), top_num)\n",
    "            \n",
    "            # 将张量数组转换为Python列表\n",
    "            list1 = top_indices_true.tolist()\n",
    "            list_pre = top_indices_predict.tolist()\n",
    "            \n",
    "            list_sum = top_indices_sum.tolist()\n",
    "\n",
    "            # 使用集合操作找到交集\n",
    "            intersection = list(set(list1) & set(list_pre))\n",
    "            \n",
    "            intersection_sum = list(set(list1) & set(list_sum))\n",
    "            \n",
    "            accuracy_i = len(intersection) / top_num       \n",
    "            accuracy += accuracy_i \n",
    "            accuracy_sum += len(intersection_sum) / top_num  \n",
    "        accuracy /= test_batch_size\n",
    "        accuracy_sum/= test_batch_size\n",
    "        mean_accuracy = accuracy\n",
    "        mean_accuracy_sum = accuracy_sum\n",
    "        break\n",
    "    \n",
    "    print(\n",
    "        \"\\tMean_test_accuracy: {:.4f}\".format(mean_accuracy),\n",
    "        \"\\tMean_test_accuracy_sum: {:.4f}\".format(mean_accuracy_sum)\n",
    "        )  \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
