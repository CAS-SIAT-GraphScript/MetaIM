{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/zjy/project/MetaIM')\n",
    "pwd = '/home/zjy/project/MetaIM/data'\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "cora_dataset = Planetoid(root=pwd+'/cora', name='cora')\n",
    "data = cora_dataset[0]\n",
    "edge_index = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), (1000, 2, 2708))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "individual_infection_path = pwd+'/for_meta/cora_individual_infection_sir_200_sample_500.npy'\n",
    "seeds_infection_path = pwd+'/for_meta/cora_seed_infection_sir_200_sample_500.npy'\n",
    "\n",
    "individual_infection = np.load(individual_infection_path)\n",
    "seeds_infection = np.load(seeds_infection_path)\n",
    "individual_infection.shape,seeds_infection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "                       [ 633, 1862, 2582,  ...,  598, 1473, 2706]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(2708, 2708), nnz=10556, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 转换为 scipy 稀疏矩阵\n",
    "adj = to_scipy_sparse_matrix(edge_index)\n",
    "\n",
    "\n",
    "# def normalize_adj(mx):\n",
    "#     \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "#     rowsum = np.array(mx.sum(1))\n",
    "#     r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "#     r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "#     r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "#     return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "\n",
    "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "# adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "adj = torch.Tensor(adj.toarray()).to_sparse()\n",
    "adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_num = int(seeds_infection[0][0].sum())\n",
    "seed_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, individual_infection,seeds_infection, seed_num):\n",
    "        self.individual_infection = individual_infection\n",
    "        self.seeds_infection = seeds_infection\n",
    "        self.feat_shape = (len(individual_infection), seed_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seeds_infection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeds= np.nonzero(self.seeds_infection[idx][0])[0]\n",
    "        \n",
    "        feature = torch.zeros(self.feat_shape[0],self.feat_shape[1])\n",
    "        for i in range(len(seeds)):\n",
    "            seed_i_infection = torch.tensor(self.individual_infection[seeds[i]])\n",
    "            feature[:, i] = seed_i_infection\n",
    "            \n",
    "        label = self.seeds_infection[idx][1]\n",
    "        \n",
    "        return self.seeds_infection[idx][0], feature, label\n",
    "\n",
    "dataset = CustomDataset(individual_infection, seeds_infection, seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义划分比例\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# 划分数据集\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*train_ratio), int(len(dataset)*test_ratio)])\n",
    "\n",
    "train_batch_size = 32\n",
    "test_batch_size = 2\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAEModel(\n",
       "  (Encoder): Encoder(\n",
       "    (FC_input): Linear(in_features=2708, out_features=1024, bias=True)\n",
       "    (FC_input2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (FC_output): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (FC_input): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (FC_hidden_1): Linear(in_features=64, out_features=1024, bias=True)\n",
       "    (FC_hidden_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (FC_output): Linear(in_features=1024, out_features=2708, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data import model \n",
    "from data.model.model import VAEModel, Encoder, Decoder\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # hidden_dim = 256\n",
    "# # latent_dim = 64\n",
    "hidden_dim = 1024\n",
    "latent_dim = 64\n",
    "\n",
    "encoder = Encoder(input_dim= len(seeds_infection[0][0]), \n",
    "                  hidden_dim=hidden_dim, \n",
    "                  latent_dim=latent_dim)\n",
    "\n",
    "decoder = Decoder(input_dim=latent_dim, \n",
    "                  latent_dim=latent_dim, \n",
    "                  hidden_dim=hidden_dim, \n",
    "                  output_dim=len(seeds_infection[0][0]))\n",
    "\n",
    "vae_model = VAEModel(Encoder=encoder, Decoder=decoder).to(device)\n",
    "\n",
    "optimizer_vae = Adam([{'params': vae_model.parameters()}], \n",
    "                 lr=1e-3)\n",
    "vae_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain_vae_loss: 20562.0610\n",
      "Epoch: 2 \tTrain_vae_loss: 13939.1168\n",
      "Epoch: 3 \tTrain_vae_loss: 13588.1479\n",
      "Epoch: 4 \tTrain_vae_loss: 13533.8307\n",
      "Epoch: 5 \tTrain_vae_loss: 13521.9241\n",
      "Epoch: 6 \tTrain_vae_loss: 13519.0553\n",
      "Epoch: 7 \tTrain_vae_loss: 13507.5110\n",
      "Epoch: 8 \tTrain_vae_loss: 13498.6711\n",
      "Epoch: 9 \tTrain_vae_loss: 13498.5120\n",
      "Epoch: 10 \tTrain_vae_loss: 13490.3065\n",
      "Epoch: 11 \tTrain_vae_loss: 13484.9831\n",
      "Epoch: 12 \tTrain_vae_loss: 13483.9281\n",
      "Epoch: 13 \tTrain_vae_loss: 13482.9317\n",
      "Epoch: 14 \tTrain_vae_loss: 13479.2623\n",
      "Epoch: 15 \tTrain_vae_loss: 13475.7062\n",
      "Epoch: 16 \tTrain_vae_loss: 13471.7580\n",
      "Epoch: 17 \tTrain_vae_loss: 13470.4894\n",
      "Epoch: 18 \tTrain_vae_loss: 13471.1375\n",
      "Epoch: 19 \tTrain_vae_loss: 13465.9470\n",
      "Epoch: 20 \tTrain_vae_loss: 13466.6287\n",
      "Epoch: 21 \tTrain_vae_loss: 13465.9558\n",
      "Epoch: 22 \tTrain_vae_loss: 13458.6444\n",
      "Epoch: 23 \tTrain_vae_loss: 13456.5239\n",
      "Epoch: 24 \tTrain_vae_loss: 13458.7015\n",
      "Epoch: 25 \tTrain_vae_loss: 13451.9376\n",
      "Epoch: 26 \tTrain_vae_loss: 13452.8068\n",
      "Epoch: 27 \tTrain_vae_loss: 13452.7919\n",
      "Epoch: 28 \tTrain_vae_loss: 13450.4070\n",
      "Epoch: 29 \tTrain_vae_loss: 13447.9791\n",
      "Epoch: 30 \tTrain_vae_loss: 13446.8503\n",
      "Epoch: 31 \tTrain_vae_loss: 13447.6439\n",
      "Epoch: 32 \tTrain_vae_loss: 13444.4487\n",
      "Epoch: 33 \tTrain_vae_loss: 13443.9184\n",
      "Epoch: 34 \tTrain_vae_loss: 13442.8148\n",
      "Epoch: 35 \tTrain_vae_loss: 13442.8395\n",
      "Epoch: 36 \tTrain_vae_loss: 13443.2214\n",
      "Epoch: 37 \tTrain_vae_loss: 13442.6301\n",
      "Epoch: 38 \tTrain_vae_loss: 13439.1588\n",
      "Epoch: 39 \tTrain_vae_loss: 13430.9186\n",
      "Epoch: 40 \tTrain_vae_loss: 13414.1073\n",
      "Epoch: 41 \tTrain_vae_loss: 13395.8854\n",
      "Epoch: 42 \tTrain_vae_loss: 13386.5075\n",
      "Epoch: 43 \tTrain_vae_loss: 13376.5463\n",
      "Epoch: 44 \tTrain_vae_loss: 13367.4058\n",
      "Epoch: 45 \tTrain_vae_loss: 13360.3953\n",
      "Epoch: 46 \tTrain_vae_loss: 13353.2638\n",
      "Epoch: 47 \tTrain_vae_loss: 13343.1945\n",
      "Epoch: 48 \tTrain_vae_loss: 13332.9256\n",
      "Epoch: 49 \tTrain_vae_loss: 13325.7050\n",
      "Epoch: 50 \tTrain_vae_loss: 13316.9263\n",
      "Epoch: 51 \tTrain_vae_loss: 13307.8048\n",
      "Epoch: 52 \tTrain_vae_loss: 13305.6307\n",
      "Epoch: 53 \tTrain_vae_loss: 13299.7001\n",
      "Epoch: 54 \tTrain_vae_loss: 13288.1371\n",
      "Epoch: 55 \tTrain_vae_loss: 13274.6889\n",
      "Epoch: 56 \tTrain_vae_loss: 13262.7957\n",
      "Epoch: 57 \tTrain_vae_loss: 13257.5885\n",
      "Epoch: 58 \tTrain_vae_loss: 13244.6125\n",
      "Epoch: 59 \tTrain_vae_loss: 13221.8337\n",
      "Epoch: 60 \tTrain_vae_loss: 13201.0422\n",
      "Epoch: 61 \tTrain_vae_loss: 13178.1283\n",
      "Epoch: 62 \tTrain_vae_loss: 13158.6404\n",
      "Epoch: 63 \tTrain_vae_loss: 13149.6042\n",
      "Epoch: 64 \tTrain_vae_loss: 13147.2081\n",
      "Epoch: 65 \tTrain_vae_loss: 13118.9604\n",
      "Epoch: 66 \tTrain_vae_loss: 13078.2396\n",
      "Epoch: 67 \tTrain_vae_loss: 13061.6442\n",
      "Epoch: 68 \tTrain_vae_loss: 13041.2194\n",
      "Epoch: 69 \tTrain_vae_loss: 13012.1421\n",
      "Epoch: 70 \tTrain_vae_loss: 12982.6704\n",
      "Epoch: 71 \tTrain_vae_loss: 12948.5416\n",
      "Epoch: 72 \tTrain_vae_loss: 12902.0645\n",
      "Epoch: 73 \tTrain_vae_loss: 12880.7080\n",
      "Epoch: 74 \tTrain_vae_loss: 12833.5254\n",
      "Epoch: 75 \tTrain_vae_loss: 12812.6325\n",
      "Epoch: 76 \tTrain_vae_loss: 12763.9644\n",
      "Epoch: 77 \tTrain_vae_loss: 12711.8725\n",
      "Epoch: 78 \tTrain_vae_loss: 12651.1700\n",
      "Epoch: 79 \tTrain_vae_loss: 12609.0937\n",
      "Epoch: 80 \tTrain_vae_loss: 12592.3481\n",
      "Epoch: 81 \tTrain_vae_loss: 12505.9733\n",
      "Epoch: 82 \tTrain_vae_loss: 12429.0745\n",
      "Epoch: 83 \tTrain_vae_loss: 12390.0769\n",
      "Epoch: 84 \tTrain_vae_loss: 12341.4740\n",
      "Epoch: 85 \tTrain_vae_loss: 12221.9453\n",
      "Epoch: 86 \tTrain_vae_loss: 12107.3169\n",
      "Epoch: 87 \tTrain_vae_loss: 12068.9981\n",
      "Epoch: 88 \tTrain_vae_loss: 11983.1399\n",
      "Epoch: 89 \tTrain_vae_loss: 11851.2137\n",
      "Epoch: 90 \tTrain_vae_loss: 11724.5631\n",
      "Epoch: 91 \tTrain_vae_loss: 11627.3676\n",
      "Epoch: 92 \tTrain_vae_loss: 11507.3310\n",
      "Epoch: 93 \tTrain_vae_loss: 11349.8189\n",
      "Epoch: 94 \tTrain_vae_loss: 11301.0739\n",
      "Epoch: 95 \tTrain_vae_loss: 11197.3492\n",
      "Epoch: 96 \tTrain_vae_loss: 11145.3770\n",
      "Epoch: 97 \tTrain_vae_loss: 10982.7445\n",
      "Epoch: 98 \tTrain_vae_loss: 10761.0173\n",
      "Epoch: 99 \tTrain_vae_loss: 10675.8966\n",
      "Epoch: 100 \tTrain_vae_loss: 10441.3680\n",
      "Epoch: 101 \tTrain_vae_loss: 10263.1556\n",
      "Epoch: 102 \tTrain_vae_loss: 10027.5116\n",
      "Epoch: 103 \tTrain_vae_loss: 9836.2238\n",
      "Epoch: 104 \tTrain_vae_loss: 9747.1284\n",
      "Epoch: 105 \tTrain_vae_loss: 9662.9908\n",
      "Epoch: 106 \tTrain_vae_loss: 9516.0330\n",
      "Epoch: 107 \tTrain_vae_loss: 9282.0757\n",
      "Epoch: 108 \tTrain_vae_loss: 9056.5718\n",
      "Epoch: 109 \tTrain_vae_loss: 8834.3216\n",
      "Epoch: 110 \tTrain_vae_loss: 8667.1102\n",
      "Epoch: 111 \tTrain_vae_loss: 8499.9380\n",
      "Epoch: 112 \tTrain_vae_loss: 8137.9747\n",
      "Epoch: 113 \tTrain_vae_loss: 7902.1191\n",
      "Epoch: 114 \tTrain_vae_loss: 7610.8493\n",
      "Epoch: 115 \tTrain_vae_loss: 7399.1219\n",
      "Epoch: 116 \tTrain_vae_loss: 7294.4375\n",
      "Epoch: 117 \tTrain_vae_loss: 7213.3938\n",
      "Epoch: 118 \tTrain_vae_loss: 6942.4481\n",
      "Epoch: 119 \tTrain_vae_loss: 6624.2134\n",
      "Epoch: 120 \tTrain_vae_loss: 6449.2790\n",
      "Epoch: 121 \tTrain_vae_loss: 6358.2718\n",
      "Epoch: 122 \tTrain_vae_loss: 6075.2674\n",
      "Epoch: 123 \tTrain_vae_loss: 5728.2106\n",
      "Epoch: 124 \tTrain_vae_loss: 5406.1541\n",
      "Epoch: 125 \tTrain_vae_loss: 5065.8835\n",
      "Epoch: 126 \tTrain_vae_loss: 4901.7624\n",
      "Epoch: 127 \tTrain_vae_loss: 4878.6513\n",
      "Epoch: 128 \tTrain_vae_loss: 4940.1921\n",
      "Epoch: 129 \tTrain_vae_loss: 4964.2376\n",
      "Epoch: 130 \tTrain_vae_loss: 4709.9830\n",
      "Epoch: 131 \tTrain_vae_loss: 4487.0312\n",
      "Epoch: 132 \tTrain_vae_loss: 4013.0974\n",
      "Epoch: 133 \tTrain_vae_loss: 3628.9460\n",
      "Epoch: 134 \tTrain_vae_loss: 3288.6021\n",
      "Epoch: 135 \tTrain_vae_loss: 3034.8831\n",
      "Epoch: 136 \tTrain_vae_loss: 2822.6702\n",
      "Epoch: 137 \tTrain_vae_loss: 2726.0215\n",
      "Epoch: 138 \tTrain_vae_loss: 2491.7715\n",
      "Epoch: 139 \tTrain_vae_loss: 2304.8358\n",
      "Epoch: 140 \tTrain_vae_loss: 2265.9071\n",
      "Epoch: 141 \tTrain_vae_loss: 2073.3896\n",
      "Epoch: 142 \tTrain_vae_loss: 1842.2366\n",
      "Epoch: 143 \tTrain_vae_loss: 1614.6438\n",
      "Epoch: 144 \tTrain_vae_loss: 1474.9305\n",
      "Epoch: 145 \tTrain_vae_loss: 1351.5280\n",
      "Epoch: 146 \tTrain_vae_loss: 1227.3656\n",
      "Epoch: 147 \tTrain_vae_loss: 1310.4969\n",
      "Epoch: 148 \tTrain_vae_loss: 1343.7653\n",
      "Epoch: 149 \tTrain_vae_loss: 1401.2281\n",
      "Epoch: 150 \tTrain_vae_loss: 1530.1896\n",
      "Epoch: 151 \tTrain_vae_loss: 1627.5770\n",
      "Epoch: 152 \tTrain_vae_loss: 1927.3350\n",
      "Epoch: 153 \tTrain_vae_loss: 2303.6164\n",
      "Epoch: 154 \tTrain_vae_loss: 2593.9729\n",
      "Epoch: 155 \tTrain_vae_loss: 2757.5779\n",
      "Epoch: 156 \tTrain_vae_loss: 2982.2700\n",
      "Epoch: 157 \tTrain_vae_loss: 2519.4115\n",
      "Epoch: 158 \tTrain_vae_loss: 1751.2640\n",
      "Epoch: 159 \tTrain_vae_loss: 1180.9260\n",
      "Epoch: 160 \tTrain_vae_loss: 783.7581\n",
      "Epoch: 161 \tTrain_vae_loss: 459.8319\n",
      "Epoch: 162 \tTrain_vae_loss: 241.3137\n",
      "Epoch: 163 \tTrain_vae_loss: 150.4976\n",
      "Epoch: 164 \tTrain_vae_loss: 100.1522\n",
      "Epoch: 165 \tTrain_vae_loss: 73.6708\n",
      "Epoch: 166 \tTrain_vae_loss: 62.9383\n",
      "Epoch: 167 \tTrain_vae_loss: 64.1700\n",
      "Epoch: 168 \tTrain_vae_loss: 91.0156\n",
      "Epoch: 169 \tTrain_vae_loss: 184.3213\n",
      "Epoch: 170 \tTrain_vae_loss: 324.1966\n",
      "Epoch: 171 \tTrain_vae_loss: 657.7150\n",
      "Epoch: 172 \tTrain_vae_loss: 880.2845\n",
      "Epoch: 173 \tTrain_vae_loss: 821.4779\n",
      "Epoch: 174 \tTrain_vae_loss: 621.5233\n",
      "Epoch: 175 \tTrain_vae_loss: 380.4250\n",
      "Epoch: 176 \tTrain_vae_loss: 190.0028\n",
      "Epoch: 177 \tTrain_vae_loss: 96.8929\n",
      "Epoch: 178 \tTrain_vae_loss: 44.1492\n",
      "Epoch: 179 \tTrain_vae_loss: 22.5328\n",
      "Epoch: 180 \tTrain_vae_loss: 15.1298\n",
      "Epoch: 181 \tTrain_vae_loss: 11.2611\n",
      "Epoch: 182 \tTrain_vae_loss: 9.2260\n",
      "Epoch: 183 \tTrain_vae_loss: 7.9638\n",
      "Epoch: 184 \tTrain_vae_loss: 6.9905\n",
      "Epoch: 185 \tTrain_vae_loss: 6.2461\n",
      "Epoch: 186 \tTrain_vae_loss: 5.6274\n",
      "Epoch: 187 \tTrain_vae_loss: 5.0753\n",
      "Epoch: 188 \tTrain_vae_loss: 4.6411\n",
      "Epoch: 189 \tTrain_vae_loss: 4.2712\n",
      "Epoch: 190 \tTrain_vae_loss: 3.9207\n",
      "Epoch: 191 \tTrain_vae_loss: 3.6392\n",
      "Epoch: 192 \tTrain_vae_loss: 3.3868\n",
      "Epoch: 193 \tTrain_vae_loss: 3.1445\n",
      "Epoch: 194 \tTrain_vae_loss: 2.9456\n",
      "Epoch: 195 \tTrain_vae_loss: 2.7635\n",
      "Epoch: 196 \tTrain_vae_loss: 2.5985\n",
      "Epoch: 197 \tTrain_vae_loss: 2.4402\n",
      "Epoch: 198 \tTrain_vae_loss: 2.3170\n",
      "Epoch: 199 \tTrain_vae_loss: 2.1861\n",
      "Epoch: 200 \tTrain_vae_loss: 2.0709\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    train_vae_loss = 0\n",
    "    mean_train_accuracy = 0\n",
    "    for batch_idx, x_feature_label in enumerate(train_loader):        \n",
    "        x = x_feature_label[0].to(device)\n",
    "        optimizer_vae.zero_grad()\n",
    "        loss = 0\n",
    "        for i, x_i in enumerate(x):\n",
    "            x_hat = vae_model(x_i)\n",
    "\n",
    "            reproduction_loss = F.binary_cross_entropy(x_hat, x_i, reduction='sum')   \n",
    "            loss += reproduction_loss    \n",
    "        train_vae_loss += loss.item()\n",
    "        loss = loss/x.size(0)\n",
    "        loss.backward()\n",
    "        optimizer_vae.step()\n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTrain_vae_loss: {:.4f}\".format(train_vae_loss / train_batch_size),\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain_vae_loss: 1.9633\n",
      "Epoch: 2 \tTrain_vae_loss: 1.8659\n",
      "Epoch: 3 \tTrain_vae_loss: 1.7771\n",
      "Epoch: 4 \tTrain_vae_loss: 1.6906\n",
      "Epoch: 5 \tTrain_vae_loss: 1.6172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTrain_vae_loss: 1.5435\n",
      "Epoch: 7 \tTrain_vae_loss: 1.4752\n",
      "Epoch: 8 \tTrain_vae_loss: 1.4123\n",
      "Epoch: 9 \tTrain_vae_loss: 1.3536\n",
      "Epoch: 10 \tTrain_vae_loss: 1.2973\n",
      "Epoch: 11 \tTrain_vae_loss: 1.2444\n",
      "Epoch: 12 \tTrain_vae_loss: 1.1956\n",
      "Epoch: 13 \tTrain_vae_loss: 1.1496\n",
      "Epoch: 14 \tTrain_vae_loss: 1.1054\n",
      "Epoch: 15 \tTrain_vae_loss: 1.0649\n",
      "Epoch: 16 \tTrain_vae_loss: 1.0251\n",
      "Epoch: 17 \tTrain_vae_loss: 0.9888\n",
      "Epoch: 18 \tTrain_vae_loss: 0.9537\n",
      "Epoch: 19 \tTrain_vae_loss: 0.9206\n",
      "Epoch: 20 \tTrain_vae_loss: 0.8889\n",
      "Epoch: 21 \tTrain_vae_loss: 0.8587\n",
      "Epoch: 22 \tTrain_vae_loss: 0.8302\n",
      "Epoch: 23 \tTrain_vae_loss: 0.8029\n",
      "Epoch: 24 \tTrain_vae_loss: 0.7764\n",
      "Epoch: 25 \tTrain_vae_loss: 0.7511\n",
      "Epoch: 26 \tTrain_vae_loss: 0.7271\n",
      "Epoch: 27 \tTrain_vae_loss: 0.7046\n",
      "Epoch: 28 \tTrain_vae_loss: 0.6839\n",
      "Epoch: 29 \tTrain_vae_loss: 0.6619\n",
      "Epoch: 30 \tTrain_vae_loss: 0.6419\n",
      "Epoch: 31 \tTrain_vae_loss: 0.6224\n",
      "Epoch: 32 \tTrain_vae_loss: 0.6043\n",
      "Epoch: 33 \tTrain_vae_loss: 0.5866\n",
      "Epoch: 34 \tTrain_vae_loss: 0.5696\n",
      "Epoch: 35 \tTrain_vae_loss: 0.5533\n",
      "Epoch: 36 \tTrain_vae_loss: 0.5374\n",
      "Epoch: 37 \tTrain_vae_loss: 0.5228\n",
      "Epoch: 38 \tTrain_vae_loss: 0.5079\n",
      "Epoch: 39 \tTrain_vae_loss: 0.4940\n",
      "Epoch: 40 \tTrain_vae_loss: 0.4805\n",
      "Epoch: 41 \tTrain_vae_loss: 0.4674\n",
      "Epoch: 42 \tTrain_vae_loss: 0.4550\n",
      "Epoch: 43 \tTrain_vae_loss: 0.4430\n",
      "Epoch: 44 \tTrain_vae_loss: 0.4312\n",
      "Epoch: 45 \tTrain_vae_loss: 0.4199\n",
      "Epoch: 46 \tTrain_vae_loss: 0.4090\n",
      "Epoch: 47 \tTrain_vae_loss: 0.3986\n",
      "Epoch: 48 \tTrain_vae_loss: 0.3885\n",
      "Epoch: 49 \tTrain_vae_loss: 0.3786\n",
      "Epoch: 50 \tTrain_vae_loss: 0.3689\n",
      "Epoch: 51 \tTrain_vae_loss: 0.3601\n",
      "Epoch: 52 \tTrain_vae_loss: 0.3512\n",
      "Epoch: 53 \tTrain_vae_loss: 0.3423\n",
      "Epoch: 54 \tTrain_vae_loss: 0.3340\n",
      "Epoch: 55 \tTrain_vae_loss: 0.3259\n",
      "Epoch: 56 \tTrain_vae_loss: 0.3181\n",
      "Epoch: 57 \tTrain_vae_loss: 0.3106\n",
      "Epoch: 58 \tTrain_vae_loss: 0.3031\n",
      "Epoch: 59 \tTrain_vae_loss: 0.2960\n",
      "Epoch: 60 \tTrain_vae_loss: 0.2890\n",
      "Epoch: 61 \tTrain_vae_loss: 0.2823\n",
      "Epoch: 62 \tTrain_vae_loss: 0.2757\n",
      "Epoch: 63 \tTrain_vae_loss: 0.2696\n",
      "Epoch: 64 \tTrain_vae_loss: 0.2633\n",
      "Epoch: 65 \tTrain_vae_loss: 0.2574\n",
      "Epoch: 66 \tTrain_vae_loss: 0.2516\n",
      "Epoch: 67 \tTrain_vae_loss: 0.2461\n",
      "Epoch: 68 \tTrain_vae_loss: 0.2403\n",
      "Epoch: 69 \tTrain_vae_loss: 0.2351\n",
      "Epoch: 70 \tTrain_vae_loss: 0.2300\n",
      "Epoch: 71 \tTrain_vae_loss: 0.2249\n",
      "Epoch: 72 \tTrain_vae_loss: 0.2199\n",
      "Epoch: 73 \tTrain_vae_loss: 0.2152\n",
      "Epoch: 74 \tTrain_vae_loss: 0.2105\n",
      "Epoch: 75 \tTrain_vae_loss: 0.2059\n",
      "Epoch: 76 \tTrain_vae_loss: 0.2016\n",
      "Epoch: 77 \tTrain_vae_loss: 0.1973\n",
      "Epoch: 78 \tTrain_vae_loss: 0.1931\n",
      "Epoch: 79 \tTrain_vae_loss: 0.1890\n",
      "Epoch: 80 \tTrain_vae_loss: 0.1850\n",
      "Epoch: 81 \tTrain_vae_loss: 0.1812\n",
      "Epoch: 82 \tTrain_vae_loss: 0.1774\n",
      "Epoch: 83 \tTrain_vae_loss: 0.1738\n",
      "Epoch: 84 \tTrain_vae_loss: 0.1702\n",
      "Epoch: 85 \tTrain_vae_loss: 0.1667\n",
      "Epoch: 86 \tTrain_vae_loss: 0.1632\n",
      "Epoch: 87 \tTrain_vae_loss: 0.1598\n",
      "Epoch: 88 \tTrain_vae_loss: 0.1566\n",
      "Epoch: 89 \tTrain_vae_loss: 0.1534\n",
      "Epoch: 90 \tTrain_vae_loss: 0.1504\n",
      "Epoch: 91 \tTrain_vae_loss: 0.1474\n",
      "Epoch: 92 \tTrain_vae_loss: 0.1444\n",
      "Epoch: 93 \tTrain_vae_loss: 0.1415\n",
      "Epoch: 94 \tTrain_vae_loss: 0.1387\n",
      "Epoch: 95 \tTrain_vae_loss: 0.1360\n",
      "Epoch: 96 \tTrain_vae_loss: 0.1333\n",
      "Epoch: 97 \tTrain_vae_loss: 0.1307\n",
      "Epoch: 98 \tTrain_vae_loss: 0.1281\n",
      "Epoch: 99 \tTrain_vae_loss: 0.1257\n",
      "Epoch: 100 \tTrain_vae_loss: 0.1231\n",
      "Epoch: 101 \tTrain_vae_loss: 0.1208\n",
      "Epoch: 102 \tTrain_vae_loss: 0.1185\n",
      "Epoch: 103 \tTrain_vae_loss: 0.1161\n",
      "Epoch: 104 \tTrain_vae_loss: 0.1139\n",
      "Epoch: 105 \tTrain_vae_loss: 0.1117\n",
      "Epoch: 106 \tTrain_vae_loss: 0.1096\n",
      "Epoch: 107 \tTrain_vae_loss: 0.1075\n",
      "Epoch: 108 \tTrain_vae_loss: 0.1055\n",
      "Epoch: 109 \tTrain_vae_loss: 0.1034\n",
      "Epoch: 110 \tTrain_vae_loss: 0.1015\n",
      "Epoch: 111 \tTrain_vae_loss: 0.0996\n",
      "Epoch: 112 \tTrain_vae_loss: 0.0977\n",
      "Epoch: 113 \tTrain_vae_loss: 0.0960\n",
      "Epoch: 114 \tTrain_vae_loss: 0.0941\n",
      "Epoch: 115 \tTrain_vae_loss: 0.0924\n",
      "Epoch: 116 \tTrain_vae_loss: 0.0907\n",
      "Epoch: 117 \tTrain_vae_loss: 0.0890\n",
      "Epoch: 118 \tTrain_vae_loss: 0.0874\n",
      "Epoch: 119 \tTrain_vae_loss: 0.0857\n",
      "Epoch: 120 \tTrain_vae_loss: 0.0842\n",
      "Epoch: 121 \tTrain_vae_loss: 0.0826\n",
      "Epoch: 122 \tTrain_vae_loss: 0.0811\n",
      "Epoch: 123 \tTrain_vae_loss: 0.0796\n",
      "Epoch: 124 \tTrain_vae_loss: 0.0782\n",
      "Epoch: 125 \tTrain_vae_loss: 0.0768\n",
      "Epoch: 126 \tTrain_vae_loss: 0.0754\n",
      "Epoch: 127 \tTrain_vae_loss: 0.0740\n",
      "Epoch: 128 \tTrain_vae_loss: 0.0727\n",
      "Epoch: 129 \tTrain_vae_loss: 0.0714\n",
      "Epoch: 130 \tTrain_vae_loss: 0.0701\n",
      "Epoch: 131 \tTrain_vae_loss: 0.0688\n",
      "Epoch: 132 \tTrain_vae_loss: 0.0676\n",
      "Epoch: 133 \tTrain_vae_loss: 0.0664\n",
      "Epoch: 134 \tTrain_vae_loss: 0.0652\n",
      "Epoch: 135 \tTrain_vae_loss: 0.0641\n",
      "Epoch: 136 \tTrain_vae_loss: 0.0629\n",
      "Epoch: 137 \tTrain_vae_loss: 0.0618\n",
      "Epoch: 138 \tTrain_vae_loss: 0.0607\n",
      "Epoch: 139 \tTrain_vae_loss: 0.0597\n",
      "Epoch: 140 \tTrain_vae_loss: 0.0586\n",
      "Epoch: 141 \tTrain_vae_loss: 0.0576\n",
      "Epoch: 142 \tTrain_vae_loss: 0.0566\n",
      "Epoch: 143 \tTrain_vae_loss: 0.0556\n",
      "Epoch: 144 \tTrain_vae_loss: 0.0546\n",
      "Epoch: 145 \tTrain_vae_loss: 0.0537\n",
      "Epoch: 146 \tTrain_vae_loss: 0.0527\n",
      "Epoch: 147 \tTrain_vae_loss: 0.0518\n",
      "Epoch: 148 \tTrain_vae_loss: 0.0509\n",
      "Epoch: 149 \tTrain_vae_loss: 0.0501\n",
      "Epoch: 150 \tTrain_vae_loss: 0.0492\n",
      "Epoch: 151 \tTrain_vae_loss: 0.0483\n",
      "Epoch: 152 \tTrain_vae_loss: 0.0475\n",
      "Epoch: 153 \tTrain_vae_loss: 0.0467\n",
      "Epoch: 154 \tTrain_vae_loss: 0.0459\n",
      "Epoch: 155 \tTrain_vae_loss: 0.0451\n",
      "Epoch: 156 \tTrain_vae_loss: 0.0443\n",
      "Epoch: 157 \tTrain_vae_loss: 0.0436\n",
      "Epoch: 158 \tTrain_vae_loss: 0.0428\n",
      "Epoch: 159 \tTrain_vae_loss: 0.0421\n",
      "Epoch: 160 \tTrain_vae_loss: 0.0414\n",
      "Epoch: 161 \tTrain_vae_loss: 0.0407\n",
      "Epoch: 162 \tTrain_vae_loss: 0.0400\n",
      "Epoch: 163 \tTrain_vae_loss: 0.0393\n",
      "Epoch: 164 \tTrain_vae_loss: 0.0386\n",
      "Epoch: 165 \tTrain_vae_loss: 0.0380\n",
      "Epoch: 166 \tTrain_vae_loss: 0.0374\n",
      "Epoch: 167 \tTrain_vae_loss: 0.0367\n",
      "Epoch: 168 \tTrain_vae_loss: 0.0361\n",
      "Epoch: 169 \tTrain_vae_loss: 0.0355\n",
      "Epoch: 170 \tTrain_vae_loss: 0.0349\n",
      "Epoch: 171 \tTrain_vae_loss: 0.0343\n",
      "Epoch: 172 \tTrain_vae_loss: 0.0338\n",
      "Epoch: 173 \tTrain_vae_loss: 0.0332\n",
      "Epoch: 174 \tTrain_vae_loss: 0.0326\n",
      "Epoch: 175 \tTrain_vae_loss: 0.0321\n",
      "Epoch: 176 \tTrain_vae_loss: 0.0315\n",
      "Epoch: 177 \tTrain_vae_loss: 0.0310\n",
      "Epoch: 178 \tTrain_vae_loss: 0.0305\n",
      "Epoch: 179 \tTrain_vae_loss: 0.0300\n",
      "Epoch: 180 \tTrain_vae_loss: 0.0295\n",
      "Epoch: 181 \tTrain_vae_loss: 0.0290\n",
      "Epoch: 182 \tTrain_vae_loss: 0.0285\n",
      "Epoch: 183 \tTrain_vae_loss: 0.0281\n",
      "Epoch: 184 \tTrain_vae_loss: 0.0276\n",
      "Epoch: 185 \tTrain_vae_loss: 0.0271\n",
      "Epoch: 186 \tTrain_vae_loss: 0.0267\n",
      "Epoch: 187 \tTrain_vae_loss: 0.0263\n",
      "Epoch: 188 \tTrain_vae_loss: 0.0258\n",
      "Epoch: 189 \tTrain_vae_loss: 0.0254\n",
      "Epoch: 190 \tTrain_vae_loss: 0.0250\n",
      "Epoch: 191 \tTrain_vae_loss: 0.0246\n",
      "Epoch: 192 \tTrain_vae_loss: 0.0242\n",
      "Epoch: 193 \tTrain_vae_loss: 0.0238\n",
      "Epoch: 194 \tTrain_vae_loss: 0.0234\n",
      "Epoch: 195 \tTrain_vae_loss: 0.0230\n",
      "Epoch: 196 \tTrain_vae_loss: 0.0226\n",
      "Epoch: 197 \tTrain_vae_loss: 0.0223\n",
      "Epoch: 198 \tTrain_vae_loss: 0.0219\n",
      "Epoch: 199 \tTrain_vae_loss: 0.0215\n",
      "Epoch: 200 \tTrain_vae_loss: 0.0212\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    train_vae_loss = 0\n",
    "    mean_train_accuracy = 0\n",
    "    for batch_idx, x_feature_label in enumerate(train_loader):        \n",
    "        x = x_feature_label[0].to(device)\n",
    "        optimizer_vae.zero_grad()\n",
    "        loss = 0\n",
    "        for i, x_i in enumerate(x):\n",
    "            x_hat = vae_model(x_i)\n",
    "\n",
    "            reproduction_loss = F.binary_cross_entropy(x_hat, x_i, reduction='sum')   \n",
    "            loss += reproduction_loss    \n",
    "        train_vae_loss += loss.item()\n",
    "        loss = loss/x.size(0)\n",
    "        loss.backward()\n",
    "        optimizer_vae.step()\n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTrain_vae_loss: {:.4f}\".format(train_vae_loss / train_batch_size),\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vae_model.parameters():\n",
    "    param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, feat_dim, latent_dim, hidden_channels, out_channels, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.conv1 = GATConv(feat_dim, hidden_channels, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads+latent_dim, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, feat_i, x_i, edge_index):\n",
    "        x = self.conv1(feat_i, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        latten = F.relu(self.linear1(x_i))\n",
    "        x =  torch.cat((x, latten), dim=-1)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (conv1): GATConv(135, 512, heads=4)\n",
       "  (conv2): GATConv(2112, 1, heads=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data.model.gat import GAT, SpGAT\n",
    "\n",
    "# forward_model = SpGAT(nfeat=feat_num, \n",
    "#                 nhid=64, \n",
    "#                 nclass=1, \n",
    "#                 dropout=0.2, \n",
    "#                 nheads=1, \n",
    "#                 alpha=0.2)\n",
    "\n",
    "feat_num =seed_num\n",
    "\n",
    "forward_model = GAT(feat_num, latent_dim, 512, 1, 4)\n",
    "\n",
    "optimizer = Adam([{'params': forward_model.parameters()}], \n",
    "                 lr=0.001)\n",
    "\n",
    "adj = adj.to(device)\n",
    "forward_model = forward_model.to(device)\n",
    "forward_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTotal: 16845.7433 \tMean_train_accuracy: 0.2025\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 2 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2058\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 3 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2056\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 4 \tTotal: 14498.2054 \tMean_train_accuracy: 0.2084\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2048\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 6 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2068\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 7 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2063\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 8 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2047\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 9 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2071\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 10 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2056\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 11 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2015\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 12 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2037\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 13 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2053\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 14 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2041\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 15 \tTotal: 14498.2066 \tMean_train_accuracy: 0.2064\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 16 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2057\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 17 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2045\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 18 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2039\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 19 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2048\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 20 \tTotal: 14498.2067 \tMean_train_accuracy: 0.2029\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 21 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2045\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 22 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2041\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 23 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2062\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 24 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2017\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 25 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2060\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 26 \tTotal: 14498.2057 \tMean_train_accuracy: 0.2053\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 27 \tTotal: 14498.2067 \tMean_train_accuracy: 0.2048\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 28 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2083\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 29 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2066\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 30 \tTotal: 14498.2057 \tMean_train_accuracy: 0.2045\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 31 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2028\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 32 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2076\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 33 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2076\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 34 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2053\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 35 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2031\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 36 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2082\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 37 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2050\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 38 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2037\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 39 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2051\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 40 \tTotal: 14498.2068 \tMean_train_accuracy: 0.2061\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 41 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2030\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 42 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2082\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 43 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2057\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 44 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2089\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 45 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2069\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 46 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2061\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 47 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2041\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 48 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2078\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 49 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2066\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 50 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2086\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 51 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2009\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 52 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2088\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 53 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2045\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 54 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2033\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 55 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2048\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 56 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2053\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 57 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2021\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 58 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2043\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 59 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2031\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 60 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2037\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 61 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2025\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 62 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2059\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 63 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2031\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 64 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2042\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 65 \tTotal: 14498.2057 \tMean_train_accuracy: 0.2065\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 66 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2025\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 67 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2087\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 68 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2061\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 69 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2058\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 70 \tTotal: 14498.2057 \tMean_train_accuracy: 0.2068\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 71 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2049\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 72 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2037\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 73 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2046\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 74 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2030\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 75 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2084\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 76 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2095\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 77 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2051\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 78 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2017\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 79 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2079\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 80 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2079\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 81 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2044\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 82 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2078\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 83 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2055\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 84 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2015\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 85 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2058\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 86 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2046\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 87 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2040\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 88 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2044\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 89 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2017\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 90 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2074\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 91 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2068\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 92 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2057\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 93 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2013\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 94 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2052\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 95 \tTotal: 14498.2066 \tMean_train_accuracy: 0.2079\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 96 \tTotal: 14498.2056 \tMean_train_accuracy: 0.2039\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 97 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2049\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 98 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2065\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 99 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2088\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 100 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2041\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 101 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2016\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 102 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2073\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 103 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2051\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 104 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2069\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 105 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2058\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 106 \tTotal: 14498.2057 \tMean_train_accuracy: 0.2018\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 107 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2021\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 108 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2018\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 109 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2067\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 110 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2049\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 111 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2081\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 112 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2074\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 113 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2069\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 114 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2038\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 115 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2036\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 116 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2078\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 117 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2058\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 118 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2028\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 119 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2020\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 120 \tTotal: 14498.2056 \tMean_train_accuracy: 0.2029\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 121 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2078\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 122 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2021\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 123 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2054\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 124 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2051\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 125 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2052\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 126 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2045\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 127 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2060\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 128 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2023\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 129 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2071\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 130 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2027\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 131 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2049\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 132 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2056\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 133 \tTotal: 14498.2066 \tMean_train_accuracy: 0.2051\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 134 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2036\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 135 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2071\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 136 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2047\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 137 \tTotal: 14498.2067 \tMean_train_accuracy: 0.2081\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 138 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2066\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 139 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2085\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 140 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2017\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 141 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2028\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 142 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2055\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 143 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2003\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 144 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2042\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 145 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2053\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 146 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2043\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 147 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2048\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 148 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2043\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 149 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2009\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 150 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2069\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 151 \tTotal: 14498.2064 \tMean_train_accuracy: 0.2063\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 152 \tTotal: 14498.2065 \tMean_train_accuracy: 0.2056\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 153 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2014\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 154 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2034\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 155 \tTotal: 14498.2058 \tMean_train_accuracy: 0.2071\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 156 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2020\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 157 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2023\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 158 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2038\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 159 \tTotal: 14498.2061 \tMean_train_accuracy: 0.2068\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 160 \tTotal: 14498.2067 \tMean_train_accuracy: 0.2069\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 161 \tTotal: 14498.2062 \tMean_train_accuracy: 0.2051\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 162 \tTotal: 14498.2060 \tMean_train_accuracy: 0.2054\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 163 \tTotal: 14498.2059 \tMean_train_accuracy: 0.2029\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n",
      "Epoch: 164 \tTotal: 14498.2063 \tMean_train_accuracy: 0.2054\n",
      "\tMean_test_accuracy: 0.1990 \tMean_test_accuracy_sum: 0.4700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m forward_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m mean_train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, x_feature_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):  \n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m  x_feature_label[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)     \n\u001b[1;32m     13\u001b[0m     features \u001b[38;5;241m=\u001b[39m x_feature_label[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/cm/shared/apps/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "edge_index = edge_index.to(device)\n",
    "top_num = 500\n",
    "encoder = vae_model.Encoder\n",
    "\n",
    "for epoch in range(2000):\n",
    "\n",
    "    total_overall = 0\n",
    "    forward_loss = 0\n",
    "\n",
    "    mean_train_accuracy = 0\n",
    "    for batch_idx, x_feature_label in enumerate(train_loader):  \n",
    "        x =  x_feature_label[0].to(device)     \n",
    "        features = x_feature_label[1].to(device)\n",
    "        labels = x_feature_label[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        train_accuracy = 0\n",
    "        for i, feat_i in enumerate(features):\n",
    "            \n",
    "            # log_sum = torch.sum(torch.log(1 - x_i), dim=1, keepdim=True)\n",
    "            # feat = 1 - torch.exp(log_sum)\n",
    "            x_i = x[i]\n",
    "            x_i = encoder(x_i).detach()\n",
    "            x_i = x_i.expand(features.shape[1], -1)\n",
    "            # final_feat_i =  torch.cat((feat_i, x_i), dim=1)\n",
    "            \n",
    "            \n",
    "            y_i = labels[i]\n",
    "            y_hat = forward_model(feat_i, x_i, edge_index)\n",
    "            _, top_indices_true = torch.topk(y_i.clone(), top_num)\n",
    "            label_2 = torch.zeros(y_i.shape).to(device)\n",
    "            label_2[top_indices_true] = 1\n",
    "            \n",
    "            _, top_indices_predict = torch.topk(y_hat.clone().squeeze(-1), top_num)\n",
    "            \n",
    "            # 将张量数组转换为Python列表\n",
    "            list1 = top_indices_true.tolist()\n",
    "            list_pre = top_indices_predict.tolist()\n",
    "\n",
    "            # 使用集合操作找到交集\n",
    "            intersection = list(set(list1) & set(list_pre))\n",
    "            accuracy_i = len(intersection) / top_num       \n",
    "            train_accuracy += accuracy_i \n",
    "\n",
    "            forward_loss = 0.5*F.mse_loss(y_hat.squeeze(-1), y_i, reduction='sum') + F.mse_loss(y_hat.squeeze(-1), label_2, reduction='sum')    \n",
    "            loss += forward_loss    \n",
    "        \n",
    "        total_overall += loss.item()    \n",
    "        train_accuracy /= len(features)\n",
    "        mean_train_accuracy = train_accuracy\n",
    "        loss = loss/features.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # for p in forward_model.parameters():\n",
    "        #     p.data.clamp_(min=0)\n",
    "        \n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTotal: {:.4f}\".format(total_overall / train_batch_size),\n",
    "        \"\\tMean_train_accuracy: {:.4f}\".format(mean_train_accuracy),\n",
    "        )  \n",
    "    \n",
    "    mean_accuracy = 0\n",
    "    mean_accuracy_sum = 0\n",
    "\n",
    "    \n",
    "    for batch_idx, x_feature_label in enumerate(test_loader):  \n",
    "        x =  x_feature_label[0].to(device)\n",
    "        features = x_feature_label[1].to(device)\n",
    "        labels = x_feature_label[2].to(device)\n",
    "        \n",
    "        accuracy = 0\n",
    "        accuracy_sum = 0\n",
    "        \n",
    "        for i, feat_i in enumerate(features):\n",
    "            x_i = x[i]\n",
    "            x_i = encoder(x_i).detach()\n",
    "            x_i = x_i.expand(features.shape[1], -1)\n",
    "            # final_feat_i =  torch.cat((feat_i, x_i), dim=1)\n",
    "            y_i = labels[i]\n",
    "            _, top_indices_true = torch.topk(y_i, top_num)\n",
    "            \n",
    "            y_hat = forward_model(feat_i, x_i, edge_index)\n",
    "            \n",
    "            _, top_indices_predict = torch.topk(y_hat.squeeze(-1), top_num)\n",
    "            \n",
    "            sum_pre = torch.sum(feat_i, dim=1, keepdim=True)\n",
    "            _, top_indices_sum = torch.topk(sum_pre.squeeze(-1), top_num)\n",
    "            \n",
    "            # 将张量数组转换为Python列表\n",
    "            list1 = top_indices_true.tolist()\n",
    "            list_pre = top_indices_predict.tolist()\n",
    "            \n",
    "            list_sum = top_indices_sum.tolist()\n",
    "\n",
    "            # 使用集合操作找到交集\n",
    "            intersection = list(set(list1) & set(list_pre))\n",
    "            \n",
    "            intersection_sum = list(set(list1) & set(list_sum))\n",
    "            \n",
    "            accuracy_i = len(intersection) / top_num       \n",
    "            accuracy += accuracy_i \n",
    "            accuracy_sum += len(intersection_sum) / top_num  \n",
    "        accuracy /= test_batch_size\n",
    "        accuracy_sum/= test_batch_size\n",
    "        mean_accuracy = accuracy\n",
    "        mean_accuracy_sum = accuracy_sum\n",
    "        break\n",
    "    \n",
    "    print(\n",
    "        \"\\tMean_test_accuracy: {:.4f}\".format(mean_accuracy),\n",
    "        \"\\tMean_test_accuracy_sum: {:.4f}\".format(mean_accuracy_sum)\n",
    "        )  \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
