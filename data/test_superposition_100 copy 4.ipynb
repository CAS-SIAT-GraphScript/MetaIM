{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=6)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "pwd = '/home/zjy/project/MetaIM/data'\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "cora_dataset = Planetoid(root=pwd+'/cora', name='cora')\n",
    "data = cora_dataset[0]\n",
    "edge_index = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), (500, 2, 2708))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "individual_infection_path = pwd+'/for_meta/cora_individual_infection_sir_100.npy'\n",
    "seeds_infection_path = pwd+'/for_meta/cora_seed_infection_sir_100.npy'\n",
    "\n",
    "individual_infection = np.load(individual_infection_path)\n",
    "seeds_infection = np.load(seeds_infection_path)\n",
    "individual_infection.shape,seeds_infection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "                       [   0,  633, 1862,  ..., 1473, 2706, 2707]]),\n",
       "       values=tensor([0.2500, 0.2500, 0.2236,  ..., 0.2000, 0.2000, 0.2000]),\n",
       "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 转换为 scipy 稀疏矩阵\n",
    "adj = to_scipy_sparse_matrix(edge_index)\n",
    "\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "adj = torch.Tensor(adj.toarray()).to_sparse()\n",
    "adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_num = int(seeds_infection[0][0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, individual_infection,seeds_infection, feat_num):\n",
    "        self.individual_infection = individual_infection\n",
    "        self.seeds_infection = seeds_infection\n",
    "        self.feat_shape = (len(individual_infection), feat_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seeds_infection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeds= np.nonzero(self.seeds_infection[idx][0])[0]\n",
    "        \n",
    "        feature = torch.zeros(self.feat_shape[0],self.feat_shape[1])\n",
    "        for i in range(len(seeds)):\n",
    "            seed_i_infection = torch.tensor(self.individual_infection[seeds[i]])\n",
    "            feature[:, i] = seed_i_infection\n",
    "            \n",
    "        label = self.seeds_infection[idx][1]\n",
    "        \n",
    "        return feature, label\n",
    "\n",
    "dataset = CustomDataset(individual_infection,seeds_infection,feat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义划分比例\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# 划分数据集\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*train_ratio), int(len(dataset)*test_ratio)])\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 4\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = input.device\n",
    "\n",
    "        N = input.size()[0]\n",
    "        if adj.layout == torch.sparse_coo:\n",
    "            edge = adj.indices()\n",
    "        else:\n",
    "            edge = adj.nonzero().t()\n",
    "\n",
    "        assert not torch.isnan(input).any()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        \n",
    "        self.attentions1 = [SpGraphAttentionLayer(nhid * nheads, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "            \n",
    "        for i, attention in enumerate(self.attentions1):\n",
    "            self.add_module('attention1_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(torch.cat([att(x, adj) for att in self.attentions], dim=1))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpGAT(\n",
       "  (attention_0): SpGraphAttentionLayer (1 -> 64)\n",
       "  (attention1_0): SpGraphAttentionLayer (64 -> 64)\n",
       "  (out_att): SpGraphAttentionLayer (64 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data.model.gat import GAT, SpGAT\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "\n",
    "forward_model = SpGAT(nfeat=1, \n",
    "                nhid=64, \n",
    "                nclass=1, \n",
    "                dropout=0.2, \n",
    "                nheads=1, \n",
    "                alpha=0.2)\n",
    "\n",
    "\n",
    "optimizer = Adam([{'params': forward_model.parameters()}], \n",
    "                 lr=1e-3)\n",
    "\n",
    "adj = adj.to(device)\n",
    "forward_model = forward_model.to(device)\n",
    "forward_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTotal: 844.7563 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 2 \tTotal: 796.6725 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 3 \tTotal: 790.9019 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 4 \tTotal: 782.6303 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 5 \tTotal: 779.2428 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 6 \tTotal: 776.8073 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 7 \tTotal: 774.9562 \tMean_train_accuracy: 0.2025\n",
      "Epoch: 8 \tTotal: 774.7693 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 9 \tTotal: 773.4746 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 10 \tTotal: 773.6754 \tMean_train_accuracy: 0.2021\n",
      "Epoch: 11 \tTotal: 773.7511 \tMean_train_accuracy: 0.2062\n",
      "Epoch: 12 \tTotal: 772.6862 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 13 \tTotal: 772.5301 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 14 \tTotal: 772.0886 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 15 \tTotal: 772.2496 \tMean_train_accuracy: 0.2056\n",
      "Epoch: 16 \tTotal: 772.6376 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 17 \tTotal: 771.2860 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 18 \tTotal: 771.1301 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 19 \tTotal: 771.0424 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 20 \tTotal: 770.1557 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 21 \tTotal: 770.4543 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 22 \tTotal: 769.8702 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 23 \tTotal: 769.5834 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 24 \tTotal: 770.6377 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 25 \tTotal: 769.4671 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 26 \tTotal: 769.5971 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 27 \tTotal: 769.2583 \tMean_train_accuracy: 0.2058\n",
      "Epoch: 28 \tTotal: 769.4702 \tMean_train_accuracy: 0.1979\n",
      "Epoch: 29 \tTotal: 768.6938 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 30 \tTotal: 768.8772 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 31 \tTotal: 769.1405 \tMean_train_accuracy: 0.2077\n",
      "Epoch: 32 \tTotal: 768.1330 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 33 \tTotal: 767.8586 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 34 \tTotal: 767.2796 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 35 \tTotal: 767.1679 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 36 \tTotal: 767.3921 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 37 \tTotal: 765.8781 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 38 \tTotal: 766.4155 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 39 \tTotal: 767.0419 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 40 \tTotal: 767.1767 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 41 \tTotal: 766.7254 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 42 \tTotal: 766.5046 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 43 \tTotal: 766.0331 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 44 \tTotal: 766.2840 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 45 \tTotal: 765.7357 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 46 \tTotal: 766.2615 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 47 \tTotal: 766.7287 \tMean_train_accuracy: 0.2113\n",
      "Epoch: 48 \tTotal: 766.0974 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 49 \tTotal: 766.0658 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 50 \tTotal: 766.2534 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 51 \tTotal: 765.2828 \tMean_train_accuracy: 0.2033\n",
      "Epoch: 52 \tTotal: 765.7361 \tMean_train_accuracy: 0.2213\n",
      "Epoch: 53 \tTotal: 765.8475 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 54 \tTotal: 765.6738 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 55 \tTotal: 766.1210 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 56 \tTotal: 765.1889 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 57 \tTotal: 765.1901 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 58 \tTotal: 765.5738 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 59 \tTotal: 765.1621 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 60 \tTotal: 764.4722 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 61 \tTotal: 764.5417 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 62 \tTotal: 764.3717 \tMean_train_accuracy: 0.2260\n",
      "Epoch: 63 \tTotal: 765.0210 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 64 \tTotal: 764.8211 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 65 \tTotal: 764.5675 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 66 \tTotal: 764.9060 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 67 \tTotal: 764.9639 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 68 \tTotal: 764.8400 \tMean_train_accuracy: 0.2060\n",
      "Epoch: 69 \tTotal: 764.3092 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 70 \tTotal: 764.6311 \tMean_train_accuracy: 0.2038\n",
      "Epoch: 71 \tTotal: 765.0007 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 72 \tTotal: 764.7784 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 73 \tTotal: 764.5707 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 74 \tTotal: 764.6248 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 75 \tTotal: 764.7770 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 76 \tTotal: 764.3358 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 77 \tTotal: 764.2957 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 78 \tTotal: 763.7884 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 79 \tTotal: 765.1401 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 80 \tTotal: 764.4073 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 81 \tTotal: 764.0528 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 82 \tTotal: 764.3152 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 83 \tTotal: 764.4568 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 84 \tTotal: 764.9764 \tMean_train_accuracy: 0.2238\n",
      "Epoch: 85 \tTotal: 764.4687 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 86 \tTotal: 763.7183 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 87 \tTotal: 763.2171 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 88 \tTotal: 765.1262 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 89 \tTotal: 764.8307 \tMean_train_accuracy: 0.2275\n",
      "Epoch: 90 \tTotal: 764.2341 \tMean_train_accuracy: 0.2287\n",
      "Epoch: 91 \tTotal: 764.1641 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 92 \tTotal: 764.0805 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 93 \tTotal: 764.0594 \tMean_train_accuracy: 0.2215\n",
      "Epoch: 94 \tTotal: 764.3759 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 95 \tTotal: 763.4403 \tMean_train_accuracy: 0.2242\n",
      "Epoch: 96 \tTotal: 763.3980 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 97 \tTotal: 763.5626 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 98 \tTotal: 763.7850 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 99 \tTotal: 763.9285 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 100 \tTotal: 763.4898 \tMean_train_accuracy: 0.1990\n",
      "Epoch: 101 \tTotal: 763.3608 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 102 \tTotal: 763.8392 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 103 \tTotal: 763.1109 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 104 \tTotal: 763.6327 \tMean_train_accuracy: 0.2258\n",
      "Epoch: 105 \tTotal: 763.4102 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 106 \tTotal: 763.8713 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 107 \tTotal: 763.7105 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 108 \tTotal: 763.8404 \tMean_train_accuracy: 0.2235\n",
      "Epoch: 109 \tTotal: 764.1457 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 110 \tTotal: 764.2186 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 111 \tTotal: 763.4311 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 112 \tTotal: 764.2141 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 113 \tTotal: 763.4762 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 114 \tTotal: 763.6786 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 115 \tTotal: 763.2328 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 116 \tTotal: 764.2974 \tMean_train_accuracy: 0.1992\n",
      "Epoch: 117 \tTotal: 763.7203 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 118 \tTotal: 763.5608 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 119 \tTotal: 763.2648 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 120 \tTotal: 763.9836 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 121 \tTotal: 764.4251 \tMean_train_accuracy: 0.2077\n",
      "Epoch: 122 \tTotal: 763.8129 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 123 \tTotal: 763.4108 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 124 \tTotal: 763.9202 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 125 \tTotal: 762.6360 \tMean_train_accuracy: 0.2246\n",
      "Epoch: 126 \tTotal: 763.2090 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 127 \tTotal: 763.1294 \tMean_train_accuracy: 0.1981\n",
      "Epoch: 128 \tTotal: 763.5550 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 129 \tTotal: 763.6722 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 130 \tTotal: 763.0438 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 131 \tTotal: 763.1988 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 132 \tTotal: 762.6462 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 133 \tTotal: 763.0873 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 134 \tTotal: 762.8435 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 135 \tTotal: 763.3958 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 136 \tTotal: 763.4052 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 137 \tTotal: 763.5396 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 138 \tTotal: 763.1740 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 139 \tTotal: 763.9115 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 140 \tTotal: 763.3278 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 141 \tTotal: 764.0351 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 142 \tTotal: 762.4221 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 143 \tTotal: 763.6452 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 144 \tTotal: 762.7391 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 145 \tTotal: 763.3639 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 146 \tTotal: 763.7831 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 147 \tTotal: 762.6888 \tMean_train_accuracy: 0.2037\n",
      "Epoch: 148 \tTotal: 763.2719 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 149 \tTotal: 763.3121 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 150 \tTotal: 763.8284 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 151 \tTotal: 763.3706 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 152 \tTotal: 763.4517 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 153 \tTotal: 763.8958 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 154 \tTotal: 763.5751 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 155 \tTotal: 763.2292 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 156 \tTotal: 762.5149 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 157 \tTotal: 763.1144 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 158 \tTotal: 763.7337 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 159 \tTotal: 763.1249 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 160 \tTotal: 763.2196 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 161 \tTotal: 763.3445 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 162 \tTotal: 763.3565 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 163 \tTotal: 763.0651 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 164 \tTotal: 763.1487 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 165 \tTotal: 763.2135 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 166 \tTotal: 762.9923 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 167 \tTotal: 763.3172 \tMean_train_accuracy: 0.2113\n",
      "Epoch: 168 \tTotal: 762.9413 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 169 \tTotal: 764.1410 \tMean_train_accuracy: 0.2275\n",
      "Epoch: 170 \tTotal: 762.5686 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 171 \tTotal: 763.2086 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 172 \tTotal: 763.0007 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 173 \tTotal: 763.0629 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 174 \tTotal: 763.4523 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 175 \tTotal: 763.0365 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 176 \tTotal: 762.9177 \tMean_train_accuracy: 0.2242\n",
      "Epoch: 177 \tTotal: 762.8678 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 178 \tTotal: 763.0608 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 179 \tTotal: 763.6710 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 180 \tTotal: 762.9439 \tMean_train_accuracy: 0.2050\n",
      "Epoch: 181 \tTotal: 763.8971 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 182 \tTotal: 762.7459 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 183 \tTotal: 762.9357 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 184 \tTotal: 763.0957 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 185 \tTotal: 762.5146 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 186 \tTotal: 763.1823 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 187 \tTotal: 763.2401 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 188 \tTotal: 763.5742 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 189 \tTotal: 762.4807 \tMean_train_accuracy: 0.2056\n",
      "Epoch: 190 \tTotal: 763.2604 \tMean_train_accuracy: 0.2046\n",
      "Epoch: 191 \tTotal: 762.9667 \tMean_train_accuracy: 0.2229\n",
      "Epoch: 192 \tTotal: 763.2885 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 193 \tTotal: 762.4015 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 194 \tTotal: 762.5215 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 195 \tTotal: 763.0272 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 196 \tTotal: 763.3043 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 197 \tTotal: 763.0967 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 198 \tTotal: 762.8499 \tMean_train_accuracy: 0.2279\n",
      "Epoch: 199 \tTotal: 762.9848 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 200 \tTotal: 762.7657 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 201 \tTotal: 762.9958 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 202 \tTotal: 763.0711 \tMean_train_accuracy: 0.2246\n",
      "Epoch: 203 \tTotal: 762.2052 \tMean_train_accuracy: 0.2275\n",
      "Epoch: 204 \tTotal: 762.4596 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 205 \tTotal: 762.2938 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 206 \tTotal: 763.1072 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 207 \tTotal: 763.0625 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 208 \tTotal: 763.0097 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 209 \tTotal: 762.8997 \tMean_train_accuracy: 0.2258\n",
      "Epoch: 210 \tTotal: 763.2616 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 211 \tTotal: 762.8910 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 212 \tTotal: 763.1556 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 213 \tTotal: 763.5209 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 214 \tTotal: 763.3467 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 215 \tTotal: 763.2832 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 216 \tTotal: 763.1682 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 217 \tTotal: 763.9208 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 218 \tTotal: 762.8588 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 219 \tTotal: 762.6049 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 220 \tTotal: 762.8849 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 221 \tTotal: 762.6472 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 222 \tTotal: 762.3848 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 223 \tTotal: 762.4766 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 224 \tTotal: 763.1792 \tMean_train_accuracy: 0.2285\n",
      "Epoch: 225 \tTotal: 762.4897 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 226 \tTotal: 763.6121 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 227 \tTotal: 763.0037 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 228 \tTotal: 763.1865 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 229 \tTotal: 762.8625 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 230 \tTotal: 763.0410 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 231 \tTotal: 763.3912 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 232 \tTotal: 762.7108 \tMean_train_accuracy: 0.2023\n",
      "Epoch: 233 \tTotal: 762.5209 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 234 \tTotal: 762.3840 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 235 \tTotal: 762.4889 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 236 \tTotal: 762.6935 \tMean_train_accuracy: 0.2187\n",
      "Epoch: 237 \tTotal: 763.0079 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 238 \tTotal: 762.9673 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 239 \tTotal: 763.1226 \tMean_train_accuracy: 0.2213\n",
      "Epoch: 240 \tTotal: 762.9465 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 241 \tTotal: 763.2906 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 242 \tTotal: 762.6317 \tMean_train_accuracy: 0.2229\n",
      "Epoch: 243 \tTotal: 762.5239 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 244 \tTotal: 763.0436 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 245 \tTotal: 762.6804 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 246 \tTotal: 762.6097 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 247 \tTotal: 762.3104 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 248 \tTotal: 762.7836 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 249 \tTotal: 762.8606 \tMean_train_accuracy: 0.2077\n",
      "Epoch: 250 \tTotal: 763.4093 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 251 \tTotal: 762.7520 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 252 \tTotal: 763.5434 \tMean_train_accuracy: 0.2017\n",
      "Epoch: 253 \tTotal: 763.1595 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 254 \tTotal: 763.6064 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 255 \tTotal: 762.4039 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 256 \tTotal: 762.2330 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 257 \tTotal: 763.1453 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 258 \tTotal: 762.8845 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 259 \tTotal: 762.7297 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 260 \tTotal: 763.4249 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 261 \tTotal: 762.3850 \tMean_train_accuracy: 0.2277\n",
      "Epoch: 262 \tTotal: 763.0051 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 263 \tTotal: 763.7426 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 264 \tTotal: 763.1308 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 265 \tTotal: 762.6690 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 266 \tTotal: 763.0419 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 267 \tTotal: 763.1694 \tMean_train_accuracy: 0.2029\n",
      "Epoch: 268 \tTotal: 762.9994 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 269 \tTotal: 762.8397 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 270 \tTotal: 762.6919 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 271 \tTotal: 763.1500 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 272 \tTotal: 762.1717 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 273 \tTotal: 762.9930 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 274 \tTotal: 762.7991 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 275 \tTotal: 763.5646 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 276 \tTotal: 762.8727 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 277 \tTotal: 762.9005 \tMean_train_accuracy: 0.2215\n",
      "Epoch: 278 \tTotal: 763.6880 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 279 \tTotal: 762.4129 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 280 \tTotal: 762.1260 \tMean_train_accuracy: 0.2273\n",
      "Epoch: 281 \tTotal: 763.2859 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 282 \tTotal: 762.6783 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 283 \tTotal: 762.6314 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 284 \tTotal: 762.8732 \tMean_train_accuracy: 0.2113\n",
      "Epoch: 285 \tTotal: 762.8999 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 286 \tTotal: 763.9235 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 287 \tTotal: 763.6726 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 288 \tTotal: 763.0780 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 289 \tTotal: 763.2444 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 290 \tTotal: 763.5754 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 291 \tTotal: 763.0816 \tMean_train_accuracy: 0.2256\n",
      "Epoch: 292 \tTotal: 762.9601 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 293 \tTotal: 762.9792 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 294 \tTotal: 762.8705 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 295 \tTotal: 762.9159 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 296 \tTotal: 762.9194 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 297 \tTotal: 763.0989 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 298 \tTotal: 763.9020 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 299 \tTotal: 763.6363 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 300 \tTotal: 763.3422 \tMean_train_accuracy: 0.2060\n",
      "Epoch: 301 \tTotal: 762.9480 \tMean_train_accuracy: 0.1990\n",
      "Epoch: 302 \tTotal: 762.4846 \tMean_train_accuracy: 0.2387\n",
      "Epoch: 303 \tTotal: 763.3137 \tMean_train_accuracy: 0.2273\n",
      "Epoch: 304 \tTotal: 762.6819 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 305 \tTotal: 763.1911 \tMean_train_accuracy: 0.2213\n",
      "Epoch: 306 \tTotal: 762.4980 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 307 \tTotal: 762.7749 \tMean_train_accuracy: 0.2029\n",
      "Epoch: 308 \tTotal: 762.9423 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 309 \tTotal: 763.2812 \tMean_train_accuracy: 0.1975\n",
      "Epoch: 310 \tTotal: 762.8792 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 311 \tTotal: 762.8709 \tMean_train_accuracy: 0.2088\n",
      "Epoch: 312 \tTotal: 761.9918 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 313 \tTotal: 763.1255 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 314 \tTotal: 762.5737 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 315 \tTotal: 762.6599 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 316 \tTotal: 762.9706 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 317 \tTotal: 763.1795 \tMean_train_accuracy: 0.2267\n",
      "Epoch: 318 \tTotal: 763.6182 \tMean_train_accuracy: 0.2238\n",
      "Epoch: 319 \tTotal: 763.1837 \tMean_train_accuracy: 0.2315\n",
      "Epoch: 320 \tTotal: 761.8411 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 321 \tTotal: 763.0934 \tMean_train_accuracy: 0.2037\n",
      "Epoch: 322 \tTotal: 763.0634 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 323 \tTotal: 763.3240 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 324 \tTotal: 763.0084 \tMean_train_accuracy: 0.1948\n",
      "Epoch: 325 \tTotal: 762.7852 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 326 \tTotal: 762.2311 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 327 \tTotal: 763.9228 \tMean_train_accuracy: 0.2037\n",
      "Epoch: 328 \tTotal: 762.7051 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 329 \tTotal: 763.1789 \tMean_train_accuracy: 0.2017\n",
      "Epoch: 330 \tTotal: 763.2793 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 331 \tTotal: 762.6043 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 332 \tTotal: 762.9832 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 333 \tTotal: 762.7700 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 334 \tTotal: 763.4369 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 335 \tTotal: 763.7422 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 336 \tTotal: 762.7632 \tMean_train_accuracy: 0.2052\n",
      "Epoch: 337 \tTotal: 762.7889 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 338 \tTotal: 762.7261 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 339 \tTotal: 763.3871 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 340 \tTotal: 762.8758 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 341 \tTotal: 762.5945 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 342 \tTotal: 762.8279 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 343 \tTotal: 762.7099 \tMean_train_accuracy: 0.2000\n",
      "Epoch: 344 \tTotal: 763.1272 \tMean_train_accuracy: 0.2340\n",
      "Epoch: 345 \tTotal: 762.4971 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 346 \tTotal: 762.7746 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 347 \tTotal: 763.0736 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 348 \tTotal: 763.2996 \tMean_train_accuracy: 0.2298\n",
      "Epoch: 349 \tTotal: 762.5992 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 350 \tTotal: 763.4623 \tMean_train_accuracy: 0.2294\n",
      "Epoch: 351 \tTotal: 762.3819 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 352 \tTotal: 763.3043 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 353 \tTotal: 762.3376 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 354 \tTotal: 762.9294 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 355 \tTotal: 762.4225 \tMean_train_accuracy: 0.2217\n",
      "Epoch: 356 \tTotal: 762.7399 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 357 \tTotal: 763.0095 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 358 \tTotal: 762.1148 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 359 \tTotal: 762.6250 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 360 \tTotal: 762.8995 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 361 \tTotal: 762.3683 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 362 \tTotal: 762.8177 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 363 \tTotal: 762.7138 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 364 \tTotal: 762.9932 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 365 \tTotal: 762.3656 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 366 \tTotal: 762.4704 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 367 \tTotal: 762.3047 \tMean_train_accuracy: 0.2077\n",
      "Epoch: 368 \tTotal: 762.2330 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 369 \tTotal: 762.8059 \tMean_train_accuracy: 0.2138\n",
      "Epoch: 370 \tTotal: 762.3458 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 371 \tTotal: 762.9715 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 372 \tTotal: 762.6150 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 373 \tTotal: 762.6506 \tMean_train_accuracy: 0.2017\n",
      "Epoch: 374 \tTotal: 762.4291 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 375 \tTotal: 762.4631 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 376 \tTotal: 762.7767 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 377 \tTotal: 762.7306 \tMean_train_accuracy: 0.2279\n",
      "Epoch: 378 \tTotal: 763.0225 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 379 \tTotal: 763.0182 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 380 \tTotal: 762.7914 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 381 \tTotal: 763.0003 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 382 \tTotal: 762.6827 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 383 \tTotal: 763.1448 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 384 \tTotal: 762.4535 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 385 \tTotal: 761.8969 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 386 \tTotal: 763.0559 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 387 \tTotal: 763.2084 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 388 \tTotal: 762.3371 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 389 \tTotal: 762.6915 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 390 \tTotal: 763.0366 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 391 \tTotal: 762.2152 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 392 \tTotal: 762.9477 \tMean_train_accuracy: 0.2052\n",
      "Epoch: 393 \tTotal: 762.9948 \tMean_train_accuracy: 0.2215\n",
      "Epoch: 394 \tTotal: 763.1739 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 395 \tTotal: 762.9582 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 396 \tTotal: 762.3763 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 397 \tTotal: 762.9950 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 398 \tTotal: 761.9238 \tMean_train_accuracy: 0.2242\n",
      "Epoch: 399 \tTotal: 762.1778 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 400 \tTotal: 763.2538 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 401 \tTotal: 762.6300 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 402 \tTotal: 763.1341 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 403 \tTotal: 763.2137 \tMean_train_accuracy: 0.2210\n",
      "Epoch: 404 \tTotal: 763.1106 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 405 \tTotal: 762.7777 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 406 \tTotal: 763.2894 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 407 \tTotal: 763.1610 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 408 \tTotal: 762.9105 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 409 \tTotal: 762.8392 \tMean_train_accuracy: 0.2213\n",
      "Epoch: 410 \tTotal: 763.1409 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 411 \tTotal: 762.9801 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 412 \tTotal: 762.8354 \tMean_train_accuracy: 0.2285\n",
      "Epoch: 413 \tTotal: 763.3596 \tMean_train_accuracy: 0.2046\n",
      "Epoch: 414 \tTotal: 762.4116 \tMean_train_accuracy: 0.2063\n",
      "Epoch: 415 \tTotal: 763.4645 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 416 \tTotal: 762.9148 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 417 \tTotal: 762.4580 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 418 \tTotal: 762.6869 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 419 \tTotal: 762.3599 \tMean_train_accuracy: 0.2021\n",
      "Epoch: 420 \tTotal: 762.2377 \tMean_train_accuracy: 0.2248\n",
      "Epoch: 421 \tTotal: 763.4260 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 422 \tTotal: 762.9970 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 423 \tTotal: 763.6341 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 424 \tTotal: 763.1942 \tMean_train_accuracy: 0.2012\n",
      "Epoch: 425 \tTotal: 763.1160 \tMean_train_accuracy: 0.1990\n",
      "Epoch: 426 \tTotal: 762.5213 \tMean_train_accuracy: 0.2056\n",
      "Epoch: 427 \tTotal: 763.2563 \tMean_train_accuracy: 0.2242\n",
      "Epoch: 428 \tTotal: 763.4360 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 429 \tTotal: 762.8642 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 430 \tTotal: 762.6774 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 431 \tTotal: 762.4902 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 432 \tTotal: 762.8160 \tMean_train_accuracy: 0.2240\n",
      "Epoch: 433 \tTotal: 762.5678 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 434 \tTotal: 763.4171 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 435 \tTotal: 762.3016 \tMean_train_accuracy: 0.2258\n",
      "Epoch: 436 \tTotal: 763.1698 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 437 \tTotal: 762.3786 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 438 \tTotal: 762.2835 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 439 \tTotal: 763.0325 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 440 \tTotal: 762.5694 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 441 \tTotal: 762.7264 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 442 \tTotal: 762.7121 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 443 \tTotal: 763.0991 \tMean_train_accuracy: 0.1956\n",
      "Epoch: 444 \tTotal: 763.5155 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 445 \tTotal: 763.0947 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 446 \tTotal: 761.9907 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 447 \tTotal: 762.5217 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 448 \tTotal: 762.8843 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 449 \tTotal: 762.8483 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 450 \tTotal: 763.2488 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 451 \tTotal: 762.6622 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 452 \tTotal: 762.1440 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 453 \tTotal: 763.6232 \tMean_train_accuracy: 0.1994\n",
      "Epoch: 454 \tTotal: 762.4568 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 455 \tTotal: 763.1152 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 456 \tTotal: 762.3513 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 457 \tTotal: 763.4686 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 458 \tTotal: 763.3922 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 459 \tTotal: 763.0483 \tMean_train_accuracy: 0.2244\n",
      "Epoch: 460 \tTotal: 762.7303 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 461 \tTotal: 762.8804 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 462 \tTotal: 762.8900 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 463 \tTotal: 762.8298 \tMean_train_accuracy: 0.2333\n",
      "Epoch: 464 \tTotal: 762.2224 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 465 \tTotal: 763.4265 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 466 \tTotal: 763.1369 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 467 \tTotal: 762.9052 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 468 \tTotal: 762.8087 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 469 \tTotal: 762.5871 \tMean_train_accuracy: 0.2010\n",
      "Epoch: 470 \tTotal: 762.7464 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 471 \tTotal: 763.0951 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 472 \tTotal: 763.3897 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 473 \tTotal: 762.5787 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 474 \tTotal: 764.1910 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 475 \tTotal: 763.4391 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 476 \tTotal: 762.8768 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 477 \tTotal: 762.9348 \tMean_train_accuracy: 0.2058\n",
      "Epoch: 478 \tTotal: 763.7668 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 479 \tTotal: 763.2273 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 480 \tTotal: 762.7089 \tMean_train_accuracy: 0.2240\n",
      "Epoch: 481 \tTotal: 762.5779 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 482 \tTotal: 762.3308 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 483 \tTotal: 763.2696 \tMean_train_accuracy: 0.2019\n",
      "Epoch: 484 \tTotal: 762.4013 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 485 \tTotal: 763.2542 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 486 \tTotal: 762.7865 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 487 \tTotal: 762.6521 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 488 \tTotal: 762.5035 \tMean_train_accuracy: 0.2254\n",
      "Epoch: 489 \tTotal: 763.4822 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 490 \tTotal: 763.3879 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 491 \tTotal: 762.5423 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 492 \tTotal: 762.8589 \tMean_train_accuracy: 0.2062\n",
      "Epoch: 493 \tTotal: 763.2628 \tMean_train_accuracy: 0.2006\n",
      "Epoch: 494 \tTotal: 762.9636 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 495 \tTotal: 762.8844 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 496 \tTotal: 762.9720 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 497 \tTotal: 763.0928 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 498 \tTotal: 762.1082 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 499 \tTotal: 763.2402 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 500 \tTotal: 762.9550 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 501 \tTotal: 762.9825 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 502 \tTotal: 762.9800 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 503 \tTotal: 762.4265 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 504 \tTotal: 763.0605 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 505 \tTotal: 762.3085 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 506 \tTotal: 763.2751 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 507 \tTotal: 763.1742 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 508 \tTotal: 763.5537 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 509 \tTotal: 762.7205 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 510 \tTotal: 762.2625 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 511 \tTotal: 762.0915 \tMean_train_accuracy: 0.2260\n",
      "Epoch: 512 \tTotal: 763.2347 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 513 \tTotal: 762.4461 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 514 \tTotal: 762.8748 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 515 \tTotal: 763.0609 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 516 \tTotal: 763.3050 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 517 \tTotal: 762.4149 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 518 \tTotal: 762.3548 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 519 \tTotal: 762.2368 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 520 \tTotal: 762.5681 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 521 \tTotal: 762.7430 \tMean_train_accuracy: 0.2244\n",
      "Epoch: 522 \tTotal: 762.7190 \tMean_train_accuracy: 0.2215\n",
      "Epoch: 523 \tTotal: 762.8202 \tMean_train_accuracy: 0.2187\n",
      "Epoch: 524 \tTotal: 762.6748 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 525 \tTotal: 762.7326 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 526 \tTotal: 763.5932 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 527 \tTotal: 764.0539 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 528 \tTotal: 763.1252 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 529 \tTotal: 763.3203 \tMean_train_accuracy: 0.2265\n",
      "Epoch: 530 \tTotal: 763.0848 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 531 \tTotal: 762.7013 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 532 \tTotal: 763.1512 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 533 \tTotal: 762.7446 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 534 \tTotal: 763.4219 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 535 \tTotal: 762.7739 \tMean_train_accuracy: 0.2021\n",
      "Epoch: 536 \tTotal: 762.4316 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 537 \tTotal: 763.1761 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 538 \tTotal: 763.0873 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 539 \tTotal: 763.8757 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 540 \tTotal: 762.7264 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 541 \tTotal: 763.6584 \tMean_train_accuracy: 0.2031\n",
      "Epoch: 542 \tTotal: 762.7815 \tMean_train_accuracy: 0.2292\n",
      "Epoch: 543 \tTotal: 763.2307 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 544 \tTotal: 762.5715 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 545 \tTotal: 763.0949 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 546 \tTotal: 763.0826 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 547 \tTotal: 762.7510 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 548 \tTotal: 762.9645 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 549 \tTotal: 762.9985 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 550 \tTotal: 763.2855 \tMean_train_accuracy: 0.2271\n",
      "Epoch: 551 \tTotal: 763.4863 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 552 \tTotal: 762.5341 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 553 \tTotal: 763.4383 \tMean_train_accuracy: 0.2056\n",
      "Epoch: 554 \tTotal: 763.2153 \tMean_train_accuracy: 0.2325\n",
      "Epoch: 555 \tTotal: 762.9458 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 556 \tTotal: 762.1360 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 557 \tTotal: 763.2507 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 558 \tTotal: 763.0830 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 559 \tTotal: 762.8565 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 560 \tTotal: 762.8118 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 561 \tTotal: 762.4157 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 562 \tTotal: 762.4280 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 563 \tTotal: 762.7328 \tMean_train_accuracy: 0.2060\n",
      "Epoch: 564 \tTotal: 763.0082 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 565 \tTotal: 763.2484 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 566 \tTotal: 762.7751 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 567 \tTotal: 762.5261 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 568 \tTotal: 762.7788 \tMean_train_accuracy: 0.2052\n",
      "Epoch: 569 \tTotal: 763.1378 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 570 \tTotal: 762.6469 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 571 \tTotal: 762.5298 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 572 \tTotal: 763.3262 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 573 \tTotal: 762.8274 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 574 \tTotal: 762.6463 \tMean_train_accuracy: 0.2235\n",
      "Epoch: 575 \tTotal: 763.0587 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 576 \tTotal: 762.5757 \tMean_train_accuracy: 0.2217\n",
      "Epoch: 577 \tTotal: 762.7346 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 578 \tTotal: 762.4713 \tMean_train_accuracy: 0.2385\n",
      "Epoch: 579 \tTotal: 763.2639 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 580 \tTotal: 762.4862 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 581 \tTotal: 763.5757 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 582 \tTotal: 763.0216 \tMean_train_accuracy: 0.2042\n",
      "Epoch: 583 \tTotal: 762.6397 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 584 \tTotal: 762.6810 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 585 \tTotal: 762.2998 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 586 \tTotal: 762.7913 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 587 \tTotal: 762.3239 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 588 \tTotal: 762.5799 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 589 \tTotal: 762.7731 \tMean_train_accuracy: 0.2327\n",
      "Epoch: 590 \tTotal: 763.2835 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 591 \tTotal: 762.9561 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 592 \tTotal: 762.9623 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 593 \tTotal: 763.2822 \tMean_train_accuracy: 0.2019\n",
      "Epoch: 594 \tTotal: 762.9556 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 595 \tTotal: 762.3076 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 596 \tTotal: 762.2868 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 597 \tTotal: 762.4918 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 598 \tTotal: 763.1883 \tMean_train_accuracy: 0.2035\n",
      "Epoch: 599 \tTotal: 763.3642 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 600 \tTotal: 763.0975 \tMean_train_accuracy: 0.2288\n",
      "Epoch: 601 \tTotal: 763.1410 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 602 \tTotal: 762.9669 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 603 \tTotal: 762.5347 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 604 \tTotal: 763.1739 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 605 \tTotal: 762.4131 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 606 \tTotal: 762.9262 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 607 \tTotal: 762.2616 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 608 \tTotal: 763.2113 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 609 \tTotal: 763.3694 \tMean_train_accuracy: 0.2138\n",
      "Epoch: 610 \tTotal: 762.6141 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 611 \tTotal: 762.8301 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 612 \tTotal: 762.9646 \tMean_train_accuracy: 0.1996\n",
      "Epoch: 613 \tTotal: 762.1272 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 614 \tTotal: 762.6363 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 615 \tTotal: 763.0284 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 616 \tTotal: 762.6694 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 617 \tTotal: 763.3737 \tMean_train_accuracy: 0.2021\n",
      "Epoch: 618 \tTotal: 762.8084 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 619 \tTotal: 763.3443 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 620 \tTotal: 761.8127 \tMean_train_accuracy: 0.2088\n",
      "Epoch: 621 \tTotal: 763.2381 \tMean_train_accuracy: 0.2273\n",
      "Epoch: 622 \tTotal: 762.8277 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 623 \tTotal: 762.6061 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 624 \tTotal: 762.6768 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 625 \tTotal: 762.6359 \tMean_train_accuracy: 0.2113\n",
      "Epoch: 626 \tTotal: 763.3303 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 627 \tTotal: 762.9278 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 628 \tTotal: 762.8391 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 629 \tTotal: 763.2606 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 630 \tTotal: 763.1802 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 631 \tTotal: 762.6939 \tMean_train_accuracy: 0.2023\n",
      "Epoch: 632 \tTotal: 762.9184 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 633 \tTotal: 762.2675 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 634 \tTotal: 763.4899 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 635 \tTotal: 763.0111 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 636 \tTotal: 762.8265 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 637 \tTotal: 762.7318 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 638 \tTotal: 762.6296 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 639 \tTotal: 762.5951 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 640 \tTotal: 762.4735 \tMean_train_accuracy: 0.2031\n",
      "Epoch: 641 \tTotal: 762.5806 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 642 \tTotal: 762.1701 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 643 \tTotal: 763.0042 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 644 \tTotal: 762.5322 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 645 \tTotal: 762.4340 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 646 \tTotal: 763.2838 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 647 \tTotal: 763.1573 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 648 \tTotal: 762.8568 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 649 \tTotal: 762.4819 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 650 \tTotal: 763.5290 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 651 \tTotal: 762.8214 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 652 \tTotal: 762.7105 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 653 \tTotal: 762.8055 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 654 \tTotal: 762.7368 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 655 \tTotal: 762.4211 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 656 \tTotal: 762.6531 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 657 \tTotal: 762.9544 \tMean_train_accuracy: 0.1985\n",
      "Epoch: 658 \tTotal: 763.1380 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 659 \tTotal: 763.0709 \tMean_train_accuracy: 0.2317\n",
      "Epoch: 660 \tTotal: 762.6141 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 661 \tTotal: 762.8683 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 662 \tTotal: 762.8888 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 663 \tTotal: 762.6968 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 664 \tTotal: 763.3722 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 665 \tTotal: 762.4918 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 666 \tTotal: 762.2799 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 667 \tTotal: 762.6994 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 668 \tTotal: 762.2674 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 669 \tTotal: 763.0536 \tMean_train_accuracy: 0.2071\n",
      "Epoch: 670 \tTotal: 763.2072 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 671 \tTotal: 762.7351 \tMean_train_accuracy: 0.2250\n",
      "Epoch: 672 \tTotal: 762.9597 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 673 \tTotal: 762.1927 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 674 \tTotal: 763.3246 \tMean_train_accuracy: 0.2210\n",
      "Epoch: 675 \tTotal: 763.0309 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 676 \tTotal: 763.1291 \tMean_train_accuracy: 0.2246\n",
      "Epoch: 677 \tTotal: 763.5308 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 678 \tTotal: 762.6521 \tMean_train_accuracy: 0.2046\n",
      "Epoch: 679 \tTotal: 762.3884 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 680 \tTotal: 762.9107 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 681 \tTotal: 763.0886 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 682 \tTotal: 763.2660 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 683 \tTotal: 762.6835 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 684 \tTotal: 763.0209 \tMean_train_accuracy: 0.2235\n",
      "Epoch: 685 \tTotal: 762.0338 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 686 \tTotal: 762.7737 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 687 \tTotal: 763.0727 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 688 \tTotal: 763.0514 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 689 \tTotal: 762.5240 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 690 \tTotal: 762.7730 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 691 \tTotal: 762.8464 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 692 \tTotal: 762.4226 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 693 \tTotal: 762.8884 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 694 \tTotal: 762.6512 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 695 \tTotal: 762.4368 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 696 \tTotal: 761.7733 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 697 \tTotal: 763.1364 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 698 \tTotal: 763.2367 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 699 \tTotal: 762.5285 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 700 \tTotal: 762.6939 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 701 \tTotal: 763.0653 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 702 \tTotal: 762.0906 \tMean_train_accuracy: 0.2292\n",
      "Epoch: 703 \tTotal: 762.2878 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 704 \tTotal: 763.0069 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 705 \tTotal: 763.2902 \tMean_train_accuracy: 0.2219\n",
      "Epoch: 706 \tTotal: 762.7712 \tMean_train_accuracy: 0.2229\n",
      "Epoch: 707 \tTotal: 762.8436 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 708 \tTotal: 762.7529 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 709 \tTotal: 762.5304 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 710 \tTotal: 762.7026 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 711 \tTotal: 763.0534 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 712 \tTotal: 762.8507 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 713 \tTotal: 762.9267 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 714 \tTotal: 762.8840 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 715 \tTotal: 761.7775 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 716 \tTotal: 762.7457 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 717 \tTotal: 762.8962 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 718 \tTotal: 763.6887 \tMean_train_accuracy: 0.1973\n",
      "Epoch: 719 \tTotal: 762.6985 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 720 \tTotal: 762.2878 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 721 \tTotal: 762.4099 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 722 \tTotal: 763.6873 \tMean_train_accuracy: 0.1950\n",
      "Epoch: 723 \tTotal: 762.8134 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 724 \tTotal: 762.2328 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 725 \tTotal: 762.3740 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 726 \tTotal: 762.7726 \tMean_train_accuracy: 0.2237\n",
      "Epoch: 727 \tTotal: 763.0492 \tMean_train_accuracy: 0.2002\n",
      "Epoch: 728 \tTotal: 763.1568 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 729 \tTotal: 762.9681 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 730 \tTotal: 763.0962 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 731 \tTotal: 762.4057 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 732 \tTotal: 763.0480 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 733 \tTotal: 762.9689 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 734 \tTotal: 763.1061 \tMean_train_accuracy: 0.2088\n",
      "Epoch: 735 \tTotal: 762.7239 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 736 \tTotal: 762.9842 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 737 \tTotal: 762.4850 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 738 \tTotal: 762.0983 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 739 \tTotal: 762.7452 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 740 \tTotal: 763.1638 \tMean_train_accuracy: 0.2065\n",
      "Epoch: 741 \tTotal: 763.0823 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 742 \tTotal: 762.6700 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 743 \tTotal: 762.9968 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 744 \tTotal: 762.8005 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 745 \tTotal: 762.6097 \tMean_train_accuracy: 0.2260\n",
      "Epoch: 746 \tTotal: 763.3100 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 747 \tTotal: 762.3693 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 748 \tTotal: 762.8000 \tMean_train_accuracy: 0.2235\n",
      "Epoch: 749 \tTotal: 762.7677 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 750 \tTotal: 762.5824 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 751 \tTotal: 762.2841 \tMean_train_accuracy: 0.2244\n",
      "Epoch: 752 \tTotal: 763.1316 \tMean_train_accuracy: 0.2138\n",
      "Epoch: 753 \tTotal: 763.1463 \tMean_train_accuracy: 0.2038\n",
      "Epoch: 754 \tTotal: 762.7442 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 755 \tTotal: 762.2782 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 756 \tTotal: 762.5945 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 757 \tTotal: 762.5413 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 758 \tTotal: 763.1309 \tMean_train_accuracy: 0.2042\n",
      "Epoch: 759 \tTotal: 762.9232 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 760 \tTotal: 763.3802 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 761 \tTotal: 762.6168 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 762 \tTotal: 762.9307 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 763 \tTotal: 762.2989 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 764 \tTotal: 762.8966 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 765 \tTotal: 762.7259 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 766 \tTotal: 762.5350 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 767 \tTotal: 762.6937 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 768 \tTotal: 762.6732 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 769 \tTotal: 762.9199 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 770 \tTotal: 762.9674 \tMean_train_accuracy: 0.2217\n",
      "Epoch: 771 \tTotal: 763.1126 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 772 \tTotal: 762.6614 \tMean_train_accuracy: 0.2210\n",
      "Epoch: 773 \tTotal: 763.2732 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 774 \tTotal: 763.4109 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 775 \tTotal: 763.3086 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 776 \tTotal: 763.0916 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 777 \tTotal: 762.5004 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 778 \tTotal: 762.7456 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 779 \tTotal: 762.7034 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 780 \tTotal: 763.3738 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 781 \tTotal: 762.6076 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 782 \tTotal: 763.2058 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 783 \tTotal: 763.0492 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 784 \tTotal: 763.0242 \tMean_train_accuracy: 0.2058\n",
      "Epoch: 785 \tTotal: 763.2122 \tMean_train_accuracy: 0.2031\n",
      "Epoch: 786 \tTotal: 763.0018 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 787 \tTotal: 762.4531 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 788 \tTotal: 762.4847 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 789 \tTotal: 762.7874 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 790 \tTotal: 763.0533 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 791 \tTotal: 762.7592 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 792 \tTotal: 762.5315 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 793 \tTotal: 762.2520 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 794 \tTotal: 762.6090 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 795 \tTotal: 762.2815 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 796 \tTotal: 763.3907 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 797 \tTotal: 762.7424 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 798 \tTotal: 762.6081 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 799 \tTotal: 762.3421 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 800 \tTotal: 762.9813 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 801 \tTotal: 762.9193 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 802 \tTotal: 762.7101 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 803 \tTotal: 762.2405 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 804 \tTotal: 762.3924 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 805 \tTotal: 762.5980 \tMean_train_accuracy: 0.2033\n",
      "Epoch: 806 \tTotal: 762.4389 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 807 \tTotal: 763.8183 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 808 \tTotal: 762.9268 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 809 \tTotal: 762.1311 \tMean_train_accuracy: 0.2262\n",
      "Epoch: 810 \tTotal: 762.3374 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 811 \tTotal: 762.8050 \tMean_train_accuracy: 0.2083\n",
      "Epoch: 812 \tTotal: 763.2106 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 813 \tTotal: 762.9799 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 814 \tTotal: 762.8764 \tMean_train_accuracy: 0.2010\n",
      "Epoch: 815 \tTotal: 763.2261 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 816 \tTotal: 763.0847 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 817 \tTotal: 762.9152 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 818 \tTotal: 762.7621 \tMean_train_accuracy: 0.2329\n",
      "Epoch: 819 \tTotal: 762.7210 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 820 \tTotal: 762.7404 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 821 \tTotal: 763.3269 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 822 \tTotal: 763.6184 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 823 \tTotal: 763.5075 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 824 \tTotal: 762.7316 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 825 \tTotal: 762.0934 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 826 \tTotal: 762.4263 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 827 \tTotal: 762.5309 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 828 \tTotal: 762.7171 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 829 \tTotal: 762.5859 \tMean_train_accuracy: 0.2238\n",
      "Epoch: 830 \tTotal: 762.5465 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 831 \tTotal: 762.7117 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 832 \tTotal: 762.7881 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 833 \tTotal: 762.0529 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 834 \tTotal: 762.5520 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 835 \tTotal: 761.5395 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 836 \tTotal: 763.2469 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 837 \tTotal: 762.4901 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 838 \tTotal: 763.3892 \tMean_train_accuracy: 0.2060\n",
      "Epoch: 839 \tTotal: 763.3899 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 840 \tTotal: 763.0600 \tMean_train_accuracy: 0.2267\n",
      "Epoch: 841 \tTotal: 762.5389 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 842 \tTotal: 762.6586 \tMean_train_accuracy: 0.2210\n",
      "Epoch: 843 \tTotal: 762.8532 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 844 \tTotal: 762.5326 \tMean_train_accuracy: 0.2088\n",
      "Epoch: 845 \tTotal: 762.4881 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 846 \tTotal: 762.8523 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 847 \tTotal: 762.4071 \tMean_train_accuracy: 0.2033\n",
      "Epoch: 848 \tTotal: 763.1833 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 849 \tTotal: 762.1811 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 850 \tTotal: 761.8122 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 851 \tTotal: 762.6446 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 852 \tTotal: 763.2693 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 853 \tTotal: 762.8530 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 854 \tTotal: 762.5610 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 855 \tTotal: 761.9088 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 856 \tTotal: 762.8336 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 857 \tTotal: 762.6604 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 858 \tTotal: 762.8803 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 859 \tTotal: 762.7979 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 860 \tTotal: 763.0383 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 861 \tTotal: 762.8640 \tMean_train_accuracy: 0.2217\n",
      "Epoch: 862 \tTotal: 762.7158 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 863 \tTotal: 762.7621 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 864 \tTotal: 762.4279 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 865 \tTotal: 762.3724 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 866 \tTotal: 762.9044 \tMean_train_accuracy: 0.2062\n",
      "Epoch: 867 \tTotal: 762.6532 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 868 \tTotal: 763.1281 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 869 \tTotal: 762.8536 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 870 \tTotal: 762.9140 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 871 \tTotal: 762.6097 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 872 \tTotal: 762.3793 \tMean_train_accuracy: 0.2015\n",
      "Epoch: 873 \tTotal: 763.2930 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 874 \tTotal: 763.2412 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 875 \tTotal: 762.9671 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 876 \tTotal: 762.7181 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 877 \tTotal: 762.8266 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 878 \tTotal: 762.2856 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 879 \tTotal: 762.8593 \tMean_train_accuracy: 0.2227\n",
      "Epoch: 880 \tTotal: 762.7232 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 881 \tTotal: 762.6184 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 882 \tTotal: 763.0297 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 883 \tTotal: 763.1363 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 884 \tTotal: 762.5852 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 885 \tTotal: 762.5130 \tMean_train_accuracy: 0.2023\n",
      "Epoch: 886 \tTotal: 763.5083 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 887 \tTotal: 762.3226 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 888 \tTotal: 762.2209 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 889 \tTotal: 762.6755 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 890 \tTotal: 762.9914 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 891 \tTotal: 762.6048 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 892 \tTotal: 762.5894 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 893 \tTotal: 763.4774 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 894 \tTotal: 763.1593 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 895 \tTotal: 762.9736 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 896 \tTotal: 763.0766 \tMean_train_accuracy: 0.2040\n",
      "Epoch: 897 \tTotal: 762.4228 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 898 \tTotal: 762.5327 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 899 \tTotal: 763.1398 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 900 \tTotal: 762.6511 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 901 \tTotal: 763.1022 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 902 \tTotal: 763.2913 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 903 \tTotal: 763.1535 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 904 \tTotal: 762.3833 \tMean_train_accuracy: 0.2244\n",
      "Epoch: 905 \tTotal: 762.7671 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 906 \tTotal: 762.7124 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 907 \tTotal: 762.8618 \tMean_train_accuracy: 0.2054\n",
      "Epoch: 908 \tTotal: 763.2427 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 909 \tTotal: 761.8630 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 910 \tTotal: 762.9109 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 911 \tTotal: 762.2869 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 912 \tTotal: 762.8718 \tMean_train_accuracy: 0.2127\n",
      "Epoch: 913 \tTotal: 763.2077 \tMean_train_accuracy: 0.2090\n",
      "Epoch: 914 \tTotal: 762.2507 \tMean_train_accuracy: 0.2108\n",
      "Epoch: 915 \tTotal: 763.1322 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 916 \tTotal: 762.9592 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 917 \tTotal: 762.5036 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 918 \tTotal: 762.5048 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 919 \tTotal: 762.8965 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 920 \tTotal: 762.7464 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 921 \tTotal: 762.7104 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 922 \tTotal: 762.3990 \tMean_train_accuracy: 0.2250\n",
      "Epoch: 923 \tTotal: 762.4679 \tMean_train_accuracy: 0.2265\n",
      "Epoch: 924 \tTotal: 763.2177 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 925 \tTotal: 761.9138 \tMean_train_accuracy: 0.2148\n",
      "Epoch: 926 \tTotal: 762.8776 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 927 \tTotal: 762.9821 \tMean_train_accuracy: 0.2283\n",
      "Epoch: 928 \tTotal: 762.5558 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 929 \tTotal: 762.2338 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 930 \tTotal: 762.5898 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 931 \tTotal: 762.1425 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 932 \tTotal: 763.5637 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 933 \tTotal: 763.2956 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 934 \tTotal: 762.7916 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 935 \tTotal: 762.2498 \tMean_train_accuracy: 0.2242\n",
      "Epoch: 936 \tTotal: 762.8057 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 937 \tTotal: 762.2985 \tMean_train_accuracy: 0.2238\n",
      "Epoch: 938 \tTotal: 762.7846 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 939 \tTotal: 763.2711 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 940 \tTotal: 762.5342 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 941 \tTotal: 762.8846 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 942 \tTotal: 763.1288 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 943 \tTotal: 762.5712 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 944 \tTotal: 763.1775 \tMean_train_accuracy: 0.2248\n",
      "Epoch: 945 \tTotal: 762.9367 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 946 \tTotal: 762.9102 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 947 \tTotal: 762.7407 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 948 \tTotal: 762.2061 \tMean_train_accuracy: 0.2196\n",
      "Epoch: 949 \tTotal: 763.5708 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 950 \tTotal: 762.7434 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 951 \tTotal: 762.2704 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 952 \tTotal: 763.4039 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 953 \tTotal: 763.2490 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 954 \tTotal: 762.6397 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 955 \tTotal: 762.4932 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 956 \tTotal: 762.8808 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 957 \tTotal: 762.5375 \tMean_train_accuracy: 0.2215\n",
      "Epoch: 958 \tTotal: 762.6413 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 959 \tTotal: 762.4217 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 960 \tTotal: 762.2330 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 961 \tTotal: 763.0573 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 962 \tTotal: 762.6415 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 963 \tTotal: 763.4821 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 964 \tTotal: 762.3629 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 965 \tTotal: 762.7393 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 966 \tTotal: 762.1222 \tMean_train_accuracy: 0.2077\n",
      "Epoch: 967 \tTotal: 762.9299 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 968 \tTotal: 762.5848 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 969 \tTotal: 762.7503 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 970 \tTotal: 762.5694 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 971 \tTotal: 763.0344 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 972 \tTotal: 762.5240 \tMean_train_accuracy: 0.2213\n",
      "Epoch: 973 \tTotal: 762.6838 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 974 \tTotal: 762.0256 \tMean_train_accuracy: 0.2231\n",
      "Epoch: 975 \tTotal: 763.0517 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 976 \tTotal: 762.3245 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 977 \tTotal: 762.0139 \tMean_train_accuracy: 0.2056\n",
      "Epoch: 978 \tTotal: 762.3532 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 979 \tTotal: 762.3910 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 980 \tTotal: 762.5829 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 981 \tTotal: 762.0865 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 982 \tTotal: 762.9501 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 983 \tTotal: 762.7515 \tMean_train_accuracy: 0.2019\n",
      "Epoch: 984 \tTotal: 762.8695 \tMean_train_accuracy: 0.2229\n",
      "Epoch: 985 \tTotal: 762.5898 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 986 \tTotal: 762.0047 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 987 \tTotal: 762.8714 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 988 \tTotal: 762.5300 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 989 \tTotal: 762.4794 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 990 \tTotal: 762.4561 \tMean_train_accuracy: 0.2258\n",
      "Epoch: 991 \tTotal: 762.5661 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 992 \tTotal: 762.0203 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 993 \tTotal: 762.3306 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 994 \tTotal: 762.9522 \tMean_train_accuracy: 0.2246\n",
      "Epoch: 995 \tTotal: 763.1315 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 996 \tTotal: 762.4832 \tMean_train_accuracy: 0.2085\n",
      "Epoch: 997 \tTotal: 762.9698 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 998 \tTotal: 762.5417 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 999 \tTotal: 762.0577 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 1000 \tTotal: 761.9955 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 1001 \tTotal: 762.8147 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 1002 \tTotal: 763.3061 \tMean_train_accuracy: 0.2121\n",
      "Epoch: 1003 \tTotal: 762.9885 \tMean_train_accuracy: 0.2063\n",
      "Epoch: 1004 \tTotal: 761.8192 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1005 \tTotal: 762.8382 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 1006 \tTotal: 763.2873 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 1007 \tTotal: 762.6735 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 1008 \tTotal: 762.3863 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 1009 \tTotal: 762.7594 \tMean_train_accuracy: 0.2138\n",
      "Epoch: 1010 \tTotal: 762.8669 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1011 \tTotal: 762.5257 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 1012 \tTotal: 762.9511 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 1013 \tTotal: 762.4923 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 1014 \tTotal: 763.0665 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 1015 \tTotal: 762.8749 \tMean_train_accuracy: 0.2298\n",
      "Epoch: 1016 \tTotal: 762.8276 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 1017 \tTotal: 762.6043 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 1018 \tTotal: 763.0838 \tMean_train_accuracy: 0.2233\n",
      "Epoch: 1019 \tTotal: 762.6793 \tMean_train_accuracy: 0.2062\n",
      "Epoch: 1020 \tTotal: 762.8862 \tMean_train_accuracy: 0.2154\n",
      "Epoch: 1021 \tTotal: 762.2223 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 1022 \tTotal: 763.0685 \tMean_train_accuracy: 0.2223\n",
      "Epoch: 1023 \tTotal: 762.2583 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 1024 \tTotal: 762.9838 \tMean_train_accuracy: 0.2242\n",
      "Epoch: 1025 \tTotal: 762.6169 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 1026 \tTotal: 762.9589 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 1027 \tTotal: 763.0617 \tMean_train_accuracy: 0.2367\n",
      "Epoch: 1028 \tTotal: 763.2777 \tMean_train_accuracy: 0.2256\n",
      "Epoch: 1029 \tTotal: 762.4506 \tMean_train_accuracy: 0.2106\n",
      "Epoch: 1030 \tTotal: 762.1130 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 1031 \tTotal: 762.6406 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 1032 \tTotal: 762.5859 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 1033 \tTotal: 762.7566 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 1034 \tTotal: 762.4837 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 1035 \tTotal: 763.3017 \tMean_train_accuracy: 0.2075\n",
      "Epoch: 1036 \tTotal: 763.2092 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1037 \tTotal: 762.5785 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 1038 \tTotal: 762.6333 \tMean_train_accuracy: 0.2237\n",
      "Epoch: 1039 \tTotal: 763.1387 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 1040 \tTotal: 762.3657 \tMean_train_accuracy: 0.2067\n",
      "Epoch: 1041 \tTotal: 763.3596 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 1042 \tTotal: 762.3092 \tMean_train_accuracy: 0.2217\n",
      "Epoch: 1043 \tTotal: 763.2110 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 1044 \tTotal: 762.2232 \tMean_train_accuracy: 0.2192\n",
      "Epoch: 1045 \tTotal: 763.7896 \tMean_train_accuracy: 0.2237\n",
      "Epoch: 1046 \tTotal: 762.5221 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 1047 \tTotal: 762.7079 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 1048 \tTotal: 762.3410 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 1049 \tTotal: 763.2195 \tMean_train_accuracy: 0.2113\n",
      "Epoch: 1050 \tTotal: 761.9794 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 1051 \tTotal: 763.1195 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 1052 \tTotal: 763.2692 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 1053 \tTotal: 762.8261 \tMean_train_accuracy: 0.2258\n",
      "Epoch: 1054 \tTotal: 763.1494 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 1055 \tTotal: 763.3317 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 1056 \tTotal: 762.6139 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 1057 \tTotal: 762.3602 \tMean_train_accuracy: 0.2265\n",
      "Epoch: 1058 \tTotal: 762.7742 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 1059 \tTotal: 762.5176 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 1060 \tTotal: 763.1642 \tMean_train_accuracy: 0.2094\n",
      "Epoch: 1061 \tTotal: 762.2099 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 1062 \tTotal: 762.7872 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 1063 \tTotal: 762.6271 \tMean_train_accuracy: 0.2250\n",
      "Epoch: 1064 \tTotal: 763.4617 \tMean_train_accuracy: 0.2027\n",
      "Epoch: 1065 \tTotal: 762.5473 \tMean_train_accuracy: 0.2163\n",
      "Epoch: 1066 \tTotal: 762.9214 \tMean_train_accuracy: 0.2183\n",
      "Epoch: 1067 \tTotal: 762.4481 \tMean_train_accuracy: 0.2194\n",
      "Epoch: 1068 \tTotal: 763.0808 \tMean_train_accuracy: 0.2058\n",
      "Epoch: 1069 \tTotal: 762.6208 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 1070 \tTotal: 763.0465 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 1071 \tTotal: 762.2631 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 1072 \tTotal: 762.5250 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 1073 \tTotal: 762.1883 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 1074 \tTotal: 762.2822 \tMean_train_accuracy: 0.2173\n",
      "Epoch: 1075 \tTotal: 762.6009 \tMean_train_accuracy: 0.2181\n",
      "Epoch: 1076 \tTotal: 762.9389 \tMean_train_accuracy: 0.2060\n",
      "Epoch: 1077 \tTotal: 762.7682 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 1078 \tTotal: 762.1875 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 1079 \tTotal: 762.6398 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 1080 \tTotal: 763.3262 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 1081 \tTotal: 763.1565 \tMean_train_accuracy: 0.2135\n",
      "Epoch: 1082 \tTotal: 762.2636 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 1083 \tTotal: 763.3506 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 1084 \tTotal: 762.6803 \tMean_train_accuracy: 0.2071\n",
      "Epoch: 1085 \tTotal: 762.8296 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 1086 \tTotal: 763.9049 \tMean_train_accuracy: 0.2115\n",
      "Epoch: 1087 \tTotal: 762.5043 \tMean_train_accuracy: 0.2117\n",
      "Epoch: 1088 \tTotal: 763.1963 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 1089 \tTotal: 763.0903 \tMean_train_accuracy: 0.2050\n",
      "Epoch: 1090 \tTotal: 762.8748 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 1091 \tTotal: 762.4298 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 1092 \tTotal: 762.6503 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 1093 \tTotal: 762.6632 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 1094 \tTotal: 762.7895 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 1095 \tTotal: 762.3468 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 1096 \tTotal: 762.7309 \tMean_train_accuracy: 0.2037\n",
      "Epoch: 1097 \tTotal: 762.3430 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 1098 \tTotal: 762.1308 \tMean_train_accuracy: 0.2042\n",
      "Epoch: 1099 \tTotal: 762.3397 \tMean_train_accuracy: 0.2235\n",
      "Epoch: 1100 \tTotal: 762.5852 \tMean_train_accuracy: 0.2198\n",
      "Epoch: 1101 \tTotal: 762.8887 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 1102 \tTotal: 763.0870 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 1103 \tTotal: 762.6302 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 1104 \tTotal: 763.1366 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 1105 \tTotal: 763.3232 \tMean_train_accuracy: 0.2017\n",
      "Epoch: 1106 \tTotal: 762.7645 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 1107 \tTotal: 763.4585 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 1108 \tTotal: 762.6331 \tMean_train_accuracy: 0.2079\n",
      "Epoch: 1109 \tTotal: 762.2763 \tMean_train_accuracy: 0.2271\n",
      "Epoch: 1110 \tTotal: 763.1643 \tMean_train_accuracy: 0.2142\n",
      "Epoch: 1111 \tTotal: 762.5146 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 1112 \tTotal: 763.0918 \tMean_train_accuracy: 0.2081\n",
      "Epoch: 1113 \tTotal: 762.8200 \tMean_train_accuracy: 0.2169\n",
      "Epoch: 1114 \tTotal: 761.8170 \tMean_train_accuracy: 0.2206\n",
      "Epoch: 1115 \tTotal: 762.9499 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 1116 \tTotal: 763.1561 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 1117 \tTotal: 762.8161 \tMean_train_accuracy: 0.2087\n",
      "Epoch: 1118 \tTotal: 763.1512 \tMean_train_accuracy: 0.2137\n",
      "Epoch: 1119 \tTotal: 762.1129 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 1120 \tTotal: 763.0800 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 1121 \tTotal: 762.7928 \tMean_train_accuracy: 0.2006\n",
      "Epoch: 1122 \tTotal: 763.1892 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 1123 \tTotal: 763.0499 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 1124 \tTotal: 762.9722 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 1125 \tTotal: 762.5386 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1126 \tTotal: 762.9519 \tMean_train_accuracy: 0.2140\n",
      "Epoch: 1127 \tTotal: 762.1222 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 1128 \tTotal: 762.8851 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 1129 \tTotal: 762.7986 \tMean_train_accuracy: 0.2015\n",
      "Epoch: 1130 \tTotal: 762.7421 \tMean_train_accuracy: 0.2131\n",
      "Epoch: 1131 \tTotal: 762.8567 \tMean_train_accuracy: 0.2221\n",
      "Epoch: 1132 \tTotal: 762.5730 \tMean_train_accuracy: 0.2125\n",
      "Epoch: 1133 \tTotal: 762.4819 \tMean_train_accuracy: 0.2304\n",
      "Epoch: 1134 \tTotal: 763.3876 \tMean_train_accuracy: 0.2150\n",
      "Epoch: 1135 \tTotal: 762.7218 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 1136 \tTotal: 762.1961 \tMean_train_accuracy: 0.2160\n",
      "Epoch: 1137 \tTotal: 762.8685 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 1138 \tTotal: 762.6511 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 1139 \tTotal: 762.6413 \tMean_train_accuracy: 0.2162\n",
      "Epoch: 1140 \tTotal: 762.8375 \tMean_train_accuracy: 0.2158\n",
      "Epoch: 1141 \tTotal: 762.8549 \tMean_train_accuracy: 0.2069\n",
      "Epoch: 1142 \tTotal: 762.8411 \tMean_train_accuracy: 0.2156\n",
      "Epoch: 1143 \tTotal: 762.3268 \tMean_train_accuracy: 0.2179\n",
      "Epoch: 1144 \tTotal: 763.0043 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 1145 \tTotal: 763.2426 \tMean_train_accuracy: 0.2202\n",
      "Epoch: 1146 \tTotal: 762.8715 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 1147 \tTotal: 762.6890 \tMean_train_accuracy: 0.2208\n",
      "Epoch: 1148 \tTotal: 762.6408 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 1149 \tTotal: 762.5051 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 1150 \tTotal: 763.0451 \tMean_train_accuracy: 0.2031\n",
      "Epoch: 1151 \tTotal: 762.8019 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 1152 \tTotal: 762.6197 \tMean_train_accuracy: 0.2235\n",
      "Epoch: 1153 \tTotal: 762.7101 \tMean_train_accuracy: 0.2152\n",
      "Epoch: 1154 \tTotal: 762.8361 \tMean_train_accuracy: 0.2323\n",
      "Epoch: 1155 \tTotal: 762.4479 \tMean_train_accuracy: 0.2144\n",
      "Epoch: 1156 \tTotal: 762.3933 \tMean_train_accuracy: 0.2225\n",
      "Epoch: 1157 \tTotal: 762.8167 \tMean_train_accuracy: 0.2146\n",
      "Epoch: 1158 \tTotal: 762.6697 \tMean_train_accuracy: 0.2112\n",
      "Epoch: 1159 \tTotal: 763.4752 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 1160 \tTotal: 763.4611 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1161 \tTotal: 763.6668 \tMean_train_accuracy: 0.2188\n",
      "Epoch: 1162 \tTotal: 762.6145 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 1163 \tTotal: 762.3255 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 1164 \tTotal: 762.2716 \tMean_train_accuracy: 0.2092\n",
      "Epoch: 1165 \tTotal: 762.7119 \tMean_train_accuracy: 0.2190\n",
      "Epoch: 1166 \tTotal: 762.1902 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 1167 \tTotal: 762.5626 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 1168 \tTotal: 762.2324 \tMean_train_accuracy: 0.2252\n",
      "Epoch: 1169 \tTotal: 763.0955 \tMean_train_accuracy: 0.2104\n",
      "Epoch: 1170 \tTotal: 763.2748 \tMean_train_accuracy: 0.2200\n",
      "Epoch: 1171 \tTotal: 762.6958 \tMean_train_accuracy: 0.2071\n",
      "Epoch: 1172 \tTotal: 762.3965 \tMean_train_accuracy: 0.2058\n",
      "Epoch: 1173 \tTotal: 762.6477 \tMean_train_accuracy: 0.2177\n",
      "Epoch: 1174 \tTotal: 763.2343 \tMean_train_accuracy: 0.2100\n",
      "Epoch: 1175 \tTotal: 763.6268 \tMean_train_accuracy: 0.2102\n",
      "Epoch: 1176 \tTotal: 762.9571 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1177 \tTotal: 762.6630 \tMean_train_accuracy: 0.2175\n",
      "Epoch: 1178 \tTotal: 762.3003 \tMean_train_accuracy: 0.2167\n",
      "Epoch: 1179 \tTotal: 762.0839 \tMean_train_accuracy: 0.2073\n",
      "Epoch: 1180 \tTotal: 762.8233 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 1181 \tTotal: 762.8437 \tMean_train_accuracy: 0.2129\n",
      "Epoch: 1182 \tTotal: 763.2039 \tMean_train_accuracy: 0.2123\n",
      "Epoch: 1183 \tTotal: 763.0700 \tMean_train_accuracy: 0.2096\n",
      "Epoch: 1184 \tTotal: 763.1974 \tMean_train_accuracy: 0.2098\n",
      "Epoch: 1185 \tTotal: 763.5758 \tMean_train_accuracy: 0.2071\n",
      "Epoch: 1186 \tTotal: 762.2157 \tMean_train_accuracy: 0.2133\n",
      "Epoch: 1187 \tTotal: 762.4529 \tMean_train_accuracy: 0.2171\n",
      "Epoch: 1188 \tTotal: 762.6134 \tMean_train_accuracy: 0.2185\n",
      "Epoch: 1189 \tTotal: 762.3219 \tMean_train_accuracy: 0.2119\n",
      "Epoch: 1190 \tTotal: 763.0369 \tMean_train_accuracy: 0.2238\n",
      "Epoch: 1191 \tTotal: 762.9374 \tMean_train_accuracy: 0.2294\n",
      "Epoch: 1192 \tTotal: 762.8701 \tMean_train_accuracy: 0.2052\n",
      "Epoch: 1193 \tTotal: 762.9957 \tMean_train_accuracy: 0.2204\n",
      "Epoch: 1194 \tTotal: 762.6200 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 1195 \tTotal: 762.9011 \tMean_train_accuracy: 0.2165\n",
      "Epoch: 1196 \tTotal: 762.1605 \tMean_train_accuracy: 0.2048\n",
      "Epoch: 1197 \tTotal: 762.4132 \tMean_train_accuracy: 0.2044\n",
      "Epoch: 1198 \tTotal: 762.4852 \tMean_train_accuracy: 0.2248\n",
      "Epoch: 1199 \tTotal: 762.6126 \tMean_train_accuracy: 0.2110\n",
      "Epoch: 1200 \tTotal: 762.7517 \tMean_train_accuracy: 0.2100\n"
     ]
    }
   ],
   "source": [
    "edge_index = edge_index.to(device)\n",
    "top_num = 300\n",
    "\n",
    "for epoch in range(1200):\n",
    "\n",
    "    total_overall = 0\n",
    "    forward_loss = 0\n",
    "\n",
    "    mean_train_accuracy = 0\n",
    "    for batch_idx, feature_label in enumerate(train_loader):        \n",
    "        features = feature_label[0].to(device)\n",
    "        labels = feature_label[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        train_accuracy = 0\n",
    "        for i, x_i in enumerate(features):\n",
    "            \n",
    "            log_sum = torch.sum(torch.log(1 - x_i), dim=1, keepdim=True)\n",
    "            feat = 1 - torch.exp(log_sum)\n",
    "            \n",
    "            y_i = labels[i]\n",
    "            y_hat = forward_model(feat, adj)\n",
    "            _, top_indices_true = torch.topk(y_i.clone(), top_num)\n",
    "            \n",
    "            _, top_indices_predict = torch.topk(y_hat.clone().squeeze(-1), top_num)\n",
    "            \n",
    "            # 将张量数组转换为Python列表\n",
    "            list1 = top_indices_true.tolist()\n",
    "            list_pre = top_indices_predict.tolist()\n",
    "\n",
    "            # 使用集合操作找到交集\n",
    "            intersection = list(set(list1) & set(list_pre))\n",
    "            accuracy_i = len(intersection) / top_num       \n",
    "            train_accuracy += accuracy_i \n",
    "\n",
    "            forward_loss = F.mse_loss(y_hat.squeeze(-1), y_i, reduction='sum')        \n",
    "            loss += forward_loss    \n",
    "        \n",
    "        total_overall += loss.item()    \n",
    "        train_accuracy /= len(features)\n",
    "        mean_train_accuracy = train_accuracy\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for p in forward_model.parameters():\n",
    "            p.data.clamp_(min=0)\n",
    "        \n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTotal: {:.4f}\".format(total_overall / train_batch_size),\n",
    "        \"\\tMean_train_accuracy: {:.4f}\".format(mean_train_accuracy),\n",
    "        )  \n",
    "    \n",
    "    # mean_accuracy = 0\n",
    "    # mean_accuracy_sum = 0\n",
    "\n",
    "    \n",
    "    # for batch_idx, feature_label in enumerate(test_loader):   \n",
    "    #     features = feature_label[0].to(device)\n",
    "    #     labels = feature_label[1].to(device)\n",
    "        \n",
    "    #     accuracy = 0\n",
    "    #     accuracy_sum = 0\n",
    "        \n",
    "    #     for i, x_i in enumerate(features):\n",
    "    #         y_i = labels[i]\n",
    "    #         _, top_indices_true = torch.topk(y_i, top_num)\n",
    "            \n",
    "    #         y_hat = forward_model(x_i, edge_index)\n",
    "            \n",
    "    #         _, top_indices_predict = torch.topk(y_hat.squeeze(-1), top_num)\n",
    "            \n",
    "    #         sum_pre = torch.sum(x_i, dim=1, keepdim=True)\n",
    "    #         _, top_indices_sum = torch.topk(sum_pre.squeeze(-1), top_num)\n",
    "            \n",
    "    #         # 将张量数组转换为Python列表\n",
    "    #         list1 = top_indices_true.tolist()\n",
    "    #         list_pre = top_indices_predict.tolist()\n",
    "            \n",
    "    #         list_sum = top_indices_sum.tolist()\n",
    "\n",
    "    #         # 使用集合操作找到交集\n",
    "    #         intersection = list(set(list1) & set(list_pre))\n",
    "            \n",
    "    #         intersection_sum = list(set(list1) & set(list_sum))\n",
    "            \n",
    "    #         accuracy_i = len(intersection) / top_num       \n",
    "    #         accuracy += accuracy_i \n",
    "    #         accuracy_sum += len(intersection_sum) / top_num  \n",
    "    #     accuracy /= test_batch_size\n",
    "    #     accuracy_sum/= test_batch_size\n",
    "    #     mean_accuracy = accuracy\n",
    "    #     mean_accuracy_sum = accuracy_sum\n",
    "    #     break\n",
    "    \n",
    "    # print(\n",
    "    #     \"\\tMean_test_accuracy: {:.4f}\".format(mean_accuracy),\n",
    "    #     \"\\tMean_test_accuracy_sum: {:.4f}\".format(mean_accuracy_sum)\n",
    "    #     )  \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
