{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "pwd = '/home/zjy/project/MetaIM/data'\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "cora_dataset = Planetoid(root=pwd+'/cora', name='cora')\n",
    "data = cora_dataset[0]\n",
    "edge_index = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), (500, 2, 2708))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "individual_infection_path = pwd+'/for_meta/cora_individual_infection_sir.npy'\n",
    "seeds_infection_path = pwd+'/for_meta/cora_seed_infection_sir.npy'\n",
    "\n",
    "individual_infection = np.load(individual_infection_path)\n",
    "seeds_infection = np.load(seeds_infection_path)\n",
    "individual_infection.shape,seeds_infection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "                       [ 633, 1862, 2582,  ...,  598, 1473, 2706]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(2708, 2708), nnz=10556, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 转换为 scipy 稀疏矩阵\n",
    "adj = to_scipy_sparse_matrix(edge_index)\n",
    "\n",
    "\n",
    "# def normalize_adj(mx):\n",
    "#     \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "#     rowsum = np.array(mx.sum(1))\n",
    "#     r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "#     r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "#     r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "#     return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "\n",
    "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "# adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "adj = torch.Tensor(adj.toarray()).to_sparse()\n",
    "adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = input.device\n",
    "\n",
    "        N = input.size()[0]\n",
    "        if adj.layout == torch.sparse_coo:\n",
    "            edge = adj.indices()\n",
    "        else:\n",
    "            edge = adj.nonzero().t()\n",
    "\n",
    "        assert not torch.isnan(input).any()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        \n",
    "        self.attentions1 = [SpGraphAttentionLayer(nhid * nheads, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "            \n",
    "        for i, attention in enumerate(self.attentions1):\n",
    "            self.add_module('attention1_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(torch.cat([att(x, adj) for att in self.attentions], dim=1))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GATConv\n",
    "# class GAT(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels, num_heads):\n",
    "#         super(GAT, self).__init__()\n",
    "#         self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads)\n",
    "#         self.conv2 = GATConv(hidden_channels * num_heads, out_channels, heads=1)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.elu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_num = int(seeds_infection[0][0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, individual_infection,seeds_infection, feat_num):\n",
    "        self.individual_infection = individual_infection\n",
    "        self.seeds_infection = seeds_infection\n",
    "        self.feat_shape = (len(individual_infection), feat_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seeds_infection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeds= np.nonzero(self.seeds_infection[idx][0])[0]\n",
    "        \n",
    "        feature = torch.zeros(self.feat_shape[0],self.feat_shape[1])\n",
    "        for i in range(len(seeds)):\n",
    "            seed_i_infection = torch.tensor(self.individual_infection[seeds[i]])\n",
    "            feature[:, i] = seed_i_infection\n",
    "            \n",
    "        label = self.seeds_infection[idx][1]\n",
    "        \n",
    "        return feature, label\n",
    "\n",
    "dataset = CustomDataset(individual_infection,seeds_infection,feat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义划分比例\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# 划分数据集\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*train_ratio), int(len(dataset)*test_ratio)])\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 4\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.elu(x)\n",
    "\n",
    "# class SpGAT_pyg(nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels, dropout, alpha, heads):\n",
    "#         super(SpGAT_pyg, self).__init__()\n",
    "#         self.dropout = dropout\n",
    "#         self.attentions = nn.ModuleList()\n",
    "#         self.attentions1 = nn.ModuleList()\n",
    "\n",
    "#         for _ in range(heads):\n",
    "#             self.attentions.append(GATConv(in_channels, hidden_channels, heads=1, dropout=dropout, concat=True))\n",
    "        \n",
    "#         for _ in range(heads):\n",
    "#             self.attentions1.append(GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout, concat=True))\n",
    "\n",
    "#         self.out_att = GATConv(hidden_channels * heads, out_channels, heads=1, dropout=dropout, concat=False)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "#         x = torch.cat([att(x, edge_index) for att in self.attentions], dim=1)\n",
    "#         x = F.elu(x)\n",
    "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "#         x = torch.cat([att(x, edge_index) for att in self.attentions1], dim=1)\n",
    "#         x = F.elu(x)\n",
    "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "#         x = self.out_att(x, edge_index)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (conv1): GATConv(135, 256, heads=4)\n",
       "  (conv2): GATConv(1024, 1, heads=1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data.model.gat import GAT, SpGAT\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "\n",
    "# forward_model = SpGAT(nfeat=feat_num, \n",
    "#                 nhid=64, \n",
    "#                 nclass=1, \n",
    "#                 dropout=0.2, \n",
    "#                 nheads=1, \n",
    "#                 alpha=0.2)\n",
    "forward_model = GAT(feat_num, 256, 1, 4)\n",
    "\n",
    "optimizer = Adam([{'params': forward_model.parameters()}], \n",
    "                 lr=1e-3)\n",
    "\n",
    "adj = adj.to(device)\n",
    "forward_model = forward_model.to(device)\n",
    "forward_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain_loss: 957.1669\n",
      "\tMean_accuracy: 0.2925 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 2 \tTrain_loss: 820.3996\n",
      "\tMean_accuracy: 0.2233 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 3 \tTrain_loss: 804.3333\n",
      "\tMean_accuracy: 0.2950 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 4 \tTrain_loss: 797.3896\n",
      "\tMean_accuracy: 0.2942 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 5 \tTrain_loss: 786.1856\n",
      "\tMean_accuracy: 0.2992 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 6 \tTrain_loss: 783.1814\n",
      "\tMean_accuracy: 0.3008 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 7 \tTrain_loss: 781.7125\n",
      "\tMean_accuracy: 0.3017 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 8 \tTrain_loss: 780.4040\n",
      "\tMean_accuracy: 0.3008 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 9 \tTrain_loss: 779.2126\n",
      "\tMean_accuracy: 0.2975 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 10 \tTrain_loss: 778.7553\n",
      "\tMean_accuracy: 0.3050 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 11 \tTrain_loss: 777.9280\n",
      "\tMean_accuracy: 0.3033 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 12 \tTrain_loss: 777.0770\n",
      "\tMean_accuracy: 0.3100 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 13 \tTrain_loss: 776.3085\n",
      "\tMean_accuracy: 0.3183 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 14 \tTrain_loss: 775.3287\n",
      "\tMean_accuracy: 0.3008 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 15 \tTrain_loss: 774.1215\n",
      "\tMean_accuracy: 0.3217 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 16 \tTrain_loss: 772.9111\n",
      "\tMean_accuracy: 0.3275 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 17 \tTrain_loss: 771.2692\n",
      "\tMean_accuracy: 0.3258 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 18 \tTrain_loss: 769.4930\n",
      "\tMean_accuracy: 0.3400 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 19 \tTrain_loss: 767.7528\n",
      "\tMean_accuracy: 0.3308 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 20 \tTrain_loss: 766.4608\n",
      "\tMean_accuracy: 0.3342 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 21 \tTrain_loss: 765.6033\n",
      "\tMean_accuracy: 0.3292 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 22 \tTrain_loss: 764.8514\n",
      "\tMean_accuracy: 0.3367 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 23 \tTrain_loss: 764.2326\n",
      "\tMean_accuracy: 0.3350 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 24 \tTrain_loss: 763.8955\n",
      "\tMean_accuracy: 0.3333 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 25 \tTrain_loss: 763.3352\n",
      "\tMean_accuracy: 0.3383 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 26 \tTrain_loss: 762.8576\n",
      "\tMean_accuracy: 0.3358 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 27 \tTrain_loss: 762.1947\n",
      "\tMean_accuracy: 0.3400 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 28 \tTrain_loss: 761.8079\n",
      "\tMean_accuracy: 0.3450 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 29 \tTrain_loss: 761.2548\n",
      "\tMean_accuracy: 0.3425 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 30 \tTrain_loss: 760.8729\n",
      "\tMean_accuracy: 0.3408 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 31 \tTrain_loss: 760.7299\n",
      "\tMean_accuracy: 0.3400 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 32 \tTrain_loss: 759.7229\n",
      "\tMean_accuracy: 0.3442 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 33 \tTrain_loss: 759.2265\n",
      "\tMean_accuracy: 0.3458 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 34 \tTrain_loss: 758.8864\n",
      "\tMean_accuracy: 0.3417 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 35 \tTrain_loss: 758.5178\n",
      "\tMean_accuracy: 0.3467 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 36 \tTrain_loss: 757.5981\n",
      "\tMean_accuracy: 0.3483 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 37 \tTrain_loss: 757.6789\n",
      "\tMean_accuracy: 0.3400 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 38 \tTrain_loss: 758.5174\n",
      "\tMean_accuracy: 0.3433 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 39 \tTrain_loss: 757.2931\n",
      "\tMean_accuracy: 0.3550 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 40 \tTrain_loss: 755.4185\n",
      "\tMean_accuracy: 0.3500 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 41 \tTrain_loss: 754.2205\n",
      "\tMean_accuracy: 0.3500 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 42 \tTrain_loss: 753.1911\n",
      "\tMean_accuracy: 0.3442 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 43 \tTrain_loss: 753.2497\n",
      "\tMean_accuracy: 0.3408 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 44 \tTrain_loss: 752.6775\n",
      "\tMean_accuracy: 0.3400 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 45 \tTrain_loss: 751.5452\n",
      "\tMean_accuracy: 0.3458 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 46 \tTrain_loss: 750.4034\n",
      "\tMean_accuracy: 0.3367 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 47 \tTrain_loss: 750.5663\n",
      "\tMean_accuracy: 0.3483 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 48 \tTrain_loss: 748.7646\n",
      "\tMean_accuracy: 0.3367 \tMean_accuracy_sum: 0.3917\n",
      "Epoch: 49 \tTrain_loss: 746.1926\n",
      "\tMean_accuracy: 0.3400 \tMean_accuracy_sum: 0.3917\n"
     ]
    }
   ],
   "source": [
    "edge_index = edge_index.to(device)\n",
    "\n",
    "for epoch in range(2000):\n",
    "\n",
    "    total_overall = 0\n",
    "    forward_loss = 0\n",
    "\n",
    "    for batch_idx, feature_label in enumerate(train_loader):        \n",
    "        features = feature_label[0].to(device)\n",
    "        labels = feature_label[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        for i, x_i in enumerate(features):\n",
    "            y_i = labels[i]\n",
    "            y_hat = forward_model(x_i, edge_index)\n",
    "\n",
    "            forward_loss = F.mse_loss(y_hat.squeeze(-1), y_i, reduction='sum')        \n",
    "            loss += forward_loss    \n",
    "        \n",
    "        total_overall += loss.item()    \n",
    "        \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    print(\"Epoch: {}\".format(epoch+1), \n",
    "        \"\\tTrain_loss: {:.4f}\".format(total_overall / train_batch_size),\n",
    "        )  \n",
    "    \n",
    "    mean_accuracy = 0\n",
    "    mean_accuracy_sum = 0\n",
    "\n",
    "    \n",
    "    for batch_idx, feature_label in enumerate(test_loader):   \n",
    "        features = feature_label[0].to(device)\n",
    "        labels = feature_label[1].to(device)\n",
    "        \n",
    "        accuracy = 0\n",
    "        accuracy_sum = 0\n",
    "        \n",
    "        for i, x_i in enumerate(features):\n",
    "            y_i = labels[i]\n",
    "            _, top_indices_true = torch.topk(y_i, 300)\n",
    "            \n",
    "            y_hat = forward_model(x_i, edge_index)\n",
    "            \n",
    "            _, top_indices_predict = torch.topk(y_hat.squeeze(-1), 300)\n",
    "            \n",
    "            sum_pre = torch.sum(x_i, dim=1, keepdim=True)\n",
    "            _, top_indices_sum = torch.topk(sum_pre.squeeze(-1), 300)\n",
    "            \n",
    "            # 将张量数组转换为Python列表\n",
    "            list1 = top_indices_true.tolist()\n",
    "            list_pre = top_indices_predict.tolist()\n",
    "            \n",
    "            list_sum = top_indices_sum.tolist()\n",
    "\n",
    "            # 使用集合操作找到交集\n",
    "            intersection = list(set(list1) & set(list_pre))\n",
    "            \n",
    "            intersection_sum = list(set(list1) & set(list_sum))\n",
    "            \n",
    "            accuracy_i = len(intersection) / 300       \n",
    "            accuracy += accuracy_i \n",
    "            accuracy_sum += len(intersection_sum) / 300  \n",
    "        accuracy /= test_batch_size\n",
    "        accuracy_sum/= test_batch_size\n",
    "        mean_accuracy = accuracy\n",
    "        mean_accuracy_sum = accuracy_sum\n",
    "        break\n",
    "    \n",
    "    print(\n",
    "        \"\\tMean_accuracy: {:.4f}\".format(mean_accuracy),\n",
    "        \"\\tMean_accuracy_sum: {:.4f}\".format(mean_accuracy_sum)\n",
    "        )  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection: [3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个张量数组\n",
    "tensor1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "tensor2 = torch.tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "# 将张量数组转换为Python列表\n",
    "list1 = tensor1.tolist()\n",
    "list2 = tensor2.tolist()\n",
    "\n",
    "# 使用集合操作找到交集\n",
    "intersection = list(set(list1) & set(list2))\n",
    "\n",
    "print(\"Intersection:\", intersection)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
